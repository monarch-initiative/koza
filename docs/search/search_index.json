{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Koza","text":""},{"location":"#a-data-transformation-framework-in-python","title":"A data transformation framework in Python","text":""},{"location":"#overview","title":"Overview","text":"<p>Koza is a data transformation framework which allows you to write semi-declarative \"ingests\"</p> <ul> <li>Transform csv, json, yaml, jsonl, or xml source data, converting them to a target csv, json, or jsonl format based on your dataclass model.  </li> <li>Koza also can output data in the KGX format</li> <li>Write data transforms in semi-declarative Python</li> <li>Configure source files, expected columns/json properties and path filters, field filters, and metadata in yaml</li> <li>Create or import mapping files to be used in ingests (eg. id mapping, type mappings)</li> <li>Create and use translation tables to map between source and target vocabularies</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Koza is available on PyPi and can be installed via pip: <pre><code>pip install koza\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":"<p>See the Ingests page for information on how to configure ingests for koza to use.</p> <p>Koza can be used as a Python library, or via the command line. CLI commands are available for validating and transforming data. See the Module page for information on using Koza as a library.</p> <p>Koza also includes some examples to help you get started (see <code>koza/examples</code>).</p>"},{"location":"#basic-examples","title":"Basic Examples","text":"<p>Validate</p> <p>Give Koza a local or remote csv file, and get some basic information (headers, number of rows)</p> <pre><code>koza validate \\\n  --file ./examples/data/string.tsv \\\n  --delimiter ' '\n</code></pre> <p>Sending a json or jsonl formatted file will confirm if the file is valid json or jsonl</p> <pre><code>koza validate \\\n  --file ./examples/data/ZFIN_PHENOTYPE_0.jsonl.gz \\\n  --format jsonl\n</code></pre> <pre><code>koza validate \\\n  --file ./examples/data/ddpheno.json.gz \\\n  --format json\n</code></pre> <p>Transform</p> <p>Try one of Koza's example ingests: <pre><code>koza transform \\\n  --source examples/string-declarative/protein-links-detailed.yaml \\\n  --global-table examples/translation_table.yaml\n</code></pre></p> <p>Note:    Koza expects a directory structure as described in the above example   with the source config file and transform code in the same directory   (these files can also simply be named <code>transform.yaml</code> and <code>transform.py</code>, as is default):    <pre><code>.\n\u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 some_source\n\u2502   \u2502   \u251c\u2500\u2500 your_ingest.yaml\n\u2502   \u2502   \u2514\u2500\u2500 your_ingest.py\n\u2502   \u2514\u2500\u2500 some_translation_table.yaml\n\u2514\u2500\u2500 ...\n</code></pre></p>"},{"location":"cli-reference/","title":"Koza CLI Reference","text":"<p>Complete reference for all Koza command-line interface commands and options.</p>"},{"location":"cli-reference/#global-options","title":"Global Options","text":""},{"location":"cli-reference/#version-information","title":"Version Information","text":"<p><pre><code>koza --version\n</code></pre> Display the current Koza version and exit.</p>"},{"location":"cli-reference/#commands","title":"Commands","text":""},{"location":"cli-reference/#transform","title":"transform","text":"<p>Transform biomedical data sources into KGX format using semi-declarative Python transforms.</p>"},{"location":"cli-reference/#synopsis","title":"Synopsis","text":"<pre><code>koza transform CONFIGURATION_YAML [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#arguments","title":"Arguments","text":"<ul> <li><code>CONFIGURATION_YAML</code> (required) - Path to the transform configuration YAML file</li> </ul>"},{"location":"cli-reference/#options","title":"Options","text":"Option Short Type Default Description <code>--input-file</code> <code>-i</code> List[str] None Override input files specified in configuration <code>--output-dir</code> <code>-o</code> str <code>./output</code> Path to output directory <code>--output-format</code> <code>-f</code> OutputFormat <code>tsv</code> Output format (<code>tsv</code>, <code>jsonl</code>, <code>parquet</code>) <code>--limit</code> <code>-n</code> int 0 Number of rows to process (0 = all) <code>--progress</code> <code>-p</code> bool False Display progress bar during transform <code>--quiet</code> <code>-q</code> bool False Suppress output except errors <code>--global-table</code> str None Path to global translation table YAML <code>--local-table</code> str None Path to local translation table YAML <code>--log-level</code> str <code>WARNING</code> Set logging level <code>--log-conf</code> str None Path to logging configuration file"},{"location":"cli-reference/#examples","title":"Examples","text":"<pre><code># Basic transform\nkoza transform examples/string/protein-links-detailed.yaml\n\n# Transform with custom output directory and format\nkoza transform config.yaml -o ./results -f jsonl\n\n# Transform with progress and row limit\nkoza transform config.yaml --progress --limit 1000\n\n# Transform with global translation table\nkoza transform config.yaml --global-table translation.yaml\n</code></pre>"},{"location":"cli-reference/#join","title":"join","text":"<p>Combine multiple KGX files into a unified DuckDB database with automatic schema harmonization.</p>"},{"location":"cli-reference/#synopsis_1","title":"Synopsis","text":"<pre><code>koza join [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_1","title":"Options","text":"Option Type Default Description <code>--nodes</code> List[str] None Node files to join (supports glob patterns) <code>--edges</code> List[str] None Edge files to join (supports glob patterns) <code>--input-dir</code> Path None Directory containing KGX files (auto-discovers) <code>--output</code> str <code>joined_graph.duckdb</code> Output DuckDB database path <code>--schema-report</code> bool False Generate detailed schema analysis report <code>--show-progress</code> bool False Display progress bars during loading <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#file-specification-formats","title":"File Specification Formats","text":"<p>Simple paths: <pre><code>koza join --nodes genes.tsv proteins.jsonl --edges interactions.parquet\n</code></pre></p> <p>With source names: <pre><code>koza join --nodes \"genes:genes.tsv\" \"proteins:proteins.jsonl\"\n</code></pre></p> <p>Glob patterns: <pre><code>koza join --nodes \"*.nodes.*\" --edges \"*.edges.*\"\n</code></pre></p>"},{"location":"cli-reference/#examples_1","title":"Examples","text":"<pre><code># Basic join with mixed formats\nkoza join \\\n  --nodes genes.tsv proteins.jsonl pathways.parquet \\\n  --edges gene_protein.tsv protein_pathway.jsonl \\\n  --output unified_graph.duckdb\n\n# Join with schema reporting and progress\nkoza join \\\n  --nodes \"*.nodes.*\" \\\n  --edges \"*.edges.*\" \\\n  --schema-report \\\n  --show-progress\n\n# Auto-discover files in directory\nkoza join --input-dir ./kgx_files --output merged.duckdb\n\n# Quiet operation for scripts\nkoza join --nodes \"*.nodes.tsv\" --edges \"*.edges.tsv\" --quiet\n</code></pre>"},{"location":"cli-reference/#output","title":"Output","text":"<ul> <li>Database file: DuckDB database with <code>nodes</code> and <code>edges</code> tables</li> <li>Schema report: <code>{output}_schema_report_join.yaml</code> (if <code>--schema-report</code>)</li> <li>CLI summary: Files processed, records loaded, schema harmonization details</li> </ul>"},{"location":"cli-reference/#split","title":"split","text":"<p>Extract subsets of data from a DuckDB database with configurable filters and format conversion.</p>"},{"location":"cli-reference/#synopsis_2","title":"Synopsis","text":"<pre><code>koza split [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_2","title":"Options","text":"Option Type Default Description <code>--database</code> Path Required Input DuckDB database path <code>--split-on</code> str Required Column to split on (creates separate files per value) <code>--output-dir</code> Path <code>./split_output</code> Directory for output files <code>--output-format</code> KGXFormat <code>tsv</code> Output format (<code>tsv</code>, <code>jsonl</code>, <code>parquet</code>) <code>--filter-nodes</code> str None SQL WHERE clause for filtering nodes <code>--filter-edges</code> str None SQL WHERE clause for filtering edges <code>--show-progress</code> bool False Display progress bars during export <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#examples_2","title":"Examples","text":"<pre><code># Split by source with original format\nkoza split --database graph.duckdb --split-on provided_by\n\n# Split with format conversion to Parquet\nkoza split \\\n  --database graph.duckdb \\\n  --split-on namespace \\\n  --output-format parquet \\\n  --output-dir ./parquet_output\n\n# Split with custom filters\nkoza split \\\n  --database graph.duckdb \\\n  --split-on category \\\n  --filter-nodes \"namespace='HGNC'\" \\\n  --filter-edges \"predicate='biolink:interacts_with'\"\n\n# Split with progress tracking\nkoza split \\\n  --database large_graph.duckdb \\\n  --split-on provided_by \\\n  --show-progress\n</code></pre>"},{"location":"cli-reference/#output_1","title":"Output","text":"<p>For each unique value in the split column, creates: - <code>{value}_nodes.{format}</code> - Node data for that value - <code>{value}_edges.{format}</code> - Edge data for that value</p>"},{"location":"cli-reference/#prune","title":"prune","text":"<p>Clean up graph integrity issues by handling dangling edges and singleton nodes.</p>"},{"location":"cli-reference/#synopsis_3","title":"Synopsis","text":"<pre><code>koza prune [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_3","title":"Options","text":"Option Type Default Description <code>--database</code> Path Required DuckDB database to prune <code>--keep-singletons</code> bool False Preserve isolated nodes in main graph <code>--remove-singletons</code> bool False Move isolated nodes to separate table <code>--dry-run</code> bool False Preview changes without applying them <code>--show-progress</code> bool False Display progress bars during operation <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#singleton-node-strategies","title":"Singleton Node Strategies","text":"<p>Keep singletons (default behavior): - Isolated nodes remain in <code>nodes</code> table - Only dangling edges are moved</p> <p>Remove singletons: - Isolated nodes moved to <code>singleton_nodes</code> table - Main graph contains only connected nodes</p>"},{"location":"cli-reference/#examples_3","title":"Examples","text":"<pre><code># Basic pruning with singleton preservation\nkoza prune --database graph.duckdb --keep-singletons\n\n# Remove singletons to separate table\nkoza prune --database graph.duckdb --remove-singletons\n\n# Preview changes without applying\nkoza prune --database graph.duckdb --dry-run\n\n# Quiet operation for automation\nkoza prune --database graph.duckdb --keep-singletons --quiet\n\n# With progress tracking for large graphs\nkoza prune \\\n  --database large_graph.duckdb \\\n  --remove-singletons \\\n  --show-progress\n</code></pre>"},{"location":"cli-reference/#output_2","title":"Output","text":"<p>Creates additional tables for data preservation: - <code>dangling_edges</code>: Edges pointing to non-existent nodes - <code>singleton_nodes</code>: Isolated nodes (if <code>--remove-singletons</code>) - CLI summary: Counts of edges/nodes moved, integrity statistics</p>"},{"location":"cli-reference/#append","title":"append","text":"<p>Add new KGX files to existing databases with schema evolution and optional deduplication.</p>"},{"location":"cli-reference/#synopsis_4","title":"Synopsis","text":"<pre><code>koza append [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_4","title":"Options","text":"Option Type Default Description <code>--database</code> Path Required Existing DuckDB database to append to <code>--nodes</code> List[str] None Node files to append <code>--edges</code> List[str] None Edge files to append <code>--deduplicate</code> bool False Remove exact duplicates after appending <code>--schema-report</code> bool False Generate schema analysis after append <code>--show-progress</code> bool False Display progress bars during loading <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#schema-evolution","title":"Schema Evolution","text":"<p>When appending files with new columns: - New columns automatically added to existing tables - Existing records get NULL values for new columns - All schema changes are tracked and reported</p>"},{"location":"cli-reference/#deduplication-strategy","title":"Deduplication Strategy","text":"<p>When <code>--deduplicate</code> is enabled: - Removes exact duplicates by ID field - Keeps first occurrence of each duplicate ID - Operates on both nodes and edges tables - Reports number of duplicates removed</p>"},{"location":"cli-reference/#examples_4","title":"Examples","text":"<pre><code># Basic append operation\nkoza append \\\n  --database existing_graph.duckdb \\\n  --nodes new_genes.tsv \\\n  --edges new_interactions.jsonl\n\n# Append with deduplication and schema reporting\nkoza append \\\n  --database graph.duckdb \\\n  --nodes genes_with_new_fields.tsv \\\n  --edges updated_interactions.parquet \\\n  --deduplicate \\\n  --schema-report\n\n# Append multiple files with progress\nkoza append \\\n  --database graph.duckdb \\\n  --nodes \"new_*.nodes.*\" \\\n  --edges \"new_*.edges.*\" \\\n  --show-progress\n\n# Quiet append for automation\nkoza append \\\n  --database graph.duckdb \\\n  --nodes corrections.tsv \\\n  --quiet\n</code></pre>"},{"location":"cli-reference/#output_3","title":"Output","text":"<ul> <li>Schema changes: Reports new columns added and their sources</li> <li>Record counts: Shows before/after record counts for nodes and edges</li> <li>Duplicate statistics: Number of duplicates removed (if <code>--deduplicate</code>)</li> <li>Schema report: Detailed analysis (if <code>--schema-report</code>)</li> </ul>"},{"location":"cli-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"cli-reference/#file-specification-formats_1","title":"File Specification Formats","text":"<p>All commands that accept file lists support multiple specification formats:</p>"},{"location":"cli-reference/#glob-patterns","title":"Glob Patterns","text":"<pre><code># Match all node files\n--nodes \"*.nodes.*\"\n\n# Match specific formats\n--nodes \"*.tsv\" --edges \"*.jsonl\"\n\n# Complex patterns\n--nodes \"**/genes*.{tsv,jsonl}\"\n</code></pre>"},{"location":"cli-reference/#source-attribution","title":"Source Attribution","text":"<pre><code># Assign source names to files\n--nodes \"genes:genes.tsv\" \"proteins:proteins.jsonl\"\n--edges \"interactions:interactions.parquet\"\n</code></pre>"},{"location":"cli-reference/#mixed-specifications","title":"Mixed Specifications","text":"<pre><code># Combine different formats\n--nodes genes.tsv \"pathways:pathways.jsonl\" proteins.parquet\n</code></pre>"},{"location":"cli-reference/#progress-and-logging","title":"Progress and Logging","text":""},{"location":"cli-reference/#progress-indicators","title":"Progress Indicators","text":"<ul> <li><code>--show-progress</code>: Display progress bars for file operations</li> <li><code>--quiet</code>: Suppress all non-error output</li> <li>Cannot use both flags together</li> </ul>"},{"location":"cli-reference/#log-levels","title":"Log Levels","text":"<p>Available for <code>transform</code> command: - <code>DEBUG</code>: Verbose debugging information - <code>INFO</code>: General information messages - <code>WARNING</code>: Warning messages (default) - <code>ERROR</code>: Error messages only</p>"},{"location":"cli-reference/#output-formats","title":"Output Formats","text":""},{"location":"cli-reference/#supported-formats","title":"Supported Formats","text":"<ul> <li>TSV: Tab-separated values (KGX standard)</li> <li>JSONL: JSON Lines format  </li> <li>Parquet: Columnar format for analytics</li> </ul>"},{"location":"cli-reference/#format-selection","title":"Format Selection","text":"<pre><code># Explicit format specification\n--output-format tsv\n--output-format jsonl  \n--output-format parquet\n\n# Automatic format detection from file extensions\n# genes.tsv \u2192 TSV format\n# genes.jsonl \u2192 JSONL format\n# genes.parquet \u2192 Parquet format\n</code></pre>"},{"location":"cli-reference/#error-handling","title":"Error Handling","text":""},{"location":"cli-reference/#common-error-messages","title":"Common Error Messages","text":""},{"location":"cli-reference/#file-not-found","title":"File Not Found","text":"<p><pre><code>Error: File not found: missing_file.tsv\n</code></pre> Solution: Verify file paths and check file permissions</p>"},{"location":"cli-reference/#database-not-found","title":"Database Not Found","text":"<p><pre><code>Error: Database not found: nonexistent.duckdb\n</code></pre> Solution: Verify database path or create database with <code>join</code> command first</p>"},{"location":"cli-reference/#schema-conflicts","title":"Schema Conflicts","text":"<p><pre><code>Warning: Type conflict for column 'score' (INTEGER vs VARCHAR)\n</code></pre> Resolution: DuckDB automatically resolves to most permissive type</p>"},{"location":"cli-reference/#permission-issues","title":"Permission Issues","text":"<p><pre><code>Error: Permission denied writing to output directory\n</code></pre> Solution: Check write permissions on output directory</p>"},{"location":"cli-reference/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error (file not found, permission denied, etc.) 2 Invalid arguments or configuration 130 Interrupted by user (Ctrl+C)"},{"location":"cli-reference/#performance-tips","title":"Performance Tips","text":""},{"location":"cli-reference/#for-large-files","title":"For Large Files","text":"<ol> <li>Use compressed formats (<code>.gz</code>, <code>.bz2</code>) to reduce I/O</li> <li>Enable progress bars to monitor long operations  </li> <li>Use Parquet format for repeated analytical operations</li> <li>Process in chunks using directory-based operations</li> </ol>"},{"location":"cli-reference/#memory-optimization","title":"Memory Optimization","text":"<ol> <li>Close other applications during large operations</li> <li>Use streaming operations (automatic for large files)</li> <li>Monitor disk space for join operations</li> <li>Consider temporary directory on fast storage</li> </ol>"},{"location":"cli-reference/#automation-scripts","title":"Automation Scripts","text":"<ol> <li>Use <code>--quiet</code> flag to suppress output in scripts</li> <li>Check exit codes for error handling</li> <li>Enable logging for debugging automated workflows</li> <li>Use glob patterns for flexible file matching</li> </ol>"},{"location":"cli-reference/#integration-examples","title":"Integration Examples","text":""},{"location":"cli-reference/#shell-scripts","title":"Shell Scripts","text":"<pre><code>#!/bin/bash\n# Complete graph processing pipeline\n\nset -e  # Exit on error\n\necho \"Starting graph processing pipeline...\"\n\n# Join all source files\nkoza join \\\n  --nodes \"sources/*.nodes.*\" \\\n  --edges \"sources/*.edges.*\" \\\n  --output raw_graph.duckdb \\\n  --show-progress\n\n# Clean up integrity issues\nkoza prune \\\n  --database raw_graph.duckdb \\\n  --keep-singletons\n\n# Add corrected data\nif [ -f \"corrections.tsv\" ]; then\n  koza append \\\n    --database raw_graph.duckdb \\\n    --nodes corrections.tsv \\\n    --deduplicate \\\n    --quiet\nfi\n\n# Export by namespace\nkoza split \\\n  --database raw_graph.duckdb \\\n  --split-on namespace \\\n  --output-format parquet \\\n  --output-dir final_graphs\n\necho \"Pipeline completed successfully\"\n</code></pre>"},{"location":"cli-reference/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\nname: Process Knowledge Graph\non: [push]\n\njobs:\n  process-graph:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Koza\n        run: pip install koza\n      - name: Join graph files\n        run: |\n          koza join \\\n            --input-dir ./data \\\n            --output graph.duckdb \\\n            --quiet\n      - name: Clean graph\n        run: |\n          koza prune \\\n            --database graph.duckdb \\\n            --keep-singletons \\\n            --quiet\n      - name: Upload results\n        uses: actions/upload-artifact@v2\n        with:\n          name: processed-graph\n          path: graph.duckdb\n</code></pre>"},{"location":"cli-reference/#configuration-files","title":"Configuration Files","text":"<p>While most commands use command-line arguments, complex configurations can be managed through:</p> <ol> <li>Environment variables for commonly used paths</li> <li>Shell aliases for frequently used command combinations  </li> <li>Configuration files for transform operations (YAML format)</li> <li>Makefiles for complex multi-step workflows</li> </ol>"},{"location":"cli-reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli-reference/#getting-help","title":"Getting Help","text":"<pre><code># General help\nkoza --help\n\n# Command-specific help  \nkoza join --help\nkoza split --help\nkoza prune --help\nkoza append --help\nkoza transform --help\n</code></pre>"},{"location":"cli-reference/#debug-information","title":"Debug Information","text":"<pre><code># Enable verbose logging for transform\nkoza transform config.yaml --log-level DEBUG\n\n# Use dry-run to preview prune operations\nkoza prune --database graph.duckdb --dry-run\n\n# Generate schema reports for analysis\nkoza join --nodes \"*.tsv\" --schema-report\n</code></pre>"},{"location":"cli-reference/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Monitor large operations with progress\nkoza join --nodes \"*.nodes.*\" --edges \"*.edges.*\" --show-progress\n\n# Time operations for benchmarking\ntime koza join --nodes \"*.nodes.tsv\" --quiet\n</code></pre> <p>For additional support, consult the comprehensive graph operations guide and the Koza documentation.</p>"},{"location":"graph-operations/","title":"Koza Graph Operations Guide","text":"<p>This guide provides comprehensive documentation for Koza's graph operations, designed to work with Knowledge Graph Exchange (KGX) files in multiple formats.</p>"},{"location":"graph-operations/#overview","title":"Overview","text":"<p>Koza graph operations provide powerful tools for manipulating, analyzing, and transforming knowledge graphs stored in KGX format. All operations are built on DuckDB for high performance and support TSV, JSONL, and Parquet formats seamlessly.</p>"},{"location":"graph-operations/#core-operations","title":"Core Operations","text":"<ul> <li>join - Combine multiple KGX files into unified database</li> <li>split - Extract subsets with format conversion</li> <li>prune - Clean graph integrity issues</li> <li>append - Incrementally add data with schema evolution</li> <li>merge - Complete pipeline: join \u2192 deduplicate \u2192 normalize \u2192 prune</li> <li>normalize - Apply SSSOM mappings to normalize identifiers</li> <li>deduplicate - Remove duplicate nodes and edges</li> </ul>"},{"location":"graph-operations/#reporting-operations","title":"Reporting Operations","text":"<ul> <li>report - Generate QC, graph stats, or schema reports</li> <li>node-report - Detailed node analysis by category/source</li> <li>edge-report - Detailed edge analysis by predicate/source</li> <li>node-examples - Extract example nodes by category</li> <li>edge-examples - Extract example edges by predicate</li> </ul>"},{"location":"graph-operations/#key-principles","title":"Key Principles","text":"<ol> <li>Format Agnostic: All operations work across TSV, JSONL, and Parquet</li> <li>Schema Flexible: Automatic harmonization handles column differences</li> <li>Non-destructive: Data is moved to archive tables, never deleted</li> <li>Performance Focused: DuckDB enables fast processing of large graphs</li> </ol>"},{"location":"graph-operations/#join-operation","title":"Join Operation","text":"<p>The <code>join</code> operation combines multiple KGX files into a unified DuckDB database with automatic schema harmonization.</p>"},{"location":"graph-operations/#basic-usage","title":"Basic Usage","text":"<pre><code># Join node and edge files from different formats\nkoza join \\\n  --nodes genes.tsv proteins.jsonl pathways.parquet \\\n  --edges gene_protein.tsv protein_pathway.jsonl \\\n  --output merged_graph.duckdb\n</code></pre>"},{"location":"graph-operations/#advanced-options","title":"Advanced Options","text":"<pre><code># Join with comprehensive reporting and progress tracking\nkoza join \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --schema-report \\\n  --show-progress \\\n  --quiet\n</code></pre>"},{"location":"graph-operations/#schema-harmonization","title":"Schema Harmonization","text":"<p>Join automatically handles files with different column schemas:</p> <ul> <li>Missing columns: Filled with NULL values</li> <li>Extra columns: Preserved in final schema</li> <li>Type conflicts: Resolved using DuckDB's type inference</li> <li>Multi-valued fields: Detected and converted to arrays where appropriate</li> </ul>"},{"location":"graph-operations/#output","title":"Output","text":"<ul> <li>Database file: DuckDB database with <code>nodes</code> and <code>edges</code> tables</li> <li>Schema report: YAML file with detailed schema analysis (if <code>--schema-report</code>)</li> <li>CLI summary: Statistics on files processed, records loaded, schema changes</li> </ul> <p>Example CLI output: <pre><code>\u2713 Join completed successfully\n  \ud83d\udcc1 Files processed: 6 (6 successful)\n  \ud83d\udcca Records loaded: 125,340 nodes, 298,567 edges\n  \ud83d\udd04 Schema harmonization: 3 missing columns filled, 2 extra preserved\n  \ud83d\udcc2 Database created: merged_graph.duckdb (4.2 MB)\n  \u23f1\ufe0f  Total time: 12.4s\n</code></pre></p>"},{"location":"graph-operations/#split-operation","title":"Split Operation","text":"<p>The <code>split</code> operation extracts subsets of data from a database with configurable filters and format conversion.</p>"},{"location":"graph-operations/#basic-usage_1","title":"Basic Usage","text":"<pre><code># Split by provided_by field into separate files\nkoza split \\\n  --database graph.duckdb \\\n  --split-on provided_by \\\n  --output-dir ./split_output\n</code></pre>"},{"location":"graph-operations/#format-conversion","title":"Format Conversion","text":"<pre><code># Convert TSV database to Parquet files during split\nkoza split \\\n  --database graph.duckdb \\\n  --split-on category \\\n  --output-format parquet \\\n  --output-dir ./parquet_output\n</code></pre>"},{"location":"graph-operations/#filtering-options","title":"Filtering Options","text":"<pre><code># Split with custom filters\nkoza split \\\n  --database graph.duckdb \\\n  --split-on namespace \\\n  --filter-nodes \"category='biolink:Gene'\" \\\n  --filter-edges \"predicate='biolink:interacts_with'\" \\\n  --output-dir ./filtered_split\n</code></pre>"},{"location":"graph-operations/#multivalued-field-handling","title":"Multivalued Field Handling","text":"<p>When splitting on array-type fields (e.g., <code>category</code>, <code>provided_by</code>), split automatically handles expansion:</p> <ul> <li>Single-valued fields: Direct matching on field value</li> <li>Array fields: Uses <code>UNNEST()</code> to expand arrays, creating one output file per unique value</li> <li>Records appear in multiple outputs: If a node has <code>category: ['biolink:Gene', 'biolink:NamedThing']</code>, it will appear in both split files</li> </ul>"},{"location":"graph-operations/#output_1","title":"Output","text":"<p>Creates separate files for each unique value in the split field: - <code>{value}_nodes.{format}</code> - <code>{value}_edges.{format}</code></p>"},{"location":"graph-operations/#prune-operation","title":"Prune Operation","text":"<p>The <code>prune</code> operation cleans up graph integrity issues by handling dangling edges and singleton nodes.</p>"},{"location":"graph-operations/#basic-usage_2","title":"Basic Usage","text":"<pre><code># Remove dangling edges, keep singleton nodes\nkoza prune \\\n  --database graph.duckdb \\\n  --keep-singletons\n</code></pre>"},{"location":"graph-operations/#singleton-node-strategies","title":"Singleton Node Strategies","text":"<p>Keep singletons (default): <pre><code>koza prune --database graph.duckdb --keep-singletons\n</code></pre></p> <p>Remove singletons: <pre><code>koza prune --database graph.duckdb --remove-singletons\n</code></pre></p>"},{"location":"graph-operations/#dangling-edge-handling","title":"Dangling Edge Handling","text":"<p>Dangling edges (pointing to non-existent nodes) are automatically: 1. Identified using LEFT JOIN queries 2. Moved to <code>dangling_edges</code> table (not deleted) 3. Categorized by missing subject vs object nodes 4. Reported with source attribution</p>"},{"location":"graph-operations/#output_2","title":"Output","text":"<p>Prune creates additional tables for data preservation: - <code>dangling_edges</code>: Edges pointing to missing nodes - <code>singleton_nodes</code>: Isolated nodes (if <code>--remove-singletons</code>) - Original tables: Cleaned of integrity issues</p> <p>Example CLI output: <pre><code>\u2713 Graph pruned successfully\n  - 156 dangling edges moved to dangling_edges table\n  - 23 singleton nodes preserved (--keep-singletons)\n  - Main graph: 174,844 connected edges remain\n\ud83d\udcca Dangling edges by source:\n  - source_a: 89 edges (missing 12 target nodes)\n  - source_b: 67 edges (missing 8 target nodes)\n</code></pre></p>"},{"location":"graph-operations/#append-operation","title":"Append Operation","text":"<p>The <code>append</code> operation adds new data to existing databases with schema evolution and optional deduplication.</p>"},{"location":"graph-operations/#basic-usage_3","title":"Basic Usage","text":"<pre><code># Add new files to existing database\nkoza append \\\n  --database existing_graph.duckdb \\\n  --nodes new_genes.tsv updated_pathways.jsonl \\\n  --edges new_interactions.parquet\n</code></pre>"},{"location":"graph-operations/#schema-evolution","title":"Schema Evolution","text":"<pre><code># Append with automatic schema evolution and deduplication\nkoza append \\\n  --database graph.duckdb \\\n  --nodes genes_with_new_fields.tsv \\\n  --deduplicate \\\n  --show-progress \\\n  --schema-report\n</code></pre>"},{"location":"graph-operations/#schema-evolution-features","title":"Schema Evolution Features","text":"<ul> <li>New columns: Automatically added to existing tables</li> <li>Backward compatibility: Existing data gets NULL for new columns</li> <li>Type safety: DuckDB handles type inference and conversion</li> <li>Change tracking: Reports all schema modifications</li> </ul>"},{"location":"graph-operations/#deduplication","title":"Deduplication","text":"<p>When <code>--deduplicate</code> is enabled: - Removes exact duplicates by ID field - Keeps first occurrence of each ID - Reports number of duplicates removed - Works on both nodes and edges tables</p>"},{"location":"graph-operations/#output_3","title":"Output","text":"<p>Append operation provides detailed change tracking:</p> <p>Example CLI output: <pre><code>\u2713 Append completed successfully\n  \ud83d\udcc1 Files processed: 3 (3 successful)\n  \ud83d\udcca Records added: 15,234\n  \ud83d\udd04 Schema evolution: 2 new columns added\n    - Added 1 new columns to nodes: custom_score\n    - Added 1 new columns to edges: confidence_value\n  \ud83d\udd27 Duplicates removed: 45\n  \ud83d\udcc8 Database growth:\n    - Nodes: 125,340 \u2192 138,574 (+13,234)\n    - Edges: 298,567 \u2192 300,567 (+2,000)\n    - Database: graph.duckdb (4.8 MB)\n  \u23f1\ufe0f  Total time: 8.2s\n</code></pre></p>"},{"location":"graph-operations/#merge-operation","title":"Merge Operation","text":"<p>The <code>merge</code> operation is a composite pipeline that orchestrates multiple operations in sequence: join \u2192 deduplicate \u2192 normalize \u2192 prune. This is the recommended way to create a complete, clean knowledge graph from multiple sources.</p>"},{"location":"graph-operations/#basic-usage_4","title":"Basic Usage","text":"<pre><code># Complete merge pipeline with all steps\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --mappings mappings/*.sssom.tsv \\\n  --output merged_graph.duckdb\n</code></pre>"},{"location":"graph-operations/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>Join: Loads all input node and edge files into a unified database</li> <li>Deduplicate: Removes duplicate nodes/edges by ID (keeps first occurrence)</li> <li>Normalize: Applies SSSOM mappings to normalize identifiers</li> <li>Prune: Removes dangling edges and handles singleton nodes</li> </ol>"},{"location":"graph-operations/#skipping-steps","title":"Skipping Steps","text":"<pre><code># Skip normalization (no SSSOM mappings)\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --skip-normalize\n\n# Skip both normalization and pruning\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --skip-normalize \\\n  --skip-prune\n\n# Skip deduplication\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --skip-deduplicate\n</code></pre>"},{"location":"graph-operations/#export-options","title":"Export Options","text":"<pre><code># Merge and export to files\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --graph-name my_graph \\\n  --output-format parquet\n\n# Create compressed archive\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --archive \\\n  --compress\n</code></pre>"},{"location":"graph-operations/#output_4","title":"Output","text":"<p>Example CLI output: <pre><code>Starting merge pipeline...\nPipeline: join \u2192 deduplicate \u2192 normalize \u2192 prune\nOutput database: merged_graph.duckdb\nStep 1: Join - Loading input files...\nJoin completed: 6 files | 125,340 nodes | 298,567 edges\nStep 2: Deduplicate - Removing duplicate nodes/edges...\nDeduplicate completed: 45 duplicate nodes, 123 duplicate edges removed\nStep 3: Normalize - Applying SSSOM mappings...\nNormalize completed: 3 mapping files | 15,234 edge references normalized\nStep 4: Prune - Cleaning graph structure...\nPrune completed: 156 dangling edges moved | 23 singleton nodes handled\nMerge pipeline completed successfully!\n</code></pre></p>"},{"location":"graph-operations/#normalize-operation","title":"Normalize Operation","text":"<p>The <code>normalize</code> operation applies SSSOM (Simple Standard for Sharing Ontological Mappings) files to normalize node identifiers in edge references.</p>"},{"location":"graph-operations/#basic-usage_5","title":"Basic Usage","text":"<pre><code># Apply SSSOM mappings to existing database\nkoza normalize \\\n  --database graph.duckdb \\\n  --mappings mappings/mondo.sssom.tsv mappings/hp.sssom.tsv\n</code></pre>"},{"location":"graph-operations/#using-a-mappings-directory","title":"Using a Mappings Directory","text":"<pre><code># Load all SSSOM files from a directory\nkoza normalize \\\n  --database graph.duckdb \\\n  --mappings-dir ./mappings\n</code></pre>"},{"location":"graph-operations/#how-it-works","title":"How It Works","text":"<ol> <li>Load SSSOM files: Reads mapping files with <code>#</code> comment handling for YAML headers</li> <li>Create mappings table: Deduplicates by <code>object_id</code> (one mapping per ID to prevent edge duplication)</li> <li>Normalize edges: Rewrites <code>subject</code> and <code>object</code> columns using the mappings</li> <li>Preserve originals: Stores original values in <code>original_subject</code> and <code>original_object</code> columns</li> </ol>"},{"location":"graph-operations/#sssom-file-format","title":"SSSOM File Format","text":"<p>Standard SSSOM TSV format with optional YAML header:</p> <pre><code>#curie_map:\n#  MONDO: http://purl.obolibrary.org/obo/MONDO_\n#  OMIM: https://omim.org/entry/\nsubject_id  predicate_id    object_id   mapping_justification\nMONDO:0005148   skos:exactMatch OMIM:222100 semapv:ManualMappingCuration\n</code></pre>"},{"location":"graph-operations/#duplicate-mapping-handling","title":"Duplicate Mapping Handling","text":"<p>When one <code>object_id</code> maps to multiple <code>subject_id</code> values: - Only the first mapping is kept (ordered by source file, then subject_id) - A warning is issued with the count of duplicate mappings removed - This prevents edge duplication during normalization</p>"},{"location":"graph-operations/#output_5","title":"Output","text":"<pre><code>\u2713 Loaded 45,678 unique mappings\n\u26a0\ufe0f  Found 234 duplicate mappings (one object_id mapped to multiple subject_ids). Keeping only one mapping per object_id.\n\u2713 Normalized 12,345 edge subject/object references\n</code></pre>"},{"location":"graph-operations/#deduplicate-operation","title":"Deduplicate Operation","text":"<p>The <code>deduplicate</code> operation removes duplicate nodes and edges from a graph database, keeping the first occurrence of each ID.</p>"},{"location":"graph-operations/#how-it-works_1","title":"How It Works","text":"<ol> <li>Identify duplicates: Finds nodes/edges with the same ID appearing multiple times</li> <li>Archive duplicates: Moves ALL duplicate rows to <code>duplicate_nodes</code> / <code>duplicate_edges</code> tables</li> <li>Keep first: Retains only the first occurrence (ordered by <code>file_source</code> or <code>provided_by</code>)</li> </ol>"},{"location":"graph-operations/#when-to-use","title":"When to Use","text":"<p>Deduplication is automatically included in the <code>merge</code> pipeline. Use standalone deduplication when: - You've appended data without deduplication - You need to clean up an existing database - You want finer control over the deduplication process</p>"},{"location":"graph-operations/#archive-tables","title":"Archive Tables","text":"<p>After deduplication, you can inspect removed duplicates:</p> <pre><code>-- View duplicate nodes\nSELECT * FROM duplicate_nodes LIMIT 10;\n\n-- Count duplicates by source\nSELECT file_source, COUNT(*) FROM duplicate_edges GROUP BY file_source;\n</code></pre> <p>Note: The deduplicate operation is not exposed as a standalone CLI command but is available programmatically and as part of the <code>merge</code> pipeline.</p>"},{"location":"graph-operations/#reporting-operations_1","title":"Reporting Operations","text":"<p>Koza provides comprehensive reporting capabilities for analyzing graph databases.</p>"},{"location":"graph-operations/#qc-report","title":"QC Report","text":"<p>Generate quality control reports with node/edge statistics grouped by source:</p> <pre><code>koza report qc \\\n  --database graph.duckdb \\\n  --output qc_report.yaml\n</code></pre>"},{"location":"graph-operations/#graph-statistics","title":"Graph Statistics","text":"<p>Generate comprehensive graph statistics:</p> <pre><code>koza report graph-stats \\\n  --database graph.duckdb \\\n  --output graph_stats.yaml\n</code></pre>"},{"location":"graph-operations/#schema-report","title":"Schema Report","text":"<p>Analyze table schemas and column coverage:</p> <pre><code>koza report schema \\\n  --database graph.duckdb \\\n  --output schema_report.yaml\n</code></pre>"},{"location":"graph-operations/#node-report","title":"Node Report","text":"<p>Generate detailed node analysis reports with category and source breakdowns:</p> <pre><code># From database\nkoza node-report \\\n  --database graph.duckdb \\\n  --output node_report.tsv\n\n# From file directly\nkoza node-report \\\n  --file nodes.tsv \\\n  --output node_report.tsv \\\n  --format tsv\n</code></pre>"},{"location":"graph-operations/#output-formats","title":"Output Formats","text":"<ul> <li>TSV: Tab-separated values (default)</li> <li>CSV: Comma-separated values</li> <li>JSON: JSON format</li> </ul>"},{"location":"graph-operations/#edge-report","title":"Edge Report","text":"<p>Generate detailed edge analysis reports with predicate and source breakdowns:</p> <pre><code># From database\nkoza edge-report \\\n  --database graph.duckdb \\\n  --output edge_report.tsv\n\n# With node denormalization (adds subject/object names)\nkoza edge-report \\\n  --database graph.duckdb \\\n  --nodes nodes.tsv \\\n  --output edge_report.tsv\n</code></pre>"},{"location":"graph-operations/#node-examples","title":"Node Examples","text":"<p>Extract example nodes for each category:</p> <pre><code>koza node-examples \\\n  --database graph.duckdb \\\n  --output examples/ \\\n  --limit 10\n</code></pre>"},{"location":"graph-operations/#edge-examples","title":"Edge Examples","text":"<p>Extract example edges for each predicate:</p> <pre><code>koza edge-examples \\\n  --database graph.duckdb \\\n  --output examples/ \\\n  --limit 10\n</code></pre>"},{"location":"graph-operations/#schema-reporting","title":"Schema Reporting","text":"<p>All operations support optional schema reporting with <code>--schema-report</code> flag.</p>"},{"location":"graph-operations/#report-contents","title":"Report Contents","text":"<p>Schema reports include: - File analysis: Format detection, column counts, record counts - Schema summary: Unique columns across all files - Column details: Data types, null percentages, example values - Format compatibility: Cross-format column mapping</p>"},{"location":"graph-operations/#report-formats","title":"Report Formats","text":"<p>CLI Summary: <pre><code>\ud83d\udccb Schema Analysis:\n  - Nodes: 4 files, 23 unique columns\n  - Edges: 2 files, 18 unique columns\n\ud83d\udcca Column Coverage:\n  - id: 100% (required field)\n  - category: 100% (nodes only)\n  - predicate: 100% (edges only)\n  - custom_score: 25% (extension field)\n</code></pre></p> <p>YAML Report (saved as <code>{database_name}_schema_report_{operation}.yaml</code>): <pre><code>operation: join\ntimestamp: \"2024-01-15T10:30:45\"\nsummary:\n  nodes:\n    file_count: 4\n    unique_columns: 23\n    total_records: 125340\n  edges:\n    file_count: 2  \n    unique_columns: 18\n    total_records: 298567\nfiles:\n  - path: \"genes.tsv\"\n    format: \"TSV\"\n    type: \"nodes\"\n    records: 50000\n    columns:\n      id: {type: \"VARCHAR\", null_pct: 0}\n      category: {type: \"VARCHAR\", null_pct: 0}\n      # ... additional columns\n</code></pre></p>"},{"location":"graph-operations/#multi-format-support","title":"Multi-Format Support","text":""},{"location":"graph-operations/#supported-formats","title":"Supported Formats","text":"<ul> <li>TSV: Tab-separated values (standard KGX format)</li> <li>JSONL: JSON Lines (one JSON object per line)</li> <li>Parquet: Columnar format for analytics</li> </ul>"},{"location":"graph-operations/#format-detection","title":"Format Detection","text":"<p>Automatic detection based on: 1. File extension (<code>.tsv</code>, <code>.jsonl</code>, <code>.parquet</code>) 2. Compression support (<code>.gz</code>, <code>.bz2</code>) 3. Content analysis for ambiguous cases</p>"},{"location":"graph-operations/#mixed-format-operations","title":"Mixed-Format Operations","text":"<p>All operations seamlessly handle mixed formats:</p> <pre><code># Join files in different formats\nkoza join \\\n  --nodes genes.tsv proteins.jsonl pathways.parquet \\\n  --edges interactions.tsv.gz associations.jsonl.bz2 \\\n  --output mixed_graph.duckdb\n</code></pre>"},{"location":"graph-operations/#format-conversion_1","title":"Format Conversion","text":"<p>Operations can convert between formats:</p> <pre><code># Convert TSV to Parquet during split\nkoza split \\\n  --database tsv_graph.duckdb \\\n  --split-on provided_by \\\n  --output-format parquet\n</code></pre>"},{"location":"graph-operations/#performance-considerations","title":"Performance Considerations","text":""},{"location":"graph-operations/#memory-management","title":"Memory Management","text":"<ul> <li>Streaming: Large files processed in chunks</li> <li>Lazy evaluation: Operations chained without intermediate copies  </li> <li>Compression: Support for compressed inputs reduces I/O</li> </ul>"},{"location":"graph-operations/#query-optimization","title":"Query Optimization","text":"<ul> <li>DuckDB engine: Vectorized execution and query optimization</li> <li>Columnar storage: Efficient for analytical workloads</li> <li>Parallel processing: Multi-threaded operations where possible</li> </ul>"},{"location":"graph-operations/#best-practices","title":"Best Practices","text":"<ol> <li>Use compressed files (<code>.gz</code>) to reduce I/O time</li> <li>Enable progress bars (<code>--show-progress</code>) for long operations</li> <li>Use <code>merge</code> for complete pipelines - handles deduplication, normalization, and pruning</li> <li>Monitor disk space for large join operations</li> <li>Use Parquet format for repeated analytical workloads</li> <li>Generate QC reports after operations to verify data quality</li> </ol>"},{"location":"graph-operations/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"graph-operations/#common-issues","title":"Common Issues","text":"<p>File not found: <pre><code>Error: File not found: missing_file.tsv\nSolution: Verify file paths and permissions\n</code></pre></p> <p>Schema conflicts: <pre><code>Warning: Type conflict for column 'score' (INTEGER vs VARCHAR)\nResolution: DuckDB will use most permissive type (VARCHAR)\n</code></pre></p> <p>Memory limits: <pre><code>Error: Out of memory during operation\nSolution: Process files in smaller batches or increase system memory\n</code></pre></p>"},{"location":"graph-operations/#recovery-strategies","title":"Recovery Strategies","text":"<ol> <li>Non-destructive operations: Original data preserved in archive tables</li> <li>Transaction safety: Operations are atomic (all succeed or all fail)  </li> <li>Backup recommendations: Copy databases before destructive operations</li> <li>Logging: Comprehensive logging for debugging issues</li> </ol>"},{"location":"graph-operations/#data-integrity","title":"Data Integrity","text":"<p>All operations maintain referential integrity: - Dangling edges moved to separate tables (not deleted) - Schema changes tracked and reversible - Duplicate data preserved with clear provenance - Foreign key relationships maintained where possible</p>"},{"location":"graph-operations/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"graph-operations/#pipeline-workflows","title":"Pipeline Workflows","text":"<p>Chain multiple operations for complete graph processing:</p> <pre><code># Complete graph processing pipeline\nkoza join \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output raw_graph.duckdb \\\n  --schema-report\n\nkoza prune \\\n  --database raw_graph.duckdb \\\n  --keep-singletons\n\nkoza append \\\n  --database raw_graph.duckdb \\\n  --nodes corrections.tsv \\\n  --deduplicate\n\nkoza split \\\n  --database raw_graph.duckdb \\\n  --split-on namespace \\\n  --output-format parquet \\\n  --output-dir final_graphs\n</code></pre>"},{"location":"graph-operations/#configuration-files","title":"Configuration Files","text":"<p>Use configuration files for complex operations:</p> <pre><code># join_config.yaml\nnodes:\n  - path: \"genes.tsv\"\n    source_name: \"gene_source\"\n  - path: \"proteins.jsonl\"\n    source_name: \"protein_source\"\nedges:\n  - path: \"interactions.parquet\"\n    source_name: \"interaction_source\"\noutput: \"configured_graph.duckdb\"\nschema_report: true\nshow_progress: true\n</code></pre> <pre><code>koza join --config join_config.yaml\n</code></pre>"},{"location":"graph-operations/#batch-processing","title":"Batch Processing","text":"<p>Process multiple databases in batch:</p> <pre><code># Process all databases in directory\nfor db in *.duckdb; do\n  echo \"Processing $db\"\n  koza prune --database \"$db\" --keep-singletons --quiet\n  koza split --database \"$db\" --split-on provided_by --output-dir \"./split_$(basename $db .duckdb)\"\ndone\n</code></pre>"},{"location":"graph-operations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"graph-operations/#performance-issues","title":"Performance Issues","text":"<p>Slow join operations: - Check available memory and disk space - Use compressed input files - Consider processing files in smaller batches</p> <p>High memory usage: - Enable streaming mode (automatic for large files) - Close other applications during processing - Use temporary directories on fast storage</p>"},{"location":"graph-operations/#data-quality-issues","title":"Data Quality Issues","text":"<p>Unexpected schema changes: - Review schema reports for column additions - Verify input file consistency - Check for encoding issues in text files</p> <p>Missing data after operations: - Check archive tables (<code>dangling_edges</code>, <code>singleton_nodes</code>) - Review operation logs for warnings - Verify input file integrity</p>"},{"location":"graph-operations/#getting-help","title":"Getting Help","text":"<p>For additional support: - Check operation-specific help: <code>koza {operation} --help</code> - Review schema reports for data insights - Enable verbose logging with <code>--debug</code> flag - Consult the Koza documentation</p>"},{"location":"graph-operations/#migration-from-cat-merge","title":"Migration from cat-merge","text":"<p>For users migrating from cat-merge workflows:</p>"},{"location":"graph-operations/#equivalent-operations","title":"Equivalent Operations","text":"cat-merge Koza Graph Operations <code>merge()</code> <code>koza merge</code> (recommended) or <code>koza join</code> + <code>koza prune</code> Directory merge <code>koza merge --nodes *.nodes.* --edges *.edges.*</code> Format conversion <code>koza split --output-format {format}</code> SSSOM normalization <code>koza normalize --mappings *.sssom.tsv</code> Deduplication Included in <code>koza merge</code> or <code>koza append --deduplicate</code> QC reporting <code>koza report qc --database graph.duckdb</code>"},{"location":"graph-operations/#key-improvements","title":"Key Improvements","text":"<ol> <li>Multi-format support: No longer limited to TSV</li> <li>Schema flexibility: Automatic handling of column differences</li> <li>Data preservation: Non-destructive operations with archiving</li> <li>Rich CLI: Progress bars, statistics, and detailed reporting</li> <li>Incremental updates: Append operation for database evolution</li> <li>Composite pipelines: <code>koza merge</code> handles join \u2192 deduplicate \u2192 normalize \u2192 prune in one command</li> <li>SSSOM normalization: Built-in support for identifier mapping via SSSOM files</li> <li>Comprehensive reporting: QC reports, graph stats, schema analysis, and example extraction</li> </ol>"},{"location":"Ingests/","title":"Configuring an Ingest","text":"<p><sub> (For CLI usage, see the CLI commands page.) </sub> </p> <p>Koza is designed to process and transform existing data into a target csv/json/jsonl format.  </p> <p>This process is internally known as an ingest. Ingests are defined by:  </p> <ol> <li>Koza config file yaml: Koza ingest configuration, including:<ul> <li>metadata, formats, required columns, any SSSOM files, etc. </li> </ul> </li> <li>Map config yaml: (Optional) configures creation of mapping dictionary  </li> <li>Transform code: a Python script, with specific transform instructions </li> </ol>"},{"location":"Ingests/koza_config/","title":"Koza Configuration (KozaConfig)","text":"<p>This document describes the KozaConfig model introduced in Koza 2, which replaces the previous SourceConfig structure. The KozaConfig provides a comprehensive configuration system for data ingests with support for multiple readers, transforms, and writers.</p> <p>Paths are relative to the directory from which you execute Koza.</p>"},{"location":"Ingests/koza_config/#overview","title":"Overview","text":"<p>KozaConfig is the main configuration class that defines how Koza processes your data. It consists of several main sections:</p> <ul> <li>name: Unique identifier for your ingest</li> <li>reader/readers: Configuration for input data sources  </li> <li>transform: Configuration for data transformation logic</li> <li>writer: Configuration for output format and properties</li> <li>metadata: Optional metadata about the dataset</li> </ul>"},{"location":"Ingests/koza_config/#basic-structure","title":"Basic Structure","text":"<pre><code>name: 'my-ingest'\nreader: # OR readers: for multiple sources\n  # reader configuration\ntransform:\n  # transformation configuration  \nwriter:\n  # output configuration\nmetadata: # optional\n  # metadata configuration\n</code></pre>"},{"location":"Ingests/koza_config/#core-configuration-properties","title":"Core Configuration Properties","text":""},{"location":"Ingests/koza_config/#required-properties","title":"Required Properties","text":"Property Type Description <code>name</code> string Name of the data ingest, should be unique and descriptive"},{"location":"Ingests/koza_config/#optional-properties","title":"Optional Properties","text":"Property Type Description <code>reader</code> ReaderConfig Single reader configuration (mutually exclusive with <code>readers</code>) <code>readers</code> dict[str, ReaderConfig] Named multiple readers (mutually exclusive with <code>reader</code>) <code>transform</code> TransformConfig Transform configuration (optional, uses defaults if not specified) <code>writer</code> WriterConfig Writer configuration (optional, uses defaults if not specified) <code>metadata</code> DatasetDescription | string Dataset metadata or path to metadata file"},{"location":"Ingests/koza_config/#reader-configuration","title":"Reader Configuration","text":"<p>Readers define how Koza processes input data files. You can use either a single <code>reader</code> or multiple named <code>readers</code>.</p>"},{"location":"Ingests/koza_config/#single-reader-example","title":"Single Reader Example","text":"<pre><code>reader:\n  format: csv\n  files:\n    - 'data/input.tsv'\n  delimiter: '\\t'\n</code></pre>"},{"location":"Ingests/koza_config/#multiple-readers-example","title":"Multiple Readers Example","text":"<pre><code>readers:\n  main_data:\n    format: csv\n    files:\n      - 'data/main.tsv'\n    delimiter: '\\t'\n  reference_data:\n    format: json\n    files:\n      - 'data/reference.json'\n</code></pre>"},{"location":"Ingests/koza_config/#base-reader-properties","title":"Base Reader Properties","text":"<p>All reader types support these common properties:</p> Property Type Description <code>files</code> list[string] List of input files to process <code>filters</code> list[ColumnFilter] List of filters to apply to data"},{"location":"Ingests/koza_config/#csv-reader-configuration","title":"CSV Reader Configuration","text":"<p>For CSV format files (<code>format: csv</code>):</p> Property Type Default Description <code>format</code> string <code>csv</code> Must be \"csv\" <code>columns</code> list[string | dict] None Column names or name/type mappings <code>field_type_map</code> dict[string, FieldType] None Mapping of column names to types <code>delimiter</code> string <code>\\t</code> Field delimiter (supports \"tab\", \"\\t\", or literal chars) <code>header_delimiter</code> string None Different delimiter for header row <code>dialect</code> string <code>excel</code> CSV dialect <code>header_mode</code> int | HeaderMode <code>infer</code> Header handling: int (0-based row), \"infer\", or \"none\" <code>header_prefix</code> string None Prefix for header processing <code>skip_blank_lines</code> bool <code>true</code> Whether to skip blank lines <code>comment_char</code> string <code>#</code> Character that indicates comments"},{"location":"Ingests/koza_config/#field-types","title":"Field Types","text":"<ul> <li><code>str</code> - String type (default)</li> <li><code>int</code> - Integer type  </li> <li><code>float</code> - Float type</li> </ul>"},{"location":"Ingests/koza_config/#header-modes","title":"Header Modes","text":"<ul> <li><code>infer</code> - Automatically detect header row</li> <li><code>none</code> - No header row present</li> <li>Integer (0-based) - Specific header row index</li> </ul>"},{"location":"Ingests/koza_config/#column-definition-examples","title":"Column Definition Examples","text":"<pre><code># Simple string columns\ncolumns:\n  - 'gene_id'\n  - 'symbol'\n  - 'score'\n\n# Mixed types  \ncolumns:\n  - 'gene_id'\n  - 'symbol' \n  - 'score': 'int'\n  - 'p_value': 'float'\n</code></pre>"},{"location":"Ingests/koza_config/#json-reader-configuration","title":"JSON Reader Configuration","text":"<p>For JSON format files (<code>format: json</code>):</p> Property Type Description <code>format</code> string Must be \"json\" <code>required_properties</code> list[string] Properties that must be present <code>json_path</code> list[string | int] Path to data within JSON structure"},{"location":"Ingests/koza_config/#jsonl-reader-configuration","title":"JSONL Reader Configuration","text":"<p>For JSON Lines format files (<code>format: jsonl</code>):</p> Property Type Description <code>format</code> string Must be \"jsonl\" <code>required_properties</code> list[string] Properties that must be present"},{"location":"Ingests/koza_config/#yaml-reader-configuration","title":"YAML Reader Configuration","text":"<p>For YAML format files (<code>format: yaml</code>):</p> Property Type Description <code>format</code> string Must be \"yaml\" <code>required_properties</code> list[string] Properties that must be present <code>json_path</code> list[string | int] Path to data within YAML structure"},{"location":"Ingests/koza_config/#column-filters","title":"Column Filters","text":"<p>Filters allow you to include or exclude rows based on column values.</p>"},{"location":"Ingests/koza_config/#filter-types","title":"Filter Types","text":""},{"location":"Ingests/koza_config/#comparison-filters","title":"Comparison Filters","text":"<p>For numeric comparisons:</p> <pre><code>filters:\n  - inclusion: 'include'  # or 'exclude'\n    column: 'score'\n    filter_code: 'gt'     # gt, ge, lt, le  \n    value: 500\n</code></pre>"},{"location":"Ingests/koza_config/#equality-filters","title":"Equality Filters","text":"<p>For exact matches:</p> <pre><code>filters:\n  - inclusion: 'include'\n    column: 'status'\n    filter_code: 'eq'     # eq, ne\n    value: 'active'\n</code></pre>"},{"location":"Ingests/koza_config/#list-filters","title":"List Filters","text":"<p>For checking membership in lists:</p> <pre><code>filters:\n  - inclusion: 'include'\n    column: 'category'\n    filter_code: 'in'     # in, in_exact\n    value: ['A', 'B', 'C']\n</code></pre>"},{"location":"Ingests/koza_config/#filter-codes","title":"Filter Codes","text":"<ul> <li><code>gt</code> - Greater than</li> <li><code>ge</code> - Greater than or equal  </li> <li><code>lt</code> - Less than</li> <li><code>le</code> - Less than or equal</li> <li><code>eq</code> - Equal to</li> <li><code>ne</code> - Not equal to</li> <li><code>in</code> - In list (case insensitive)</li> <li><code>in_exact</code> - In list (exact match)</li> </ul>"},{"location":"Ingests/koza_config/#transform-configuration","title":"Transform Configuration","text":"<p>The transform section configures how data is processed and transformed.</p> Property Type Default Description <code>code</code> string None Path to Python transform file <code>module</code> string None Python module to import <code>global_table</code> string | dict None Global translation table <code>local_table</code> string | dict None Local translation table <code>mappings</code> list[string] <code>[]</code> List of mapping files <code>on_map_failure</code> MapErrorEnum <code>warning</code> How to handle mapping failures <code>extra_fields</code> dict <code>{}</code> Additional custom fields"},{"location":"Ingests/koza_config/#map-error-handling","title":"Map Error Handling","text":"<ul> <li><code>warning</code> - Log warnings for mapping failures</li> <li><code>error</code> - Raise errors for mapping failures</li> </ul>"},{"location":"Ingests/koza_config/#example-transform-configuration","title":"Example Transform Configuration","text":"<pre><code>transform:\n  code: 'transform.py'\n  global_table: 'tables/global_mappings.yaml'\n  local_table: 'tables/local_mappings.yaml'\n  mappings:\n    - 'mappings/gene_mappings.yaml'\n  on_map_failure: 'warning'\n  custom_param: 'value'  # Goes into extra_fields\n</code></pre>"},{"location":"Ingests/koza_config/#writer-configuration","title":"Writer Configuration","text":"<p>The writer section configures output format and properties.</p> Property Type Default Description <code>format</code> OutputFormat <code>tsv</code> Output format <code>sssom_config</code> SSSOMConfig None SSSOM mapping configuration <code>node_properties</code> list[string] None Node properties to include <code>edge_properties</code> list[string] None Edge properties to include <code>min_node_count</code> int None Minimum nodes required <code>min_edge_count</code> int None Minimum edges required <code>max_node_count</code> int None Maximum nodes allowed <code>max_edge_count</code> int None Maximum edges allowed"},{"location":"Ingests/koza_config/#output-formats","title":"Output Formats","text":"<ul> <li><code>tsv</code> - Tab-separated values</li> <li><code>jsonl</code> - JSON Lines  </li> <li><code>kgx</code> - KGX format</li> <li><code>passthrough</code> - Pass data through unchanged</li> </ul>"},{"location":"Ingests/koza_config/#example-writer-configuration","title":"Example Writer Configuration","text":"<pre><code>writer:\n  format: tsv\n  node_properties:\n    - 'id'\n    - 'category'\n    - 'name'\n  edge_properties:\n    - 'id'\n    - 'subject'\n    - 'predicate'\n    - 'object'\n    - 'category'\n</code></pre>"},{"location":"Ingests/koza_config/#sssom-configuration","title":"SSSOM Configuration","text":"<p>SSSOM (Simple Standard for Sharing Ontological Mappings) integration:</p> Property Type Description <code>files</code> list[string] SSSOM mapping files <code>filter_prefixes</code> list[string] Prefixes to filter by <code>subject_target_prefixes</code> list[string] Subject mapping prefixes <code>object_target_prefixes</code> list[string] Object mapping prefixes <code>use_match</code> list[Match] Match types to use"},{"location":"Ingests/koza_config/#match-types","title":"Match Types","text":"<ul> <li><code>exact</code> - Exact matches</li> <li><code>narrow</code> - Narrow matches  </li> <li><code>broad</code> - Broad matches</li> </ul> <pre><code>writer:\n  sssom_config:\n    files:\n      - 'mappings/ontology_mappings.sssom.tsv'\n    subject_target_prefixes: ['MONDO']\n    object_target_prefixes: ['HP', 'GO']\n    use_match: ['exact']\n</code></pre>"},{"location":"Ingests/koza_config/#metadata-configuration","title":"Metadata Configuration","text":"<p>Metadata can be defined inline or loaded from a separate file.</p>"},{"location":"Ingests/koza_config/#inline-metadata","title":"Inline Metadata","text":"<pre><code>metadata:\n  name: 'My Data Source'\n  description: 'Description of the data and processing'\n  ingest_title: 'Source Database Name'\n  ingest_url: 'https://source-database.org'\n  provided_by: 'my_source_gene_disease'\n  rights: 'https://source-database.org/license'\n</code></pre>"},{"location":"Ingests/koza_config/#external-metadata-file","title":"External Metadata File","text":"<pre><code>metadata: './metadata.yaml'\n</code></pre>"},{"location":"Ingests/koza_config/#metadata-properties","title":"Metadata Properties","text":"Property Type Description <code>name</code> string Human-readable name of data source <code>ingest_title</code> string Title of data source (maps to biolink name) <code>ingest_url</code> string URL of data source (maps to biolink iri) <code>description</code> string Description of data/ingest process <code>provided_by</code> string Source identifier, format: <code>&lt;source&gt;_&lt;type&gt;</code> <code>rights</code> string License/rights information URL"},{"location":"Ingests/koza_config/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example showing all major configuration options:</p> <pre><code>name: 'comprehensive-example'\n\nmetadata:\n  name: 'Example Database'\n  description: 'Comprehensive example of Koza configuration'\n  ingest_title: 'Example DB'\n  ingest_url: 'https://example-db.org'\n  provided_by: 'example_gene_disease'\n  rights: 'https://example-db.org/license'\n\nreader:\n  format: csv\n  files:\n    - 'data/genes.tsv'\n    - 'data/diseases.tsv'\n  delimiter: '\\t'\n  columns:\n    - 'gene_id'\n    - 'gene_symbol'\n    - 'disease_id'\n    - 'confidence': 'float'\n  filters:\n    - inclusion: 'include'\n      column: 'confidence'\n      filter_code: 'ge'\n      value: 0.7\n    - inclusion: 'exclude'\n      column: 'gene_symbol'\n      filter_code: 'eq'\n      value: 'DEPRECATED'\n\ntransform:\n  code: 'transform.py'\n  global_table: 'tables/global_mappings.yaml'\n  on_map_failure: 'warning'\n\nwriter:\n  format: tsv\n  node_properties:\n    - 'id'\n    - 'category'\n    - 'name'\n    - 'provided_by'\n  edge_properties:\n    - 'id'\n    - 'subject'\n    - 'predicate'\n    - 'object'\n    - 'category'\n    - 'provided_by'\n    - 'confidence'\n  min_node_count: 100\n  min_edge_count: 50\n</code></pre>"},{"location":"Ingests/koza_config/#migration-from-sourceconfig","title":"Migration from SourceConfig","text":"<p>If you're migrating from the old SourceConfig format to KozaConfig:</p> <ol> <li>Structure Changes: </li> <li>Top-level properties are now organized under <code>reader</code>, <code>transform</code>, and <code>writer</code> sections</li> <li><code>files</code> moves to <code>reader.files</code></li> <li>Transform-related properties move to <code>transform</code> section</li> <li> <p>Output properties move to <code>writer</code> section</p> </li> <li> <p>Property Mapping:</p> </li> <li><code>transform_code</code> \u2192 <code>transform.code</code></li> <li><code>global_table</code> \u2192 <code>transform.global_table</code> </li> <li><code>local_table</code> \u2192 <code>transform.local_table</code></li> <li><code>node_properties</code> \u2192 <code>writer.node_properties</code></li> <li> <p><code>edge_properties</code> \u2192 <code>writer.edge_properties</code></p> </li> <li> <p>New Features:</p> </li> <li>Multiple readers support with <code>readers</code></li> <li>Enhanced filter system with more comparison operators</li> <li>SSSOM integration in writer</li> <li>Improved metadata handling</li> </ol> <p>Next Steps: Transform Code</p>"},{"location":"Ingests/mapping/","title":"Mapping","text":"<p>Mapping with Koza is optional, but can be done in two ways:  </p> <ul> <li>Automated mapping with SSSOM files  </li> <li>Manual mapping with a map config yaml</li> </ul>"},{"location":"Ingests/mapping/#sssom-mapping","title":"SSSOM Mapping","text":"<p>Koza supports mapping with SSSOM files (Semantic Similarity of Source and Target Ontology Mappings). Simply add the path to the SSSOM file to your source config, the desired target prefixes, and any prefixes you want to use to filter the SSSOM file. Koza will automatically create a mapping lookup table which will automatically attempt to map any values in the source file to an ID with the target prefix.</p> <pre><code>sssom_config:\n    sssom_file: './path/to/your_mapping_file.sssom.tsv'\n    filter_prefixes: \n        - 'SOMEPREFIX'\n        - 'OTHERPREFIX'\n    target_prefixes: \n        - 'OTHERPREFIX'\n    use_match:\n        - 'exact'\n</code></pre> <p>Note: Currently, only the <code>exact</code> match type is supported (<code>narrow</code> and <code>broad</code> match types will be added in the future).</p>"},{"location":"Ingests/mapping/#manual-mapping-additional-data","title":"Manual Mapping / Additional Data","text":"<p>The map config yaml allows you to include data from other sources in your ingests, which may have different columns or formats.  </p> <p>If you don't have an SSSOM file, or you want to manually map some values, you can use a map config yaml. You can then add this map to your source config yaml in the <code>depends_on</code> property.  </p> <p>Koza will then create a nested dictionary with the specified key and values. For example, the following map config yaml maps values from the <code>STRING</code> column to the <code>entrez</code> and <code>NCBI taxid</code> columns.</p> <pre><code># koza/examples/maps/entrez-2-string.yaml\nname: ...\nfiles: ...\n\ncolumns:\n- 'NCBI taxid'\n- 'entrez'\n- 'STRING'\n\nkey: 'STRING'\n\nvalues:\n- 'entrez'\n- 'NCBI taxid'\n</code></pre> <p>The mapping dict will be available in your transform script from the <code>koza_app</code> object (see the Transform Code section below).</p> <p>Next Steps: Transform Code</p>"},{"location":"Ingests/testing/","title":"Testing","text":"<p>Koza includes a <code>mock_koza</code> fixture (see <code>src/koza/utils/testing_utils</code>) that can be used to test your ingest configuration. This fixture accepts the following arguments:</p> Argument Type Description Required Arguments <code>name</code> <code>str</code> The name of the ingest <code>data</code> <code>Union[Dict, List[Dict]]</code> The data to be ingested <code>transform_code</code> <code>str</code> Path to the transform code to be used Optional Arguments <code>map_cache</code> <code>Dict</code> Map cache to be used <code>filters</code> <code>List(str)</code> List of filters to apply to data <code>global_table</code> <code>str</code> Path to the global table <code>local_table</code> <code>str</code> Path to the local table <p>The <code>mock_koza</code> fixture returns a list of entities that would be generated by the ingest configuration. This list can then be used to test the output based on the transform script.</p> <p>Here is an example of how to use the <code>mock_koza</code> fixture to test an ingest configuration:</p> <pre><code>import pytest\n\nfrom koza.utils.testing_utils import mock_koza\n\n# Define the source name and transform script path\nINGEST_NAME = \"your_ingest_name\"\nTRANSFORM_SCRIPT = \"./src/{{cookiecutter.__project_slug}}/transform.py\"\n\n# Define an example row to test (as a dictionary)\n@pytest.fixture\ndef example_row():\n    return {\n        \"example_column_1\": \"entity_1\",\n        \"example_column_2\": \"entity_6\",\n        \"example_column_3\": \"biolink:related_to\",\n    }\n\n# Or a list of rows\n@pytest.fixture\ndef example_list_of_rows():\n    return [\n        {\n            \"example_column_1\": \"entity_1\",\n            \"example_column_2\": \"entity_6\",\n            \"example_column_3\": \"biolink:related_to\",\n        },\n        {\n            \"example_column_1\": \"entity_2\",\n            \"example_column_2\": \"entity_7\",\n            \"example_column_3\": \"biolink:related_to\",\n        },\n    ]\n\n# Define the mock koza transform\n@pytest.fixture\ndef mock_transform(mock_koza, example_row):\n    return mock_koza(\n        INGEST_NAME,\n        example_row,\n        TRANSFORM_SCRIPT,\n    )\n\n# Or for multiple rows\n@pytest.fixture\ndef mock_transform_multiple_rows(mock_koza, example_list_of_rows):\n    return mock_koza(\n        INGEST_NAME,\n        example_list_of_rows,\n        TRANSFORM_SCRIPT,\n    )\n\n# Test the output of the transform\n\ndef test_single_row(mock_transform):\n    assert len(mock_transform) == 1\n    entity = mock_transform[0]\n    assert entity\n    assert entity.subject == \"entity_1\"\n\n\ndef test_multiple_rows(mock_transform_multiple_rows):\n    assert len(mock_transform_multiple_rows) == 2\n    entity_1 = mock_transform_multiple_rows[0]\n    entity_2 = mock_transform_multiple_rows[1]\n    assert entity_1.subject == \"entity_1\"\n    assert entity_2.subject == \"entity_2\"\n</code></pre>"},{"location":"Ingests/transform/","title":"Transform Code","text":"<p>This Python script is where you'll define the specific steps of your data transformation. Koza will load this script and execute it for each row of data in your source file, applying any filters and mapping as defined in your source config yaml, and outputting the transformed data to the target csv/json/jsonl file.</p> <p>When Koza is called, either by command-line or as a library, it creates a <code>KozaTransform</code> object for the specified ingest. This KozaTransform will be your entry point to Koza and is available as a global variable in your transform code.</p> <p>The KozaTransform object has the following methods which can be used in your transform code:</p> Method Description <code>write(*args)</code> Writes the transformed data to the target file <p>Your transform code should define functions decorated with Koza decorators that process the data:</p> <p>Once you have processed a row of data, and created a biolink entity node or edge object (or both), you can pass these to <code>koza.write()</code> to output the transformed data to the target file.</p> Example Python Transform Script <pre><code>import re\nimport uuid\nfrom typing import Any\nfrom biolink_model.datamodel.pydanticmodel_v2 import PairwiseGeneToGeneInteraction, Protein\n\nimport koza\n\n@koza.transform_record()\ndef transform_record(koza: koza.KozaTransform, record: dict[str, Any]):\n    # Process the record data\n    protein_a = Protein(id=\"ENSEMBL:\" + re.sub(r\"\\d+\\.\", \"\", record[\"protein1\"]))\n    protein_b = Protein(id=\"ENSEMBL:\" + re.sub(r\"\\d+\\.\", \"\", record[\"protein2\"]))\n\n    # Create interaction\n    pairwise_gene_to_gene_interaction = PairwiseGeneToGeneInteraction(\n        id=\"uuid:\" + str(uuid.uuid1()),\n        subject=protein_a.id,\n        object=protein_b.id,\n        predicate=\"biolink:interacts_with\",\n        knowledge_level=\"not_provided\",\n        agent_type=\"not_provided\",\n    )\n\n    # Write the transformed data\n    koza.write(protein_a, protein_b, pairwise_gene_to_gene_interaction)\n</code></pre> <p>The <code>@koza.transform_record()</code> decorator indicates that this function processes individual records. If you pass nodes as well as edges to <code>koza.write()</code>, Koza will automatically create a node file and an edge file. If you pass only nodes, Koza will create only a node file, and if you pass only edges, Koza will create only an edge file.</p>"},{"location":"Usage/CLI/","title":"<code>koza</code>","text":"<p>Usage:</p> <pre><code>$ koza [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--version</code></li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>transform</code>: Transform a source file</li> <li><code>join</code>: Join multiple KGX files into a unified...</li> <li><code>split</code>: Split a KGX file by specified fields with...</li> <li><code>prune</code>: Prune graph by removing dangling edges and...</li> <li><code>append</code>: Append new KGX files to an existing graph...</li> <li><code>normalize</code>: Apply SSSOM mappings to normalize edge...</li> <li><code>merge</code>: Complete merge pipeline: join \u2192 normalize...</li> <li><code>report</code>: Generate comprehensive reports for KGX...</li> <li><code>node-report</code>: Generate tabular node report with GROUP BY...</li> <li><code>edge-report</code>: Generate tabular edge report with...</li> <li><code>node-examples</code>: Generate sample rows per node type.</li> <li><code>edge-examples</code>: Generate sample rows per edge type.</li> </ul>"},{"location":"Usage/CLI/#koza-transform","title":"<code>koza transform</code>","text":"<p>Transform a source file</p> <p>Usage:</p> <pre><code>$ koza transform [OPTIONS] CONFIGURATION_YAML\n</code></pre> <p>Arguments:</p> <ul> <li><code>CONFIGURATION_YAML</code>: Configuration YAML file  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-i, --input-file TEXT</code>: Override input files</li> <li><code>-o, --output-dir TEXT</code>: Path to output directory  [default: ./output]</li> <li><code>-f, --output-format [tsv|jsonl|kgx|passthrough]</code>: Output format  [default: tsv]</li> <li><code>-n, --limit INTEGER</code>: Number of rows to process (if skipped, processes entire source file)  [default: 0]</li> <li><code>-p, --progress</code>: Display progress of transform</li> <li><code>-q, --quiet</code>: Disable log output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-join","title":"<code>koza join</code>","text":"<p>Join multiple KGX files into a unified DuckDB database</p> <p>Examples:     # Auto-discover files in directory     koza join --input-dir tmp/ -o graph.duckdb</p> <pre><code># Use glob patterns\nkoza join -n &amp;quot;tmp/*_nodes.tsv&amp;quot; -e &amp;quot;tmp/*_edges.tsv&amp;quot; -o graph.duckdb\n\n# Mix directory discovery with additional files\nkoza join --input-dir tmp/ -n extra_nodes.tsv -o graph.duckdb\n\n# Multiple individual files\nkoza join -n file1.tsv -n file2.tsv -e edges.tsv -o graph.duckdb\n</code></pre> <p>Usage:</p> <pre><code>$ koza join [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-n, --nodes TEXT</code>: Node files or glob patterns (can specify multiple)</li> <li><code>-e, --edges TEXT</code>: Edge files or glob patterns (can specify multiple)</li> <li><code>-d, --input-dir TEXT</code>: Directory to auto-discover KGX files</li> <li><code>-o, --output TEXT</code>: Path to output database file (default: in-memory)</li> <li><code>-f, --format [tsv|jsonl|parquet]</code>: Output format for any exported files  [default: tsv]</li> <li><code>--schema-report</code>: Generate schema compliance report</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-split","title":"<code>koza split</code>","text":"<p>Split a KGX file by specified fields with format conversion support</p> <p>Usage:</p> <pre><code>$ koza split [OPTIONS] FILE FIELDS\n</code></pre> <p>Arguments:</p> <ul> <li><code>FILE</code>: Path to the KGX file to split  [required]</li> <li><code>FIELDS</code>: Comma-separated list of fields to split on  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-o, --output-dir TEXT</code>: Output directory for split files  [default: ./output]</li> <li><code>-f, --format [tsv|jsonl|parquet]</code>: Output format (default: preserve input format)</li> <li><code>--remove-prefixes</code>: Remove prefixes from values in filenames</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-prune","title":"<code>koza prune</code>","text":"<p>Prune graph by removing dangling edges and handling singleton nodes</p> <p>Examples:     # Keep singleton nodes, move dangling edges     koza prune graph.duckdb</p> <pre><code># Remove singleton nodes to separate table\nkoza prune graph.duckdb --remove-singletons\n\n# Experimental: filter small components\nkoza prune graph.duckdb --min-component-size 10\n</code></pre> <p>Usage:</p> <pre><code>$ koza prune [OPTIONS] DATABASE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DATABASE</code>: Path to the DuckDB database file to prune  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--keep-singletons</code>: Keep singleton nodes in main table</li> <li><code>--remove-singletons</code>: Move singleton nodes to separate table</li> <li><code>--min-component-size INTEGER</code>: Minimum connected component size (experimental)</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-append","title":"<code>koza append</code>","text":"<p>Append new KGX files to an existing graph database</p> <p>Examples:     # Append specific files to existing database     koza append graph.duckdb -n new_nodes.tsv -e new_edges.tsv</p> <pre><code># Auto-discover files in directory and append\nkoza append graph.duckdb --input-dir new_data/\n\n# Append with deduplication and schema reporting\nkoza append graph.duckdb -n &amp;quot;*.tsv&amp;quot; --deduplicate --schema-report\n</code></pre> <p>Usage:</p> <pre><code>$ koza append [OPTIONS] DATABASE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DATABASE</code>: Path to existing DuckDB database file  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-n, --nodes TEXT</code>: Node files or glob patterns (can specify multiple)</li> <li><code>-e, --edges TEXT</code>: Edge files or glob patterns (can specify multiple)</li> <li><code>-d, --input-dir TEXT</code>: Directory to auto-discover KGX files</li> <li><code>--deduplicate</code>: Remove duplicates during append</li> <li><code>--schema-report</code>: Generate schema compliance report</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-normalize","title":"<code>koza normalize</code>","text":"<p>Apply SSSOM mappings to normalize edge subject/object references</p> <p>This operation loads SSSOM mapping files and applies them to rewrite edge subject and object identifiers to their canonical/equivalent forms. Node identifiers themselves are not changed - only edge references are normalized.</p> <p>Examples:     # Apply specific mapping files     koza normalize graph.duckdb -m gene_mappings.sssom.tsv -m mondo.sssom.tsv</p> <pre><code># Auto-discover SSSOM files in directory\nkoza normalize graph.duckdb --mappings-dir ./sssom/\n\n# Apply mappings with glob pattern\nkoza normalize graph.duckdb -m &amp;quot;*.sssom.tsv&amp;quot;\n</code></pre> <p>Usage:</p> <pre><code>$ koza normalize [OPTIONS] DATABASE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DATABASE</code>: Path to existing DuckDB database file  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-m, --mappings TEXT</code>: SSSOM mapping files or glob patterns (can specify multiple)</li> <li><code>-d, --mappings-dir TEXT</code>: Directory containing SSSOM mapping files</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-merge","title":"<code>koza merge</code>","text":"<p>Complete merge pipeline: join \u2192 normalize \u2192 prune</p> <p>This composite operation orchestrates the full graph processing pipeline: 1. Join: Load and combine multiple KGX files into a unified database 2. Normalize: Apply SSSOM mappings to edge subject/object references 3. Prune: Remove dangling edges and handle singleton nodes</p> <p>The pipeline can be customized by skipping steps or configuring options.</p> <p>Examples:     # Full pipeline with auto-discovery     koza merge --input-dir ./data/ --mappings-dir ./sssom/ -o clean_graph.duckdb</p> <pre><code># Specific files with export\nkoza merge -n nodes.tsv -e edges.tsv -m mappings.sssom.tsv --export --export-dir ./output/\n\n# Skip normalization, only join and prune\nkoza merge -n &amp;quot;*.tsv&amp;quot; -e &amp;quot;*.tsv&amp;quot; --skip-normalize -o graph.duckdb\n\n# Custom singleton handling\nkoza merge --input-dir ./data/ -m &amp;quot;*.sssom.tsv&amp;quot; --remove-singletons\n</code></pre> <p>Usage:</p> <pre><code>$ koza merge [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-n, --nodes TEXT</code>: Node files or glob patterns (can specify multiple)</li> <li><code>-e, --edges TEXT</code>: Edge files or glob patterns (can specify multiple)</li> <li><code>-m, --mappings TEXT</code>: SSSOM mapping files or glob patterns (can specify multiple)</li> <li><code>-d, --input-dir TEXT</code>: Directory to auto-discover KGX files</li> <li><code>--mappings-dir TEXT</code>: Directory containing SSSOM mapping files</li> <li><code>-o, --output TEXT</code>: Path to output database file (default: temporary)</li> <li><code>--export</code>: Export final clean data to files</li> <li><code>--export-dir TEXT</code>: Directory for exported files (required if --export)</li> <li><code>-f, --format [tsv|jsonl|parquet]</code>: Output format for exported files  [default: tsv]</li> <li><code>--archive</code>: Export as archive (tar) instead of loose files</li> <li><code>--compress</code>: Compress archive as tar.gz (requires --archive)</li> <li><code>--graph-name TEXT</code>: Name for graph files in archive (default: merged_graph)</li> <li><code>--skip-normalize</code>: Skip normalization step</li> <li><code>--skip-prune</code>: Skip pruning step</li> <li><code>--keep-singletons</code>: Keep singleton nodes (default)  [default: True]</li> <li><code>--remove-singletons</code>: Move singleton nodes to separate table</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-report","title":"<code>koza report</code>","text":"<p>Generate comprehensive reports for KGX graph databases.</p> <p>Available report types:</p> <p>\u2022 qc: Quality control analysis by data source</p> <p>\u2022 graph-stats: Comprehensive graph statistics (similar to merged_graph_stats.yaml)</p> <p>\u2022 schema: Database schema analysis and biolink compliance</p> <p>Examples:</p> <pre><code># Generate QC report\nkoza report qc -d merged.duckdb -o qc_report.yaml\n\n# Generate graph statistics\nkoza report graph-stats -d merged.duckdb -o graph_stats.yaml\n\n# Generate schema report\nkoza report schema -d merged.duckdb -o schema_report.yaml\n\n# Quick QC analysis (console output only)\nkoza report qc -d merged.duckdb\n</code></pre> <p>Usage:</p> <pre><code>$ koza report [OPTIONS] REPORT_TYPE\n</code></pre> <p>Arguments:</p> <ul> <li><code>REPORT_TYPE</code>: Type of report to generate: qc, graph-stats, or schema  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file  [required]</li> <li><code>-o, --output TEXT</code>: Path to output report file (YAML format)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-node-report","title":"<code>koza node-report</code>","text":"<p>Generate tabular node report with GROUP BY ALL categorical columns.</p> <p>Outputs count of nodes grouped by categorical columns (namespace, category, etc.).</p> <p>Examples:</p> <pre><code># From database\nkoza node-report -d merged.duckdb -o node_report.tsv\n\n# From file\nkoza node-report -f nodes.tsv -o node_report.parquet --format parquet\n\n# Custom columns\nkoza node-report -d merged.duckdb -o report.tsv -c namespace -c category -c provided_by\n</code></pre> <p>Usage:</p> <pre><code>$ koza node-report [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-f, --file TEXT</code>: Path to node file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output report file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-c, --column TEXT</code>: Categorical columns to group by (can specify multiple)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-edge-report","title":"<code>koza edge-report</code>","text":"<p>Generate tabular edge report with denormalized node info.</p> <p>Joins edges to nodes to get subject_category, object_category, etc., then outputs count of edges grouped by categorical columns.</p> <p>Examples:</p> <pre><code># From database\nkoza edge-report -d merged.duckdb -o edge_report.tsv\n\n# From files\nkoza edge-report -n nodes.tsv -e edges.tsv -o edge_report.parquet --format parquet\n\n# Custom columns\nkoza edge-report -d merged.duckdb -o report.tsv \\\n    -c subject_category -c predicate -c object_category -c primary_knowledge_source\n</code></pre> <p>Usage:</p> <pre><code>$ koza edge-report [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-n, --nodes TEXT</code>: Path to node file (for denormalization)</li> <li><code>-e, --edges TEXT</code>: Path to edge file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output report file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-c, --column TEXT</code>: Categorical columns to group by (can specify multiple)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-node-examples","title":"<code>koza node-examples</code>","text":"<p>Generate sample rows per node type.</p> <p>Samples N example rows for each distinct value in the type column (default: category).</p> <p>Examples:</p> <pre><code># From database (5 examples per category)\nkoza node-examples -d merged.duckdb -o node_examples.tsv\n\n# From file with 10 examples per type\nkoza node-examples -f nodes.tsv -o examples.tsv -n 10\n\n# Group by different column\nkoza node-examples -d merged.duckdb -o examples.tsv -t provided_by\n</code></pre> <p>Usage:</p> <pre><code>$ koza node-examples [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-f, --file TEXT</code>: Path to node file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output examples file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-n, --sample-size INTEGER</code>: Number of examples per type  [default: 5]</li> <li><code>-t, --type-column TEXT</code>: Column to partition examples by  [default: category]</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-edge-examples","title":"<code>koza edge-examples</code>","text":"<p>Generate sample rows per edge type.</p> <p>Samples N example rows for each distinct combination of type columns (default: subject_category, predicate, object_category).</p> <p>Examples:</p> <pre><code># From database (5 examples per edge type)\nkoza edge-examples -d merged.duckdb -o edge_examples.tsv\n\n# From files with 10 examples\nkoza edge-examples -n nodes.tsv -e edges.tsv -o examples.tsv -s 10\n\n# Custom type columns\nkoza edge-examples -d merged.duckdb -o examples.tsv -t predicate -t primary_knowledge_source\n</code></pre> <p>Usage:</p> <pre><code>$ koza edge-examples [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-n, --nodes TEXT</code>: Path to node file (for denormalization)</li> <li><code>-e, --edges TEXT</code>: Path to edge file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output examples file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-s, --sample-size INTEGER</code>: Number of examples per type  [default: 5]</li> <li><code>-t, --type-column TEXT</code>: Columns to partition examples by (can specify multiple)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/Module/","title":"Module","text":""}]}