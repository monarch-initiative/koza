{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Koza","text":""},{"location":"#a-data-transformation-framework-in-python","title":"A data transformation framework in Python","text":""},{"location":"#overview","title":"Overview","text":"<p>Koza is a data transformation framework which allows you to write semi-declarative \"ingests\"</p> <ul> <li>Transform csv, json, yaml, jsonl, or xml source data, converting them to a target csv, json, or jsonl format based on your dataclass model.</li> <li>Koza also can output data in the KGX format</li> <li>Write data transforms in semi-declarative Python</li> <li>Configure source files, expected columns/json properties and path filters, field filters, and metadata in yaml</li> <li>Create or import mapping files to be used in ingests (eg. id mapping, type mappings)</li> <li>Create and use translation tables to map between source and target vocabularies</li> </ul> <p>Koza also provides Graph Operations for working with KGX-formatted knowledge graphs:</p> <ul> <li>Join multiple node and edge files into a unified graph database</li> <li>Split graphs by field values (e.g., by data source)</li> <li>Normalize identifiers using SSSOM mappings</li> <li>Prune dangling edges and deduplicate records</li> <li>Merge complete pipelines with a single command</li> <li>Report on graph statistics and quality metrics</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Koza is available on PyPi and can be installed via pip: <pre><code>pip install koza\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":"<p>See the Ingests page for information on how to configure ingests for koza to use.</p> <p>Koza can be used as a Python library, or via the command line. CLI commands are available for validating and transforming data. See the Module page for information on using Koza as a library.</p> <p>Koza also includes some examples to help you get started (see <code>koza/examples</code>).</p>"},{"location":"#basic-examples","title":"Basic Examples","text":"<p>Validate</p> <p>Give Koza a local or remote csv file, and get some basic information (headers, number of rows)</p> <pre><code>koza validate \\\n  --file ./examples/data/string.tsv \\\n  --delimiter ' '\n</code></pre> <p>Sending a json or jsonl formatted file will confirm if the file is valid json or jsonl</p> <pre><code>koza validate \\\n  --file ./examples/data/ZFIN_PHENOTYPE_0.jsonl.gz \\\n  --format jsonl\n</code></pre> <pre><code>koza validate \\\n  --file ./examples/data/ddpheno.json.gz \\\n  --format json\n</code></pre> <p>Transform</p> <p>Try one of Koza's example ingests: <pre><code>koza transform \\\n  --source examples/string-declarative/protein-links-detailed.yaml \\\n  --global-table examples/translation_table.yaml\n</code></pre></p> <p>Note:    Koza expects a directory structure as described in the above example   with the source config file and transform code in the same directory   (these files can also simply be named <code>transform.yaml</code> and <code>transform.py</code>, as is default):    <pre><code>.\n\u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 some_source\n\u2502   \u2502   \u251c\u2500\u2500 your_ingest.yaml\n\u2502   \u2502   \u2514\u2500\u2500 your_ingest.py\n\u2502   \u2514\u2500\u2500 some_translation_table.yaml\n\u2514\u2500\u2500 ...\n</code></pre></p>"},{"location":"cli-reference/","title":"Koza CLI Reference","text":"<p>Complete reference for all Koza command-line interface commands and options.</p>"},{"location":"cli-reference/#global-options","title":"Global Options","text":""},{"location":"cli-reference/#version-information","title":"Version Information","text":"<p><pre><code>koza --version\n</code></pre> Display the current Koza version and exit.</p>"},{"location":"cli-reference/#commands","title":"Commands","text":""},{"location":"cli-reference/#transform","title":"transform","text":"<p>Transform biomedical data sources into KGX format using semi-declarative Python transforms.</p>"},{"location":"cli-reference/#synopsis","title":"Synopsis","text":"<pre><code># Config file mode (traditional)\nkoza transform CONFIG.yaml [OPTIONS]\n\n# Config-free mode with Python transform (input files as positional args)\nkoza transform TRANSFORM.py [OPTIONS] [INPUT_FILES]...\n</code></pre>"},{"location":"cli-reference/#arguments","title":"Arguments","text":"<ul> <li><code>CONFIG_OR_TRANSFORM</code> (required) - Configuration YAML file OR Python transform file</li> <li><code>INPUT_FILES</code> (optional, variadic) - Input files (supports shell glob expansion)</li> <li>Config-free mode (<code>.py</code> file): Required. These files are processed by the transform.</li> <li>Config file mode (<code>.yaml</code> file): Optional. If provided, overrides the <code>files</code> list in the config's reader section.</li> </ul>"},{"location":"cli-reference/#options","title":"Options","text":"Option Short Type Default Description <code>--output-dir</code> <code>-o</code> str <code>./output</code> Path to output directory <code>--output-format</code> <code>-f</code> OutputFormat <code>tsv</code> Output format (<code>tsv</code>, <code>jsonl</code>, <code>parquet</code>) <code>--input-format</code> InputFormat auto Input format (auto-detected from extension if not specified) <code>--delimiter</code> <code>-d</code> str auto Field delimiter for CSV/TSV (default: tab for .tsv, comma for .csv) <code>--limit</code> <code>-n</code> int 0 Number of rows to process (0 = all) <code>--progress</code> <code>-p</code> bool False Display progress bar during transform <code>--quiet</code> <code>-q</code> bool False Suppress output except errors"},{"location":"cli-reference/#examples","title":"Examples","text":"<pre><code># Basic transform with config file\nkoza transform examples/string/protein-links-detailed.yaml\n\n# Transform with custom output directory and format\nkoza transform config.yaml -o ./results -f jsonl\n\n# Transform with progress and row limit\nkoza transform config.yaml --progress --limit 1000\n\n# Config-free mode with Python transform file (input files at end)\nkoza transform transform.py -o ./output -f jsonl data/*.yaml\n\n# Config-free mode with explicit input format\nkoza transform transform.py --input-format yaml data/*.dat\n</code></pre>"},{"location":"cli-reference/#join","title":"join","text":"<p>Combine multiple KGX files into a unified DuckDB database with automatic schema harmonization.</p>"},{"location":"cli-reference/#synopsis_1","title":"Synopsis","text":"<pre><code>koza join [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_1","title":"Options","text":"Option Type Default Description <code>--nodes</code> List[str] None Node files to join (supports glob patterns) <code>--edges</code> List[str] None Edge files to join (supports glob patterns) <code>--input-dir</code> Path None Directory containing KGX files (auto-discovers) <code>--output</code> str <code>joined_graph.duckdb</code> Output DuckDB database path <code>--schema-report</code> bool False Generate detailed schema analysis report <code>--show-progress</code> bool False Display progress bars during loading <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#file-specification-formats","title":"File Specification Formats","text":"<p>Simple paths: <pre><code>koza join --nodes genes.tsv proteins.jsonl --edges interactions.parquet\n</code></pre></p> <p>With source names: <pre><code>koza join --nodes \"genes:genes.tsv\" \"proteins:proteins.jsonl\"\n</code></pre></p> <p>Glob patterns: <pre><code>koza join --nodes \"*.nodes.*\" --edges \"*.edges.*\"\n</code></pre></p>"},{"location":"cli-reference/#examples_1","title":"Examples","text":"<pre><code># Basic join with mixed formats\nkoza join \\\n  --nodes genes.tsv proteins.jsonl pathways.parquet \\\n  --edges gene_protein.tsv protein_pathway.jsonl \\\n  --output unified_graph.duckdb\n\n# Join with schema reporting and progress\nkoza join \\\n  --nodes \"*.nodes.*\" \\\n  --edges \"*.edges.*\" \\\n  --schema-report \\\n  --show-progress\n\n# Auto-discover files in directory\nkoza join --input-dir ./kgx_files --output merged.duckdb\n\n# Quiet operation for scripts\nkoza join --nodes \"*.nodes.tsv\" --edges \"*.edges.tsv\" --quiet\n</code></pre>"},{"location":"cli-reference/#output","title":"Output","text":"<ul> <li>Database file: DuckDB database with <code>nodes</code> and <code>edges</code> tables</li> <li>Schema report: <code>{output}_schema_report_join.yaml</code> (if <code>--schema-report</code>)</li> <li>CLI summary: Files processed, records loaded, schema harmonization details</li> </ul>"},{"location":"cli-reference/#split","title":"split","text":"<p>Extract subsets of data from a DuckDB database with configurable filters and format conversion.</p>"},{"location":"cli-reference/#synopsis_2","title":"Synopsis","text":"<pre><code>koza split [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_2","title":"Options","text":"Option Type Default Description <code>--database</code> Path Required Input DuckDB database path <code>--split-on</code> str Required Column to split on (creates separate files per value) <code>--output-dir</code> Path <code>./split_output</code> Directory for output files <code>--output-format</code> KGXFormat <code>tsv</code> Output format (<code>tsv</code>, <code>jsonl</code>, <code>parquet</code>) <code>--filter-nodes</code> str None SQL WHERE clause for filtering nodes <code>--filter-edges</code> str None SQL WHERE clause for filtering edges <code>--show-progress</code> bool False Display progress bars during export <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#examples_2","title":"Examples","text":"<pre><code># Split by source with original format\nkoza split --database graph.duckdb --split-on provided_by\n\n# Split with format conversion to Parquet\nkoza split \\\n  --database graph.duckdb \\\n  --split-on namespace \\\n  --output-format parquet \\\n  --output-dir ./parquet_output\n\n# Split with custom filters\nkoza split \\\n  --database graph.duckdb \\\n  --split-on category \\\n  --filter-nodes \"namespace='HGNC'\" \\\n  --filter-edges \"predicate='biolink:interacts_with'\"\n\n# Split with progress tracking\nkoza split \\\n  --database large_graph.duckdb \\\n  --split-on provided_by \\\n  --show-progress\n</code></pre>"},{"location":"cli-reference/#output_1","title":"Output","text":"<p>For each unique value in the split column, creates: - <code>{value}_nodes.{format}</code> - Node data for that value - <code>{value}_edges.{format}</code> - Edge data for that value</p>"},{"location":"cli-reference/#prune","title":"prune","text":"<p>Clean up graph integrity issues by handling dangling edges and singleton nodes.</p>"},{"location":"cli-reference/#synopsis_3","title":"Synopsis","text":"<pre><code>koza prune [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_3","title":"Options","text":"Option Type Default Description <code>--database</code> Path Required DuckDB database to prune <code>--keep-singletons</code> bool False Preserve isolated nodes in main graph <code>--remove-singletons</code> bool False Move isolated nodes to separate table <code>--dry-run</code> bool False Preview changes without applying them <code>--show-progress</code> bool False Display progress bars during operation <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#singleton-node-strategies","title":"Singleton Node Strategies","text":"<p>Keep singletons (default behavior): - Isolated nodes remain in <code>nodes</code> table - Only dangling edges are moved</p> <p>Remove singletons: - Isolated nodes moved to <code>singleton_nodes</code> table - Main graph contains only connected nodes</p>"},{"location":"cli-reference/#examples_3","title":"Examples","text":"<pre><code># Basic pruning with singleton preservation\nkoza prune --database graph.duckdb --keep-singletons\n\n# Remove singletons to separate table\nkoza prune --database graph.duckdb --remove-singletons\n\n# Preview changes without applying\nkoza prune --database graph.duckdb --dry-run\n\n# Quiet operation for automation\nkoza prune --database graph.duckdb --keep-singletons --quiet\n\n# With progress tracking for large graphs\nkoza prune \\\n  --database large_graph.duckdb \\\n  --remove-singletons \\\n  --show-progress\n</code></pre>"},{"location":"cli-reference/#output_2","title":"Output","text":"<p>Creates additional tables for data preservation: - <code>dangling_edges</code>: Edges pointing to non-existent nodes - <code>singleton_nodes</code>: Isolated nodes (if <code>--remove-singletons</code>) - CLI summary: Counts of edges/nodes moved, integrity statistics</p>"},{"location":"cli-reference/#append","title":"append","text":"<p>Add new KGX files to existing databases with schema evolution and optional deduplication.</p>"},{"location":"cli-reference/#synopsis_4","title":"Synopsis","text":"<pre><code>koza append [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options_4","title":"Options","text":"Option Type Default Description <code>--database</code> Path Required Existing DuckDB database to append to <code>--nodes</code> List[str] None Node files to append <code>--edges</code> List[str] None Edge files to append <code>--deduplicate</code> bool False Remove exact duplicates after appending <code>--schema-report</code> bool False Generate schema analysis after append <code>--show-progress</code> bool False Display progress bars during loading <code>--quiet</code> bool False Suppress all output except errors"},{"location":"cli-reference/#schema-evolution","title":"Schema Evolution","text":"<p>When appending files with new columns: - New columns automatically added to existing tables - Existing records get NULL values for new columns - All schema changes are tracked and reported</p>"},{"location":"cli-reference/#deduplication-strategy","title":"Deduplication Strategy","text":"<p>When <code>--deduplicate</code> is enabled: - Removes exact duplicates by ID field - Keeps first occurrence of each duplicate ID - Operates on both nodes and edges tables - Reports number of duplicates removed</p>"},{"location":"cli-reference/#examples_4","title":"Examples","text":"<pre><code># Basic append operation\nkoza append \\\n  --database existing_graph.duckdb \\\n  --nodes new_genes.tsv \\\n  --edges new_interactions.jsonl\n\n# Append with deduplication and schema reporting\nkoza append \\\n  --database graph.duckdb \\\n  --nodes genes_with_new_fields.tsv \\\n  --edges updated_interactions.parquet \\\n  --deduplicate \\\n  --schema-report\n\n# Append multiple files with progress\nkoza append \\\n  --database graph.duckdb \\\n  --nodes \"new_*.nodes.*\" \\\n  --edges \"new_*.edges.*\" \\\n  --show-progress\n\n# Quiet append for automation\nkoza append \\\n  --database graph.duckdb \\\n  --nodes corrections.tsv \\\n  --quiet\n</code></pre>"},{"location":"cli-reference/#output_3","title":"Output","text":"<ul> <li>Schema changes: Reports new columns added and their sources</li> <li>Record counts: Shows before/after record counts for nodes and edges</li> <li>Duplicate statistics: Number of duplicates removed (if <code>--deduplicate</code>)</li> <li>Schema report: Detailed analysis (if <code>--schema-report</code>)</li> </ul>"},{"location":"cli-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"cli-reference/#file-specification-formats_1","title":"File Specification Formats","text":"<p>All commands that accept file lists support multiple specification formats:</p>"},{"location":"cli-reference/#glob-patterns","title":"Glob Patterns","text":"<pre><code># Match all node files\n--nodes \"*.nodes.*\"\n\n# Match specific formats\n--nodes \"*.tsv\" --edges \"*.jsonl\"\n\n# Complex patterns\n--nodes \"**/genes*.{tsv,jsonl}\"\n</code></pre>"},{"location":"cli-reference/#source-attribution","title":"Source Attribution","text":"<pre><code># Assign source names to files\n--nodes \"genes:genes.tsv\" \"proteins:proteins.jsonl\"\n--edges \"interactions:interactions.parquet\"\n</code></pre>"},{"location":"cli-reference/#mixed-specifications","title":"Mixed Specifications","text":"<pre><code># Combine different formats\n--nodes genes.tsv \"pathways:pathways.jsonl\" proteins.parquet\n</code></pre>"},{"location":"cli-reference/#progress-and-logging","title":"Progress and Logging","text":""},{"location":"cli-reference/#progress-indicators","title":"Progress Indicators","text":"<ul> <li><code>--show-progress</code>: Display progress bars for file operations</li> <li><code>--quiet</code>: Suppress all non-error output</li> <li>Cannot use both flags together</li> </ul>"},{"location":"cli-reference/#log-levels","title":"Log Levels","text":"<p>Available for <code>transform</code> command: - <code>DEBUG</code>: Verbose debugging information - <code>INFO</code>: General information messages - <code>WARNING</code>: Warning messages (default) - <code>ERROR</code>: Error messages only</p>"},{"location":"cli-reference/#output-formats","title":"Output Formats","text":""},{"location":"cli-reference/#supported-formats","title":"Supported Formats","text":"<ul> <li>TSV: Tab-separated values (KGX standard)</li> <li>JSONL: JSON Lines format  </li> <li>Parquet: Columnar format for analytics</li> </ul>"},{"location":"cli-reference/#format-selection","title":"Format Selection","text":"<pre><code># Explicit format specification\n--output-format tsv\n--output-format jsonl  \n--output-format parquet\n\n# Automatic format detection from file extensions\n# genes.tsv \u2192 TSV format\n# genes.jsonl \u2192 JSONL format\n# genes.parquet \u2192 Parquet format\n</code></pre>"},{"location":"cli-reference/#error-handling","title":"Error Handling","text":""},{"location":"cli-reference/#common-error-messages","title":"Common Error Messages","text":""},{"location":"cli-reference/#file-not-found","title":"File Not Found","text":"<p><pre><code>Error: File not found: missing_file.tsv\n</code></pre> Solution: Verify file paths and check file permissions</p>"},{"location":"cli-reference/#database-not-found","title":"Database Not Found","text":"<p><pre><code>Error: Database not found: nonexistent.duckdb\n</code></pre> Solution: Verify database path or create database with <code>join</code> command first</p>"},{"location":"cli-reference/#schema-conflicts","title":"Schema Conflicts","text":"<p><pre><code>Warning: Type conflict for column 'score' (INTEGER vs VARCHAR)\n</code></pre> Resolution: DuckDB automatically resolves to most permissive type</p>"},{"location":"cli-reference/#permission-issues","title":"Permission Issues","text":"<p><pre><code>Error: Permission denied writing to output directory\n</code></pre> Solution: Check write permissions on output directory</p>"},{"location":"cli-reference/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error (file not found, permission denied, etc.) 2 Invalid arguments or configuration 130 Interrupted by user (Ctrl+C)"},{"location":"cli-reference/#performance-tips","title":"Performance Tips","text":""},{"location":"cli-reference/#for-large-files","title":"For Large Files","text":"<ol> <li>Use compressed formats (<code>.gz</code>, <code>.bz2</code>) to reduce I/O</li> <li>Enable progress bars to monitor long operations  </li> <li>Use Parquet format for repeated analytical operations</li> <li>Process in chunks using directory-based operations</li> </ol>"},{"location":"cli-reference/#memory-optimization","title":"Memory Optimization","text":"<ol> <li>Close other applications during large operations</li> <li>Use streaming operations (automatic for large files)</li> <li>Monitor disk space for join operations</li> <li>Consider temporary directory on fast storage</li> </ol>"},{"location":"cli-reference/#automation-scripts","title":"Automation Scripts","text":"<ol> <li>Use <code>--quiet</code> flag to suppress output in scripts</li> <li>Check exit codes for error handling</li> <li>Enable logging for debugging automated workflows</li> <li>Use glob patterns for flexible file matching</li> </ol>"},{"location":"cli-reference/#integration-examples","title":"Integration Examples","text":""},{"location":"cli-reference/#shell-scripts","title":"Shell Scripts","text":"<pre><code>#!/bin/bash\n# Complete graph processing pipeline\n\nset -e  # Exit on error\n\necho \"Starting graph processing pipeline...\"\n\n# Join all source files\nkoza join \\\n  --nodes \"sources/*.nodes.*\" \\\n  --edges \"sources/*.edges.*\" \\\n  --output raw_graph.duckdb \\\n  --show-progress\n\n# Clean up integrity issues\nkoza prune \\\n  --database raw_graph.duckdb \\\n  --keep-singletons\n\n# Add corrected data\nif [ -f \"corrections.tsv\" ]; then\n  koza append \\\n    --database raw_graph.duckdb \\\n    --nodes corrections.tsv \\\n    --deduplicate \\\n    --quiet\nfi\n\n# Export by namespace\nkoza split \\\n  --database raw_graph.duckdb \\\n  --split-on namespace \\\n  --output-format parquet \\\n  --output-dir final_graphs\n\necho \"Pipeline completed successfully\"\n</code></pre>"},{"location":"cli-reference/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\nname: Process Knowledge Graph\non: [push]\n\njobs:\n  process-graph:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install Koza\n        run: pip install koza\n      - name: Join graph files\n        run: |\n          koza join \\\n            --input-dir ./data \\\n            --output graph.duckdb \\\n            --quiet\n      - name: Clean graph\n        run: |\n          koza prune \\\n            --database graph.duckdb \\\n            --keep-singletons \\\n            --quiet\n      - name: Upload results\n        uses: actions/upload-artifact@v2\n        with:\n          name: processed-graph\n          path: graph.duckdb\n</code></pre>"},{"location":"cli-reference/#configuration-files","title":"Configuration Files","text":"<p>While most commands use command-line arguments, complex configurations can be managed through:</p> <ol> <li>Environment variables for commonly used paths</li> <li>Shell aliases for frequently used command combinations  </li> <li>Configuration files for transform operations (YAML format)</li> <li>Makefiles for complex multi-step workflows</li> </ol>"},{"location":"cli-reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli-reference/#getting-help","title":"Getting Help","text":"<pre><code># General help\nkoza --help\n\n# Command-specific help  \nkoza join --help\nkoza split --help\nkoza prune --help\nkoza append --help\nkoza transform --help\n</code></pre>"},{"location":"cli-reference/#debug-information","title":"Debug Information","text":"<pre><code># Enable verbose logging for transform\nkoza transform config.yaml --log-level DEBUG\n\n# Use dry-run to preview prune operations\nkoza prune --database graph.duckdb --dry-run\n\n# Generate schema reports for analysis\nkoza join --nodes \"*.tsv\" --schema-report\n</code></pre>"},{"location":"cli-reference/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Monitor large operations with progress\nkoza join --nodes \"*.nodes.*\" --edges \"*.edges.*\" --show-progress\n\n# Time operations for benchmarking\ntime koza join --nodes \"*.nodes.tsv\" --quiet\n</code></pre> <p>For additional support, consult the comprehensive graph operations guide and the Koza documentation.</p>"},{"location":"graph-operations/","title":"Graph Operations","text":"<p>Documentation Moved</p> <p>The graph operations documentation has been reorganized into a structured format following the Diataxis framework.</p> <p>Please visit the new documentation:</p> <p>Graph Operations Overview</p>"},{"location":"graph-operations/#quick-links","title":"Quick Links","text":""},{"location":"graph-operations/#tutorials","title":"Tutorials","text":"<ul> <li>Build Your First Graph - Step-by-step introduction</li> <li>Complete Merge Workflow - Full pipeline tutorial</li> </ul>"},{"location":"graph-operations/#how-to-guides","title":"How-to Guides","text":"<ul> <li>Join Files</li> <li>Split Graphs</li> <li>Normalize IDs</li> <li>Clean Graphs</li> <li>Incremental Updates</li> <li>Generate Reports</li> <li>Export Formats</li> </ul>"},{"location":"graph-operations/#reference","title":"Reference","text":"<ul> <li>CLI Reference</li> <li>Python API</li> <li>Configuration</li> </ul>"},{"location":"graph-operations/#explanation","title":"Explanation","text":"<ul> <li>Architecture</li> <li>Schema Handling</li> <li>Data Integrity</li> <li>Biolink Compliance</li> </ul>"},{"location":"Ingests/","title":"Configuring an Ingest","text":"<p><sub> (For CLI usage, see the CLI commands page.) </sub> </p> <p>Koza is designed to process and transform existing data into a target csv/json/jsonl format.  </p> <p>This process is internally known as an ingest. Ingests are defined by:  </p> <ol> <li>Koza config file yaml: Koza ingest configuration, including:<ul> <li>metadata, formats, required columns, any SSSOM files, etc. </li> </ul> </li> <li>Map config yaml: (Optional) configures creation of mapping dictionary  </li> <li>Transform code: a Python script, with specific transform instructions </li> </ol>"},{"location":"Ingests/koza_config/","title":"Koza Configuration (KozaConfig)","text":"<p>This document describes the KozaConfig model introduced in Koza 2, which replaces the previous SourceConfig structure. The KozaConfig provides a comprehensive configuration system for data ingests with support for multiple readers, transforms, and writers.</p> <p>Paths are relative to the directory from which you execute Koza.</p>"},{"location":"Ingests/koza_config/#overview","title":"Overview","text":"<p>KozaConfig is the main configuration class that defines how Koza processes your data. It consists of several main sections:</p> <ul> <li>name: Unique identifier for your ingest</li> <li>reader/readers: Configuration for input data sources  </li> <li>transform: Configuration for data transformation logic</li> <li>writer: Configuration for output format and properties</li> <li>metadata: Optional metadata about the dataset</li> </ul>"},{"location":"Ingests/koza_config/#basic-structure","title":"Basic Structure","text":"<pre><code>name: 'my-ingest'\nreader: # OR readers: for multiple sources\n  # reader configuration\ntransform:\n  # transformation configuration  \nwriter:\n  # output configuration\nmetadata: # optional\n  # metadata configuration\n</code></pre>"},{"location":"Ingests/koza_config/#core-configuration-properties","title":"Core Configuration Properties","text":""},{"location":"Ingests/koza_config/#required-properties","title":"Required Properties","text":"Property Type Description <code>name</code> string Name of the data ingest, should be unique and descriptive"},{"location":"Ingests/koza_config/#optional-properties","title":"Optional Properties","text":"Property Type Description <code>reader</code> ReaderConfig Single reader configuration (mutually exclusive with <code>readers</code>) <code>readers</code> dict[str, ReaderConfig] Named multiple readers (mutually exclusive with <code>reader</code>) <code>transform</code> TransformConfig Transform configuration (optional, uses defaults if not specified) <code>writer</code> WriterConfig Writer configuration (optional, uses defaults if not specified) <code>metadata</code> DatasetDescription | string Dataset metadata or path to metadata file"},{"location":"Ingests/koza_config/#reader-configuration","title":"Reader Configuration","text":"<p>Readers define how Koza processes input data files. You can use either a single <code>reader</code> or multiple named <code>readers</code>.</p>"},{"location":"Ingests/koza_config/#single-reader-example","title":"Single Reader Example","text":"<pre><code>reader:\n  format: csv\n  files:\n    - 'data/input.tsv'\n  delimiter: '\\t'\n</code></pre>"},{"location":"Ingests/koza_config/#multiple-readers-example","title":"Multiple Readers Example","text":"<pre><code>readers:\n  main_data:\n    format: csv\n    files:\n      - 'data/main.tsv'\n    delimiter: '\\t'\n  reference_data:\n    format: json\n    files:\n      - 'data/reference.json'\n</code></pre>"},{"location":"Ingests/koza_config/#base-reader-properties","title":"Base Reader Properties","text":"<p>All reader types support these common properties:</p> Property Type Description <code>files</code> list[string] List of input files to process <code>filters</code> list[ColumnFilter] List of filters to apply to data"},{"location":"Ingests/koza_config/#csv-reader-configuration","title":"CSV Reader Configuration","text":"<p>For CSV format files (<code>format: csv</code>):</p> Property Type Default Description <code>format</code> string <code>csv</code> Must be \"csv\" <code>columns</code> list[string | dict] None Column names or name/type mappings <code>field_type_map</code> dict[string, FieldType] None Mapping of column names to types <code>delimiter</code> string <code>\\t</code> Field delimiter (supports \"tab\", \"\\t\", or literal chars) <code>header_delimiter</code> string None Different delimiter for header row <code>dialect</code> string <code>excel</code> CSV dialect <code>header_mode</code> int | HeaderMode <code>infer</code> Header handling: int (0-based row), \"infer\", or \"none\" <code>header_prefix</code> string None Prefix for header processing <code>skip_blank_lines</code> bool <code>true</code> Whether to skip blank lines <code>comment_char</code> string <code>#</code> Character that indicates comments"},{"location":"Ingests/koza_config/#field-types","title":"Field Types","text":"<ul> <li><code>str</code> - String type (default)</li> <li><code>int</code> - Integer type  </li> <li><code>float</code> - Float type</li> </ul>"},{"location":"Ingests/koza_config/#header-modes","title":"Header Modes","text":"<ul> <li><code>infer</code> - Automatically detect header row</li> <li><code>none</code> - No header row present</li> <li>Integer (0-based) - Specific header row index</li> </ul>"},{"location":"Ingests/koza_config/#column-definition-examples","title":"Column Definition Examples","text":"<pre><code># Simple string columns\ncolumns:\n  - 'gene_id'\n  - 'symbol'\n  - 'score'\n\n# Mixed types  \ncolumns:\n  - 'gene_id'\n  - 'symbol' \n  - 'score': 'int'\n  - 'p_value': 'float'\n</code></pre>"},{"location":"Ingests/koza_config/#json-reader-configuration","title":"JSON Reader Configuration","text":"<p>For JSON format files (<code>format: json</code>):</p> Property Type Description <code>format</code> string Must be \"json\" <code>required_properties</code> list[string] Properties that must be present <code>json_path</code> list[string | int] Path to data within JSON structure"},{"location":"Ingests/koza_config/#jsonl-reader-configuration","title":"JSONL Reader Configuration","text":"<p>For JSON Lines format files (<code>format: jsonl</code>):</p> Property Type Description <code>format</code> string Must be \"jsonl\" <code>required_properties</code> list[string] Properties that must be present"},{"location":"Ingests/koza_config/#yaml-reader-configuration","title":"YAML Reader Configuration","text":"<p>For YAML format files (<code>format: yaml</code>):</p> Property Type Description <code>format</code> string Must be \"yaml\" <code>required_properties</code> list[string] Properties that must be present <code>json_path</code> list[string | int] Path to data within YAML structure"},{"location":"Ingests/koza_config/#column-filters","title":"Column Filters","text":"<p>Filters allow you to include or exclude rows based on column values.</p>"},{"location":"Ingests/koza_config/#filter-types","title":"Filter Types","text":""},{"location":"Ingests/koza_config/#comparison-filters","title":"Comparison Filters","text":"<p>For numeric comparisons:</p> <pre><code>filters:\n  - inclusion: 'include'  # or 'exclude'\n    column: 'score'\n    filter_code: 'gt'     # gt, ge, lt, le  \n    value: 500\n</code></pre>"},{"location":"Ingests/koza_config/#equality-filters","title":"Equality Filters","text":"<p>For exact matches:</p> <pre><code>filters:\n  - inclusion: 'include'\n    column: 'status'\n    filter_code: 'eq'     # eq, ne\n    value: 'active'\n</code></pre>"},{"location":"Ingests/koza_config/#list-filters","title":"List Filters","text":"<p>For checking membership in lists:</p> <pre><code>filters:\n  - inclusion: 'include'\n    column: 'category'\n    filter_code: 'in'     # in, in_exact\n    value: ['A', 'B', 'C']\n</code></pre>"},{"location":"Ingests/koza_config/#filter-codes","title":"Filter Codes","text":"<ul> <li><code>gt</code> - Greater than</li> <li><code>ge</code> - Greater than or equal  </li> <li><code>lt</code> - Less than</li> <li><code>le</code> - Less than or equal</li> <li><code>eq</code> - Equal to</li> <li><code>ne</code> - Not equal to</li> <li><code>in</code> - In list (case insensitive)</li> <li><code>in_exact</code> - In list (exact match)</li> </ul>"},{"location":"Ingests/koza_config/#transform-configuration","title":"Transform Configuration","text":"<p>The transform section configures how data is processed and transformed.</p> Property Type Default Description <code>code</code> string None Path to Python transform file <code>module</code> string None Python module to import <code>global_table</code> string | dict None Global translation table <code>local_table</code> string | dict None Local translation table <code>mappings</code> list[string] <code>[]</code> List of mapping files <code>on_map_failure</code> MapErrorEnum <code>warning</code> How to handle mapping failures <code>extra_fields</code> dict <code>{}</code> Additional custom fields"},{"location":"Ingests/koza_config/#map-error-handling","title":"Map Error Handling","text":"<ul> <li><code>warning</code> - Log warnings for mapping failures</li> <li><code>error</code> - Raise errors for mapping failures</li> </ul>"},{"location":"Ingests/koza_config/#example-transform-configuration","title":"Example Transform Configuration","text":"<pre><code>transform:\n  code: 'transform.py'\n  global_table: 'tables/global_mappings.yaml'\n  local_table: 'tables/local_mappings.yaml'\n  mappings:\n    - 'mappings/gene_mappings.yaml'\n  on_map_failure: 'warning'\n  custom_param: 'value'  # Goes into extra_fields\n</code></pre>"},{"location":"Ingests/koza_config/#writer-configuration","title":"Writer Configuration","text":"<p>The writer section configures output format and properties.</p> Property Type Default Description <code>format</code> OutputFormat <code>tsv</code> Output format <code>sssom_config</code> SSSOMConfig None SSSOM mapping configuration <code>node_properties</code> list[string] None Node properties to include <code>edge_properties</code> list[string] None Edge properties to include <code>min_node_count</code> int None Minimum nodes required <code>min_edge_count</code> int None Minimum edges required <code>max_node_count</code> int None Maximum nodes allowed <code>max_edge_count</code> int None Maximum edges allowed"},{"location":"Ingests/koza_config/#output-formats","title":"Output Formats","text":"<ul> <li><code>tsv</code> - Tab-separated values</li> <li><code>jsonl</code> - JSON Lines  </li> <li><code>kgx</code> - KGX format</li> <li><code>passthrough</code> - Pass data through unchanged</li> </ul>"},{"location":"Ingests/koza_config/#example-writer-configuration","title":"Example Writer Configuration","text":"<pre><code>writer:\n  format: tsv\n  node_properties:\n    - 'id'\n    - 'category'\n    - 'name'\n  edge_properties:\n    - 'id'\n    - 'subject'\n    - 'predicate'\n    - 'object'\n    - 'category'\n</code></pre>"},{"location":"Ingests/koza_config/#sssom-configuration","title":"SSSOM Configuration","text":"<p>SSSOM (Simple Standard for Sharing Ontological Mappings) integration:</p> Property Type Description <code>files</code> list[string] SSSOM mapping files <code>filter_prefixes</code> list[string] Prefixes to filter by <code>subject_target_prefixes</code> list[string] Subject mapping prefixes <code>object_target_prefixes</code> list[string] Object mapping prefixes <code>use_match</code> list[Match] Match types to use"},{"location":"Ingests/koza_config/#match-types","title":"Match Types","text":"<ul> <li><code>exact</code> - Exact matches</li> <li><code>narrow</code> - Narrow matches  </li> <li><code>broad</code> - Broad matches</li> </ul> <pre><code>writer:\n  sssom_config:\n    files:\n      - 'mappings/ontology_mappings.sssom.tsv'\n    subject_target_prefixes: ['MONDO']\n    object_target_prefixes: ['HP', 'GO']\n    use_match: ['exact']\n</code></pre>"},{"location":"Ingests/koza_config/#metadata-configuration","title":"Metadata Configuration","text":"<p>Metadata can be defined inline or loaded from a separate file.</p>"},{"location":"Ingests/koza_config/#inline-metadata","title":"Inline Metadata","text":"<pre><code>metadata:\n  name: 'My Data Source'\n  description: 'Description of the data and processing'\n  ingest_title: 'Source Database Name'\n  ingest_url: 'https://source-database.org'\n  provided_by: 'my_source_gene_disease'\n  rights: 'https://source-database.org/license'\n</code></pre>"},{"location":"Ingests/koza_config/#external-metadata-file","title":"External Metadata File","text":"<pre><code>metadata: './metadata.yaml'\n</code></pre>"},{"location":"Ingests/koza_config/#metadata-properties","title":"Metadata Properties","text":"Property Type Description <code>name</code> string Human-readable name of data source <code>ingest_title</code> string Title of data source (maps to biolink name) <code>ingest_url</code> string URL of data source (maps to biolink iri) <code>description</code> string Description of data/ingest process <code>provided_by</code> string Source identifier, format: <code>&lt;source&gt;_&lt;type&gt;</code> <code>rights</code> string License/rights information URL"},{"location":"Ingests/koza_config/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example showing all major configuration options:</p> <pre><code>name: 'comprehensive-example'\n\nmetadata:\n  name: 'Example Database'\n  description: 'Comprehensive example of Koza configuration'\n  ingest_title: 'Example DB'\n  ingest_url: 'https://example-db.org'\n  provided_by: 'example_gene_disease'\n  rights: 'https://example-db.org/license'\n\nreader:\n  format: csv\n  files:\n    - 'data/genes.tsv'\n    - 'data/diseases.tsv'\n  delimiter: '\\t'\n  columns:\n    - 'gene_id'\n    - 'gene_symbol'\n    - 'disease_id'\n    - 'confidence': 'float'\n  filters:\n    - inclusion: 'include'\n      column: 'confidence'\n      filter_code: 'ge'\n      value: 0.7\n    - inclusion: 'exclude'\n      column: 'gene_symbol'\n      filter_code: 'eq'\n      value: 'DEPRECATED'\n\ntransform:\n  code: 'transform.py'\n  global_table: 'tables/global_mappings.yaml'\n  on_map_failure: 'warning'\n\nwriter:\n  format: tsv\n  node_properties:\n    - 'id'\n    - 'category'\n    - 'name'\n    - 'provided_by'\n  edge_properties:\n    - 'id'\n    - 'subject'\n    - 'predicate'\n    - 'object'\n    - 'category'\n    - 'provided_by'\n    - 'confidence'\n  min_node_count: 100\n  min_edge_count: 50\n</code></pre>"},{"location":"Ingests/koza_config/#migration-from-sourceconfig","title":"Migration from SourceConfig","text":"<p>If you're migrating from the old SourceConfig format to KozaConfig:</p> <ol> <li>Structure Changes: </li> <li>Top-level properties are now organized under <code>reader</code>, <code>transform</code>, and <code>writer</code> sections</li> <li><code>files</code> moves to <code>reader.files</code></li> <li>Transform-related properties move to <code>transform</code> section</li> <li> <p>Output properties move to <code>writer</code> section</p> </li> <li> <p>Property Mapping:</p> </li> <li><code>transform_code</code> \u2192 <code>transform.code</code></li> <li><code>global_table</code> \u2192 <code>transform.global_table</code> </li> <li><code>local_table</code> \u2192 <code>transform.local_table</code></li> <li><code>node_properties</code> \u2192 <code>writer.node_properties</code></li> <li> <p><code>edge_properties</code> \u2192 <code>writer.edge_properties</code></p> </li> <li> <p>New Features:</p> </li> <li>Multiple readers support with <code>readers</code></li> <li>Enhanced filter system with more comparison operators</li> <li>SSSOM integration in writer</li> <li>Improved metadata handling</li> </ol> <p>Next Steps: Transform Code</p>"},{"location":"Ingests/mapping/","title":"Mapping","text":"<p>Mapping with Koza is optional, but can be done in two ways:  </p> <ul> <li>Automated mapping with SSSOM files  </li> <li>Manual mapping with a map config yaml</li> </ul>"},{"location":"Ingests/mapping/#sssom-mapping","title":"SSSOM Mapping","text":"<p>Koza supports mapping with SSSOM files (Semantic Similarity of Source and Target Ontology Mappings). Simply add the path to the SSSOM file to your source config, the desired target prefixes, and any prefixes you want to use to filter the SSSOM file. Koza will automatically create a mapping lookup table which will automatically attempt to map any values in the source file to an ID with the target prefix.</p> <pre><code>sssom_config:\n    sssom_file: './path/to/your_mapping_file.sssom.tsv'\n    filter_prefixes: \n        - 'SOMEPREFIX'\n        - 'OTHERPREFIX'\n    target_prefixes: \n        - 'OTHERPREFIX'\n    use_match:\n        - 'exact'\n</code></pre> <p>Note: Currently, only the <code>exact</code> match type is supported (<code>narrow</code> and <code>broad</code> match types will be added in the future).</p>"},{"location":"Ingests/mapping/#manual-mapping-additional-data","title":"Manual Mapping / Additional Data","text":"<p>The map config yaml allows you to include data from other sources in your ingests, which may have different columns or formats.  </p> <p>If you don't have an SSSOM file, or you want to manually map some values, you can use a map config yaml. You can then add this map to your source config yaml in the <code>depends_on</code> property.  </p> <p>Koza will then create a nested dictionary with the specified key and values. For example, the following map config yaml maps values from the <code>STRING</code> column to the <code>entrez</code> and <code>NCBI taxid</code> columns.</p> <pre><code># koza/examples/maps/entrez-2-string.yaml\nname: ...\nfiles: ...\n\ncolumns:\n- 'NCBI taxid'\n- 'entrez'\n- 'STRING'\n\nkey: 'STRING'\n\nvalues:\n- 'entrez'\n- 'NCBI taxid'\n</code></pre> <p>The mapping dict will be available in your transform script from the <code>koza_app</code> object (see the Transform Code section below).</p> <p>Next Steps: Transform Code</p>"},{"location":"Ingests/testing/","title":"Testing","text":"<p>Koza includes a <code>mock_koza</code> fixture (see <code>src/koza/utils/testing_utils</code>) that can be used to test your ingest configuration. This fixture accepts the following arguments:</p> Argument Type Description Required Arguments <code>name</code> <code>str</code> The name of the ingest <code>data</code> <code>Union[Dict, List[Dict]]</code> The data to be ingested <code>transform_code</code> <code>str</code> Path to the transform code to be used Optional Arguments <code>map_cache</code> <code>Dict</code> Map cache to be used <code>filters</code> <code>List(str)</code> List of filters to apply to data <code>global_table</code> <code>str</code> Path to the global table <code>local_table</code> <code>str</code> Path to the local table <p>The <code>mock_koza</code> fixture returns a list of entities that would be generated by the ingest configuration. This list can then be used to test the output based on the transform script.</p> <p>Here is an example of how to use the <code>mock_koza</code> fixture to test an ingest configuration:</p> <pre><code>import pytest\n\nfrom koza.utils.testing_utils import mock_koza\n\n# Define the source name and transform script path\nINGEST_NAME = \"your_ingest_name\"\nTRANSFORM_SCRIPT = \"./src/{{cookiecutter.__project_slug}}/transform.py\"\n\n# Define an example row to test (as a dictionary)\n@pytest.fixture\ndef example_row():\n    return {\n        \"example_column_1\": \"entity_1\",\n        \"example_column_2\": \"entity_6\",\n        \"example_column_3\": \"biolink:related_to\",\n    }\n\n# Or a list of rows\n@pytest.fixture\ndef example_list_of_rows():\n    return [\n        {\n            \"example_column_1\": \"entity_1\",\n            \"example_column_2\": \"entity_6\",\n            \"example_column_3\": \"biolink:related_to\",\n        },\n        {\n            \"example_column_1\": \"entity_2\",\n            \"example_column_2\": \"entity_7\",\n            \"example_column_3\": \"biolink:related_to\",\n        },\n    ]\n\n# Define the mock koza transform\n@pytest.fixture\ndef mock_transform(mock_koza, example_row):\n    return mock_koza(\n        INGEST_NAME,\n        example_row,\n        TRANSFORM_SCRIPT,\n    )\n\n# Or for multiple rows\n@pytest.fixture\ndef mock_transform_multiple_rows(mock_koza, example_list_of_rows):\n    return mock_koza(\n        INGEST_NAME,\n        example_list_of_rows,\n        TRANSFORM_SCRIPT,\n    )\n\n# Test the output of the transform\n\ndef test_single_row(mock_transform):\n    assert len(mock_transform) == 1\n    entity = mock_transform[0]\n    assert entity\n    assert entity.subject == \"entity_1\"\n\n\ndef test_multiple_rows(mock_transform_multiple_rows):\n    assert len(mock_transform_multiple_rows) == 2\n    entity_1 = mock_transform_multiple_rows[0]\n    entity_2 = mock_transform_multiple_rows[1]\n    assert entity_1.subject == \"entity_1\"\n    assert entity_2.subject == \"entity_2\"\n</code></pre>"},{"location":"Ingests/transform/","title":"Transform Code","text":"<p>This Python script is where you'll define the specific steps of your data transformation. Koza will load this script and execute it for each row of data in your source file, applying any filters and mapping as defined in your source config yaml, and outputting the transformed data to the target csv/json/jsonl file.</p> <p>When Koza is called, either by command-line or as a library, it creates a <code>KozaTransform</code> object for the specified ingest. This KozaTransform will be your entry point to Koza and is available as a global variable in your transform code.</p> <p>The KozaTransform object has the following methods which can be used in your transform code:</p> Method Description <code>write(*args)</code> Writes the transformed data to the target file <p>Your transform code should define functions decorated with Koza decorators that process the data:</p> <p>Once you have processed a row of data, and created a biolink entity node or edge object (or both), you can pass these to <code>koza.write()</code> to output the transformed data to the target file.</p> Example Python Transform Script <pre><code>import re\nimport uuid\nfrom typing import Any\nfrom biolink_model.datamodel.pydanticmodel_v2 import PairwiseGeneToGeneInteraction, Protein\n\nimport koza\n\n@koza.transform_record()\ndef transform_record(koza: koza.KozaTransform, record: dict[str, Any]):\n    # Process the record data\n    protein_a = Protein(id=\"ENSEMBL:\" + re.sub(r\"\\d+\\.\", \"\", record[\"protein1\"]))\n    protein_b = Protein(id=\"ENSEMBL:\" + re.sub(r\"\\d+\\.\", \"\", record[\"protein2\"]))\n\n    # Create interaction\n    pairwise_gene_to_gene_interaction = PairwiseGeneToGeneInteraction(\n        id=\"uuid:\" + str(uuid.uuid1()),\n        subject=protein_a.id,\n        object=protein_b.id,\n        predicate=\"biolink:interacts_with\",\n        knowledge_level=\"not_provided\",\n        agent_type=\"not_provided\",\n    )\n\n    # Write the transformed data\n    koza.write(protein_a, protein_b, pairwise_gene_to_gene_interaction)\n</code></pre> <p>The <code>@koza.transform_record()</code> decorator indicates that this function processes individual records. If you pass nodes as well as edges to <code>koza.write()</code>, Koza will automatically create a node file and an edge file. If you pass only nodes, Koza will create only a node file, and if you pass only edges, Koza will create only an edge file.</p>"},{"location":"Usage/CLI/","title":"<code>koza</code>","text":"<p>Usage:</p> <pre><code>$ koza [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--version</code></li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>transform</code>: Transform a source file.</li> <li><code>join</code>: Join multiple KGX files into a unified...</li> <li><code>split</code>: Split a KGX file by specified fields with...</li> <li><code>prune</code>: Prune graph by removing dangling edges and...</li> <li><code>append</code>: Append new KGX files to an existing graph...</li> <li><code>normalize</code>: Apply SSSOM mappings to normalize edge...</li> <li><code>merge</code>: Complete merge pipeline: join \u2192 normalize...</li> <li><code>report</code>: Generate comprehensive reports for KGX...</li> <li><code>node-report</code>: Generate tabular node report with GROUP BY...</li> <li><code>edge-report</code>: Generate tabular edge report with...</li> <li><code>node-examples</code>: Generate sample rows per node type.</li> <li><code>edge-examples</code>: Generate sample rows per edge type.</li> </ul>"},{"location":"Usage/CLI/#koza-transform","title":"<code>koza transform</code>","text":"<p>Transform a source file.</p> <p>Accepts either a config YAML file or a Python transform file directly.</p> <p>Examples:     # With config file (existing behavior)     koza transform config.yaml</p> <pre><code># Config-free mode with transform file (shell expands the glob)\nkoza transform transform.py -o ./output -f jsonl data/*.yaml\n\n# With output options\nkoza transform transform.py -f jsonl -o ./output data/*.yaml\n\n# Explicit input format\nkoza transform transform.py --input-format yaml data/*.dat\n\n# CSV with comma delimiter (default for .csv files)\nkoza transform transform.py data/*.csv\n\n# TSV with explicit delimiter\nkoza transform transform.py -d &amp;#x27;\\t&amp;#x27; data/*.txt\n</code></pre> <p>Usage:</p> <pre><code>$ koza transform [OPTIONS] CONFIG_OR_TRANSFORM [INPUT_FILES]...\n</code></pre> <p>Arguments:</p> <ul> <li><code>CONFIG_OR_TRANSFORM</code>: Configuration YAML file OR Python transform file  [required]</li> <li><code>[INPUT_FILES]...</code>: Input files (required for .py transforms, supports shell glob expansion)</li> </ul> <p>Options:</p> <ul> <li><code>--input-format [csv|jsonl|json|yaml]</code>: Input format (auto-detected if not specified)</li> <li><code>-o, --output-dir TEXT</code>: Path to output directory  [default: ./output]</li> <li><code>-f, --output-format [tsv|jsonl|kgx|passthrough]</code>: Output format  [default: tsv]</li> <li><code>-n, --limit INTEGER</code>: Number of rows to process (if skipped, processes entire source file)  [default: 0]</li> <li><code>-p, --progress</code>: Display progress of transform</li> <li><code>-q, --quiet</code>: Disable log output</li> <li><code>-d, --delimiter TEXT</code>: Field delimiter for CSV/TSV files (default: tab for .tsv, comma for .csv)</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-join","title":"<code>koza join</code>","text":"<p>Join multiple KGX files into a unified DuckDB database</p> <p>Examples:     # Auto-discover files in directory     koza join --input-dir tmp/ -o graph.duckdb</p> <pre><code># Use glob patterns\nkoza join -n &amp;quot;tmp/*_nodes.tsv&amp;quot; -e &amp;quot;tmp/*_edges.tsv&amp;quot; -o graph.duckdb\n\n# Mix directory discovery with additional files\nkoza join --input-dir tmp/ -n extra_nodes.tsv -o graph.duckdb\n\n# Multiple individual files\nkoza join -n file1.tsv -n file2.tsv -e edges.tsv -o graph.duckdb\n</code></pre> <p>Usage:</p> <pre><code>$ koza join [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-n, --nodes TEXT</code>: Node files or glob patterns (can specify multiple)</li> <li><code>-e, --edges TEXT</code>: Edge files or glob patterns (can specify multiple)</li> <li><code>-d, --input-dir TEXT</code>: Directory to auto-discover KGX files</li> <li><code>-o, --output TEXT</code>: Path to output database file (default: in-memory)</li> <li><code>-f, --format [tsv|jsonl|parquet]</code>: Output format for any exported files  [default: tsv]</li> <li><code>--schema-report</code>: Generate schema compliance report</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-split","title":"<code>koza split</code>","text":"<p>Split a KGX file by specified fields with format conversion support</p> <p>Usage:</p> <pre><code>$ koza split [OPTIONS] FILE FIELDS\n</code></pre> <p>Arguments:</p> <ul> <li><code>FILE</code>: Path to the KGX file to split  [required]</li> <li><code>FIELDS</code>: Comma-separated list of fields to split on  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-o, --output-dir TEXT</code>: Output directory for split files  [default: ./output]</li> <li><code>-f, --format [tsv|jsonl|parquet]</code>: Output format (default: preserve input format)</li> <li><code>--remove-prefixes</code>: Remove prefixes from values in filenames</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-prune","title":"<code>koza prune</code>","text":"<p>Prune graph by removing dangling edges and handling singleton nodes</p> <p>Examples:     # Keep singleton nodes, move dangling edges     koza prune graph.duckdb</p> <pre><code># Remove singleton nodes to separate table\nkoza prune graph.duckdb --remove-singletons\n\n# Experimental: filter small components\nkoza prune graph.duckdb --min-component-size 10\n</code></pre> <p>Usage:</p> <pre><code>$ koza prune [OPTIONS] DATABASE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DATABASE</code>: Path to the DuckDB database file to prune  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--keep-singletons</code>: Keep singleton nodes in main table</li> <li><code>--remove-singletons</code>: Move singleton nodes to separate table</li> <li><code>--min-component-size INTEGER</code>: Minimum connected component size (experimental)</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-append","title":"<code>koza append</code>","text":"<p>Append new KGX files to an existing graph database</p> <p>Examples:     # Append specific files to existing database     koza append graph.duckdb -n new_nodes.tsv -e new_edges.tsv</p> <pre><code># Auto-discover files in directory and append\nkoza append graph.duckdb --input-dir new_data/\n\n# Append with deduplication and schema reporting\nkoza append graph.duckdb -n &amp;quot;*.tsv&amp;quot; --deduplicate --schema-report\n</code></pre> <p>Usage:</p> <pre><code>$ koza append [OPTIONS] DATABASE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DATABASE</code>: Path to existing DuckDB database file  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-n, --nodes TEXT</code>: Node files or glob patterns (can specify multiple)</li> <li><code>-e, --edges TEXT</code>: Edge files or glob patterns (can specify multiple)</li> <li><code>-d, --input-dir TEXT</code>: Directory to auto-discover KGX files</li> <li><code>--deduplicate</code>: Remove duplicates during append</li> <li><code>--schema-report</code>: Generate schema compliance report</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-normalize","title":"<code>koza normalize</code>","text":"<p>Apply SSSOM mappings to normalize edge subject/object references</p> <p>This operation loads SSSOM mapping files and applies them to rewrite edge subject and object identifiers to their canonical/equivalent forms. Node identifiers themselves are not changed - only edge references are normalized.</p> <p>Examples:     # Apply specific mapping files     koza normalize graph.duckdb -m gene_mappings.sssom.tsv -m mondo.sssom.tsv</p> <pre><code># Auto-discover SSSOM files in directory\nkoza normalize graph.duckdb --mappings-dir ./sssom/\n\n# Apply mappings with glob pattern\nkoza normalize graph.duckdb -m &amp;quot;*.sssom.tsv&amp;quot;\n</code></pre> <p>Usage:</p> <pre><code>$ koza normalize [OPTIONS] DATABASE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DATABASE</code>: Path to existing DuckDB database file  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-m, --mappings TEXT</code>: SSSOM mapping files or glob patterns (can specify multiple)</li> <li><code>-d, --mappings-dir TEXT</code>: Directory containing SSSOM mapping files</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-merge","title":"<code>koza merge</code>","text":"<p>Complete merge pipeline: join \u2192 normalize \u2192 prune</p> <p>This composite operation orchestrates the full graph processing pipeline: 1. Join: Load and combine multiple KGX files into a unified database 2. Normalize: Apply SSSOM mappings to edge subject/object references 3. Prune: Remove dangling edges and handle singleton nodes</p> <p>The pipeline can be customized by skipping steps or configuring options.</p> <p>Examples:     # Full pipeline with auto-discovery     koza merge --input-dir ./data/ --mappings-dir ./sssom/ -o clean_graph.duckdb</p> <pre><code># Specific files with export\nkoza merge -n nodes.tsv -e edges.tsv -m mappings.sssom.tsv --export --export-dir ./output/\n\n# Skip normalization, only join and prune\nkoza merge -n &amp;quot;*.tsv&amp;quot; -e &amp;quot;*.tsv&amp;quot; --skip-normalize -o graph.duckdb\n\n# Custom singleton handling\nkoza merge --input-dir ./data/ -m &amp;quot;*.sssom.tsv&amp;quot; --remove-singletons\n</code></pre> <p>Usage:</p> <pre><code>$ koza merge [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-n, --nodes TEXT</code>: Node files or glob patterns (can specify multiple)</li> <li><code>-e, --edges TEXT</code>: Edge files or glob patterns (can specify multiple)</li> <li><code>-m, --mappings TEXT</code>: SSSOM mapping files or glob patterns (can specify multiple)</li> <li><code>-d, --input-dir TEXT</code>: Directory to auto-discover KGX files</li> <li><code>--mappings-dir TEXT</code>: Directory containing SSSOM mapping files</li> <li><code>-o, --output TEXT</code>: Path to output database file (default: temporary)</li> <li><code>--export</code>: Export final clean data to files</li> <li><code>--export-dir TEXT</code>: Directory for exported files (required if --export)</li> <li><code>-f, --format [tsv|jsonl|parquet]</code>: Output format for exported files  [default: tsv]</li> <li><code>--archive</code>: Export as archive (tar) instead of loose files</li> <li><code>--compress</code>: Compress archive as tar.gz (requires --archive)</li> <li><code>--graph-name TEXT</code>: Name for graph files in archive (default: merged_graph)</li> <li><code>--skip-normalize</code>: Skip normalization step</li> <li><code>--skip-prune</code>: Skip pruning step</li> <li><code>--keep-singletons</code>: Keep singleton nodes (default)  [default: True]</li> <li><code>--remove-singletons</code>: Move singleton nodes to separate table</li> <li><code>-q, --quiet</code>: Suppress output</li> <li><code>-p, --progress</code>: Show progress bars  [default: True]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-report","title":"<code>koza report</code>","text":"<p>Generate comprehensive reports for KGX graph databases.</p> <p>Available report types:</p> <p>\u2022 qc: Quality control analysis by data source</p> <p>\u2022 graph-stats: Comprehensive graph statistics (similar to merged_graph_stats.yaml)</p> <p>\u2022 schema: Database schema analysis and biolink compliance</p> <p>Examples:</p> <pre><code># Generate QC report\nkoza report qc -d merged.duckdb -o qc_report.yaml\n\n# Generate graph statistics\nkoza report graph-stats -d merged.duckdb -o graph_stats.yaml\n\n# Generate schema report\nkoza report schema -d merged.duckdb -o schema_report.yaml\n\n# Quick QC analysis (console output only)\nkoza report qc -d merged.duckdb\n</code></pre> <p>Usage:</p> <pre><code>$ koza report [OPTIONS] REPORT_TYPE\n</code></pre> <p>Arguments:</p> <ul> <li><code>REPORT_TYPE</code>: Type of report to generate: qc, graph-stats, or schema  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file  [required]</li> <li><code>-o, --output TEXT</code>: Path to output report file (YAML format)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-node-report","title":"<code>koza node-report</code>","text":"<p>Generate tabular node report with GROUP BY ALL categorical columns.</p> <p>Outputs count of nodes grouped by categorical columns (namespace, category, etc.).</p> <p>Examples:</p> <pre><code># From database\nkoza node-report -d merged.duckdb -o node_report.tsv\n\n# From file\nkoza node-report -f nodes.tsv -o node_report.parquet --format parquet\n\n# Custom columns\nkoza node-report -d merged.duckdb -o report.tsv -c namespace -c category -c provided_by\n</code></pre> <p>Usage:</p> <pre><code>$ koza node-report [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-f, --file TEXT</code>: Path to node file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output report file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-c, --column TEXT</code>: Categorical columns to group by (can specify multiple)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-edge-report","title":"<code>koza edge-report</code>","text":"<p>Generate tabular edge report with denormalized node info.</p> <p>Joins edges to nodes to get subject_category, object_category, etc., then outputs count of edges grouped by categorical columns.</p> <p>Examples:</p> <pre><code># From database\nkoza edge-report -d merged.duckdb -o edge_report.tsv\n\n# From files\nkoza edge-report -n nodes.tsv -e edges.tsv -o edge_report.parquet --format parquet\n\n# Custom columns\nkoza edge-report -d merged.duckdb -o report.tsv \\\n    -c subject_category -c predicate -c object_category -c primary_knowledge_source\n</code></pre> <p>Usage:</p> <pre><code>$ koza edge-report [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-n, --nodes TEXT</code>: Path to node file (for denormalization)</li> <li><code>-e, --edges TEXT</code>: Path to edge file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output report file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-c, --column TEXT</code>: Categorical columns to group by (can specify multiple)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-node-examples","title":"<code>koza node-examples</code>","text":"<p>Generate sample rows per node type.</p> <p>Samples N example rows for each distinct value in the type column (default: category).</p> <p>Examples:</p> <pre><code># From database (5 examples per category)\nkoza node-examples -d merged.duckdb -o node_examples.tsv\n\n# From file with 10 examples per type\nkoza node-examples -f nodes.tsv -o examples.tsv -n 10\n\n# Group by different column\nkoza node-examples -d merged.duckdb -o examples.tsv -t provided_by\n</code></pre> <p>Usage:</p> <pre><code>$ koza node-examples [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-f, --file TEXT</code>: Path to node file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output examples file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-n, --sample-size INTEGER</code>: Number of examples per type  [default: 5]</li> <li><code>-t, --type-column TEXT</code>: Column to partition examples by  [default: category]</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/CLI/#koza-edge-examples","title":"<code>koza edge-examples</code>","text":"<p>Generate sample rows per edge type.</p> <p>Samples N example rows for each distinct combination of type columns (default: subject_category, predicate, object_category).</p> <p>Examples:</p> <pre><code># From database (5 examples per edge type)\nkoza edge-examples -d merged.duckdb -o edge_examples.tsv\n\n# From files with 10 examples\nkoza edge-examples -n nodes.tsv -e edges.tsv -o examples.tsv -s 10\n\n# Custom type columns\nkoza edge-examples -d merged.duckdb -o examples.tsv -t predicate -t primary_knowledge_source\n</code></pre> <p>Usage:</p> <pre><code>$ koza edge-examples [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --database TEXT</code>: Path to DuckDB database file</li> <li><code>-n, --nodes TEXT</code>: Path to node file (for denormalization)</li> <li><code>-e, --edges TEXT</code>: Path to edge file (TSV, JSONL, or Parquet)</li> <li><code>-o, --output TEXT</code>: Path to output examples file</li> <li><code>--format [tsv|parquet|jsonl]</code>: Output format  [default: tsv]</li> <li><code>-s, --sample-size INTEGER</code>: Number of examples per type  [default: 5]</li> <li><code>-t, --type-column TEXT</code>: Columns to partition examples by (can specify multiple)</li> <li><code>-q, --quiet</code>: Suppress progress output</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"Usage/Module/","title":"Module","text":""},{"location":"graph-operations/","title":"Graph Operations","text":"<p>Graph operations provide tools for building, transforming, and analyzing knowledge graphs in KGX format. Built on DuckDB, these operations handle the lifecycle from raw data files to merged graphs.</p>"},{"location":"graph-operations/#quick-start","title":"Quick Start","text":"<p>Combine multiple KGX files into a single graph:</p> <pre><code>koza join \\\n  -n source1_nodes.tsv -n source2_nodes.tsv \\\n  -e source1_edges.tsv -e source2_edges.tsv \\\n  -o my_graph.duckdb\n</code></pre>"},{"location":"graph-operations/#when-to-use-each-operation","title":"When to Use Each Operation","text":"<pre><code>flowchart TD\n    A[KGX Files] --&gt; B{Multiple sources?}\n    B --&gt;|Yes| C[join]\n    B --&gt;|No| D{Need full pipeline?}\n    C --&gt; D\n    D --&gt;|Yes| E[merge]\n    D --&gt;|No| F{Need ID harmonization?}\n    E --&gt; G[Done]\n    F --&gt;|Yes| H[normalize]\n    F --&gt;|No| I{Dangling edges?}\n    H --&gt; I\n    I --&gt;|Yes| J[prune]\n    I --&gt;|No| K{Need subsets?}\n    J --&gt; K\n    K --&gt;|Yes| L[split]\n    K --&gt;|No| G\n    L --&gt; G</code></pre> Operation Use When You Need To... join Combine multiple KGX files into one database merge Run complete pipeline: join \u2192 normalize \u2192 prune split Extract subsets by field value (e.g., by source) normalize Apply SSSOM mappings to harmonize edge identifiers prune Remove dangling edges and optionally singleton nodes append Add new data to an existing database"},{"location":"graph-operations/#documentation-sections","title":"Documentation Sections","text":""},{"location":"graph-operations/#tutorials","title":"Tutorials","text":"<p>Step-by-step lessons for learning graph operations from scratch.</p>"},{"location":"graph-operations/#how-to-guides","title":"How-to Guides","text":"<p>Practical recipes for specific tasks and common workflows.</p>"},{"location":"graph-operations/#reference","title":"Reference","text":"<p>Technical documentation for CLI commands, Python API, and configuration.</p>"},{"location":"graph-operations/#explanation","title":"Explanation","text":"<p>Background concepts and architectural decisions explained.</p>"},{"location":"graph-operations/#key-features","title":"Key Features","text":"<ul> <li>Supported formats: TSV, JSONL, and Parquet files</li> <li>Schema harmonization: Handles different column sets across input files</li> <li>Archive behavior: Problem data is moved to archive tables, not deleted</li> <li>Provenance tracking: Records source attribution for all records</li> <li>SQL access: Graphs can be queried with DuckDB SQL</li> </ul>"},{"location":"graph-operations/explanation/","title":"Explanation","text":"<p>This section covers concepts, architecture, and design decisions behind graph operations.</p>"},{"location":"graph-operations/explanation/#topics","title":"Topics","text":""},{"location":"graph-operations/explanation/#architecture","title":"Architecture","text":"<p>Describes how graph operations are structured:</p> <ul> <li>Why DuckDB? - Columnar storage and SQL for graph processing</li> <li>In-memory vs persistent - Different operational modes</li> <li>Processing pipeline - Data flow through operations</li> <li>GraphDatabase context manager - Connection handling and transactions</li> </ul>"},{"location":"graph-operations/explanation/#schema-handling","title":"Schema Handling","text":"<p>Covers how graph operations manage different schemas:</p> <ul> <li>The schema challenge - Why different sources have different columns</li> <li>UNION ALL BY NAME - DuckDB's schema harmonization approach</li> <li>Auto-detection - How formats and types are inferred</li> <li>Schema evolution - Adding columns during append operations</li> <li>NULL handling - Treatment of missing values</li> </ul>"},{"location":"graph-operations/explanation/#data-integrity","title":"Data Integrity","text":"<p>Explains the non-destructive approach to data quality:</p> <ul> <li>Philosophy: move, don't delete - Why problem data is archived</li> <li>Archive tables - Where problematic data is stored</li> <li>Provenance tracking - How source attribution works</li> <li>Recovery - Retrieving data from archives</li> <li>Use cases - QC and debugging scenarios</li> </ul>"},{"location":"graph-operations/explanation/#biolink-compliance","title":"Biolink Compliance","text":"<p>Describes integration with the Biolink model:</p> <ul> <li>What is Biolink? - The knowledge graph standard</li> <li>Required fields - Minimum columns for valid KGX</li> <li>Multivalued fields - Arrays in node/edge properties</li> <li>Compliance checking - How validation works</li> <li>Common issues - Frequent compliance problems and fixes</li> </ul>"},{"location":"graph-operations/explanation/#key-concepts","title":"Key Concepts","text":""},{"location":"graph-operations/explanation/#kgx-format","title":"KGX Format","text":"<p>Knowledge Graph Exchange (KGX) is a standard format for representing knowledge graphs as node and edge tables. Graph operations work with KGX files in TSV, JSONL, or Parquet format.</p>"},{"location":"graph-operations/explanation/#duckdb","title":"DuckDB","text":"<p>DuckDB is an embedded analytical database with:</p> <ul> <li>Columnar processing</li> <li>SQL query interface</li> <li>In-memory and persistent modes</li> <li>Data compression</li> </ul>"},{"location":"graph-operations/explanation/#sssom","title":"SSSOM","text":"<p>Simple Standard for Sharing Ontological Mappings (SSSOM) is a format for representing identifier mappings. Graph operations use SSSOM files to normalize identifiers during the merge process.</p>"},{"location":"graph-operations/explanation/#design-principles","title":"Design Principles","text":"<ol> <li>Non-destructive: Problem data is moved to archive tables, not deleted</li> <li>Provenance: All records track their source file</li> <li>Flexibility: Operations work with any valid KGX files</li> <li>Performance: DuckDB handles processing of large graphs</li> <li>SQL access: Graphs can be queried directly</li> </ol>"},{"location":"graph-operations/explanation/architecture/","title":"Architecture","text":""},{"location":"graph-operations/explanation/architecture/#overview","title":"Overview","text":"<p>The graph operations module is built on a DuckDB-based architecture that provides high-performance analytical processing for knowledge graphs. At its core is the <code>GraphDatabase</code> class, which manages DuckDB connections and provides a unified interface for loading, transforming, and exporting graph data.</p> <p>The architecture follows a pipeline pattern where data flows through discrete stages: loading into temporary tables, combining with schema-flexible unions, processing (normalization, deduplication, pruning), and finally export or persistence.</p>"},{"location":"graph-operations/explanation/architecture/#why-duckdb","title":"Why DuckDB?","text":"<p>DuckDB was chosen as the foundation for graph operations for these reasons:</p> <p>Columnar storage for analytics workloads: Knowledge graph operations involve scanning and aggregating large datasets - counting nodes by category, grouping edges by predicate, or identifying duplicates. DuckDB's columnar storage is optimized for these analytical queries and outperforms row-oriented databases for this use case.</p> <p>SQL interface for flexibility: Using SQL allows complex graph operations to be expressed declaratively. Operations like finding dangling edges, applying identifier mappings, or generating QC reports can be written as straightforward SQL queries that are easy to understand and modify.</p> <p>In-memory and persistent modes: DuckDB supports both in-memory databases (for quick, temporary processing) and persistent files (for iterative workflows). Users can choose the appropriate mode for their use case without changing their code.</p> <p>Performance with large files: DuckDB can directly read TSV, JSONL, and Parquet files without requiring a separate loading step. Its parallel query execution and efficient compression handle multi-gigabyte knowledge graphs.</p> <p>No external server needed: DuckDB runs as an embedded library. There is no need to install, configure, or manage a separate database server.</p>"},{"location":"graph-operations/explanation/architecture/#in-memory-vs-persistent","title":"In-Memory vs Persistent","text":"<p>The <code>GraphDatabase</code> class supports two operating modes:</p>"},{"location":"graph-operations/explanation/architecture/#in-memory-mode","title":"In-memory mode","text":"<pre><code># Create an in-memory database\nwith GraphDatabase() as db:\n    # All data is temporary\n    pass\n</code></pre> <p>Use in-memory mode when:</p> <ul> <li>Performing one-off transformations or analyses</li> <li>Processing data that fits comfortably in RAM</li> <li>You don't need to preserve intermediate results</li> <li>Running in environments with limited disk space</li> </ul>"},{"location":"graph-operations/explanation/architecture/#persistent-mode","title":"Persistent mode","text":"<pre><code># Create or open a persistent database\nwith GraphDatabase(db_path=Path(\"my_graph.duckdb\")) as db:\n    # Data persists after the context exits\n    pass\n</code></pre> <p>Use persistent mode when:</p> <ul> <li>Working with large graphs that use DuckDB's disk-based storage</li> <li>Building iterative pipelines where you want to inspect intermediate results</li> <li>Running multiple operations over time and want to avoid reloading source files</li> <li>Generating QC reports from a database created by a previous operation</li> </ul>"},{"location":"graph-operations/explanation/architecture/#processing-pipeline","title":"Processing Pipeline","text":"<p>Data flows through the system in a well-defined pipeline:</p>"},{"location":"graph-operations/explanation/architecture/#1-load-files-into-temporary-tables","title":"1. Load files into temporary tables","text":"<p>Each input file is loaded into a uniquely-named temporary table. During loading:</p> <ul> <li>Format is auto-detected from file extension (<code>.tsv</code>, <code>.jsonl</code>, <code>.parquet</code>)</li> <li>A <code>file_source</code> column tracks which file each record came from</li> <li>A <code>provided_by</code> column is optionally generated for provenance tracking</li> <li>Pipe-delimited fields are automatically converted to arrays for multivalued properties</li> </ul> <pre><code>result = db.load_file(file_spec, generate_provided_by=True)\n# Creates temp table like: temp_nodes_my_source_12345678\n</code></pre>"},{"location":"graph-operations/explanation/architecture/#2-combine-with-union-all-by-name","title":"2. Combine with UNION ALL BY NAME","text":"<p>After loading all files, temporary tables are combined into final <code>nodes</code> and <code>edges</code> tables using DuckDB's <code>UNION ALL BY NAME</code>. This approach:</p> <ul> <li>Handles schema differences (files with different columns are merged, with NULL for missing values)</li> <li>Preserves all columns from all input files</li> <li>Avoids the need to pre-define a fixed schema</li> </ul> <pre><code>CREATE TABLE nodes AS\n    SELECT * FROM temp_nodes_file1\n    UNION ALL BY NAME\n    SELECT * FROM temp_nodes_file2\n    UNION ALL BY NAME\n    SELECT * FROM temp_nodes_file3\n</code></pre>"},{"location":"graph-operations/explanation/architecture/#3-process-normalize-deduplicate-prune","title":"3. Process (normalize, deduplicate, prune)","text":"<p>Once data is in the database, various operations can be applied:</p> <ul> <li>Normalize: Apply SSSOM mappings to harmonize identifiers in edge references</li> <li>Deduplicate: Remove duplicate nodes and edges, archiving them for QC analysis</li> <li>Prune: Identify and move dangling edges and optionally singleton nodes</li> </ul> <p>Each operation modifies the main tables in place and may populate archive tables for later inspection.</p>"},{"location":"graph-operations/explanation/architecture/#4-export-or-persist","title":"4. Export or persist","text":"<p>Finally, data can be exported back to file formats:</p> <pre><code># Export to individual files\ndb.export_to_format(\"nodes\", output_path, KGXFormat.TSV)\n\n# Export to a tar archive\ndb.export_to_archive(archive_path, \"my_graph\", KGXFormat.TSV, compress=True)\n\n# Export to loose files\ndb.export_to_loose_files(output_dir, \"my_graph\", KGXFormat.PARQUET)\n</code></pre>"},{"location":"graph-operations/explanation/architecture/#graphdatabase-context-manager","title":"GraphDatabase Context Manager","text":"<p>The <code>GraphDatabase</code> class implements the context manager protocol (<code>__enter__</code> and <code>__exit__</code>) for safe resource management:</p> <pre><code>with GraphDatabase(db_path=Path(\"graph.duckdb\")) as db:\n    # Connection is automatically managed\n    db.load_file(file_spec)\n    stats = db.get_stats()\n# Connection is automatically closed, even if an exception occurs\n</code></pre>"},{"location":"graph-operations/explanation/architecture/#read-only-mode","title":"Read-only mode","text":"<p>For operations that only query data (like generating reports), the database can be opened in read-only mode:</p> <pre><code>with GraphDatabase(db_path, read_only=True) as db:\n    # Safe for concurrent readers\n    stats = db.get_stats()\n    qc_report = generate_qc_report(config)\n</code></pre> <p>Read-only mode:</p> <ul> <li>Allows multiple concurrent readers on the same database file</li> <li>Prevents accidental modifications during reporting</li> <li>Is required when you want to query a database while another process might be writing</li> </ul>"},{"location":"graph-operations/explanation/architecture/#automatic-setup","title":"Automatic setup","text":"<p>When opened in read-write mode (the default), <code>GraphDatabase</code> automatically initializes the database schema, creating necessary tables like <code>file_schemas</code> for tracking loaded file metadata. Main data tables (<code>nodes</code>, <code>edges</code>) are created dynamically when files are loaded, preserving whatever columns exist in the source data.</p>"},{"location":"graph-operations/explanation/architecture/#table-structure","title":"Table Structure","text":"<p>The database uses a consistent table structure across operations:</p>"},{"location":"graph-operations/explanation/architecture/#main-tables","title":"Main tables","text":"Table Description <code>nodes</code> Primary node data with all columns from source files <code>edges</code> Primary edge data with all columns from source files <code>mappings</code> SSSOM mappings for identifier normalization (created during normalize)"},{"location":"graph-operations/explanation/architecture/#archive-tables","title":"Archive tables","text":"<p>Archive tables store records that were removed from main tables during QC operations, allowing later investigation:</p> Table Description <code>dangling_edges</code> Edges where subject or object doesn't exist in nodes table <code>duplicate_nodes</code> Nodes with duplicate IDs (all but the first occurrence) <code>duplicate_edges</code> Edges with duplicate (subject, predicate, object) combinations <code>singleton_nodes</code> Nodes not referenced by any edge (when <code>--remove-singletons</code> is used)"},{"location":"graph-operations/explanation/architecture/#metadata-tables","title":"Metadata tables","text":"Table Description <code>file_schemas</code> Column information for each loaded file, used for schema analysis"},{"location":"graph-operations/explanation/architecture/#sql-access","title":"SQL Access","text":"<p>Since the underlying storage is DuckDB, you can query the database directly using any DuckDB client or the Python API:</p>"},{"location":"graph-operations/explanation/architecture/#using-the-duckdb-cli","title":"Using the DuckDB CLI","text":"<pre><code>duckdb my_graph.duckdb\n\n-- Count nodes by category\nSELECT category, COUNT(*) as count\nFROM nodes\nGROUP BY category\nORDER BY count DESC;\n\n-- Find edges with specific predicates\nSELECT subject, predicate, object\nFROM edges\nWHERE predicate = 'biolink:interacts_with'\nLIMIT 10;\n\n-- Analyze dangling edges\nSELECT\n    split_part(subject, ':', 1) as subject_prefix,\n    COUNT(*) as count\nFROM dangling_edges\nGROUP BY 1\nORDER BY 2 DESC;\n</code></pre>"},{"location":"graph-operations/explanation/architecture/#using-python","title":"Using Python","text":"<pre><code>import duckdb\n\nconn = duckdb.connect(\"my_graph.duckdb\", read_only=True)\n\n# Run custom analytics\nresult = conn.execute(\"\"\"\n    SELECT\n        provided_by,\n        COUNT(DISTINCT id) as node_count,\n        COUNT(DISTINCT category) as category_count\n    FROM nodes\n    GROUP BY provided_by\n\"\"\").fetchdf()\n\nprint(result)\nconn.close()\n</code></pre>"},{"location":"graph-operations/explanation/architecture/#using-the-graphdatabase-connection","title":"Using the GraphDatabase connection","text":"<p>Within operations or custom scripts, you can access the underlying DuckDB connection:</p> <pre><code>with GraphDatabase(db_path) as db:\n    # Access the raw DuckDB connection\n    result = db.conn.execute(\"SELECT COUNT(*) FROM nodes\").fetchone()\n    print(f\"Total nodes: {result[0]}\")\n</code></pre> <p>This direct SQL access supports custom analyses, ad-hoc queries, and integration with other tools that use DuckDB.</p>"},{"location":"graph-operations/explanation/biolink-compliance/","title":"Biolink Compliance","text":"<p>Understanding how graph operations work with the Biolink Model and KGX format.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#overview","title":"Overview","text":"<p>Knowledge graphs in the biomedical domain use standardized data models for interoperability and consistent querying. The Biolink Model defines the vocabulary and structure for representing biological and medical knowledge. Graph operations in Koza work with Biolink-compliant data in the KGX (Knowledge Graph Exchange) format.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#what-is-biolink","title":"What is Biolink?","text":"<p>The Biolink Model is a high-level data model for representing biological and biomedical knowledge graphs. Key aspects include:</p> <ul> <li>Standard vocabulary: Defines consistent terms for node types (categories) and edge types (predicates) across different data sources</li> <li>Node categories: Hierarchical classification of entities like <code>biolink:Gene</code>, <code>biolink:Disease</code>, <code>biolink:ChemicalEntity</code></li> <li>Edge predicates: Standardized relationship types like <code>biolink:treats</code>, <code>biolink:interacts_with</code>, <code>biolink:associated_with</code></li> <li>Slot definitions: Properties that can be attached to nodes and edges, such as <code>name</code>, <code>description</code>, <code>in_taxon</code></li> <li>Maintained by Monarch Initiative: The model evolves based on community needs and is actively maintained</li> </ul> <p>The Biolink Model uses a LinkML schema definition. This schema specifies field definitions, including whether fields are multivalued, required, or optional. Tools can read this schema programmatically.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#kgx-format","title":"KGX Format","text":"<p>KGX (Knowledge Graph Exchange) is the file format that implements the Biolink Model for data exchange. It uses a tabular representation for large-scale data processing.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#node-files","title":"Node Files","text":"<p>Node files contain entity definitions with columns including:</p> Column Description <code>id</code> Unique identifier (CURIE format, e.g., <code>HGNC:1234</code>) <code>category</code> Biolink category (e.g., <code>biolink:Gene</code>) <code>name</code> Human-readable name <code>description</code> Detailed description <code>provided_by</code> Data source attribution"},{"location":"graph-operations/explanation/biolink-compliance/#edge-files","title":"Edge Files","text":"<p>Edge files define relationships between nodes:</p> Column Description <code>subject</code> Source node ID (CURIE) <code>predicate</code> Relationship type (e.g., <code>biolink:interacts_with</code>) <code>object</code> Target node ID (CURIE) <code>id</code> Unique edge identifier <code>category</code> Edge category (e.g., <code>biolink:Association</code>) <code>primary_knowledge_source</code> Original data source"},{"location":"graph-operations/explanation/biolink-compliance/#file-naming-convention","title":"File Naming Convention","text":"<p>KGX files follow a naming convention that graph operations use for automatic detection:</p> <ul> <li><code>*_nodes.tsv</code> or <code>nodes.tsv</code> - Node files</li> <li><code>*_edges.tsv</code> or <code>edges.tsv</code> - Edge files</li> </ul> <p>Supported formats include TSV, JSONL, and Parquet.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#required-fields","title":"Required Fields","text":"<p>For valid KGX data, the following fields are required or strongly recommended:</p>"},{"location":"graph-operations/explanation/biolink-compliance/#nodes","title":"Nodes","text":"Field Requirement Notes <code>id</code> Required Must be a valid CURIE (e.g., <code>HGNC:1234</code>, <code>HP:0001234</code>) <code>category</code> Recommended Biolink category; defaults to <code>biolink:NamedThing</code> if missing <code>name</code> Recommended Human-readable label for the entity"},{"location":"graph-operations/explanation/biolink-compliance/#edges","title":"Edges","text":"Field Requirement Notes <code>subject</code> Required CURIE of the source node <code>predicate</code> Required Biolink predicate for the relationship <code>object</code> Required CURIE of the target node <code>id</code> Recommended Unique identifier for the edge <code>primary_knowledge_source</code> Recommended Attribution for TRAPI compliance"},{"location":"graph-operations/explanation/biolink-compliance/#common-fields","title":"Common Fields","text":"<p>Beyond the required fields, these columns appear frequently in KGX data:</p>"},{"location":"graph-operations/explanation/biolink-compliance/#node-properties","title":"Node Properties","text":"<ul> <li><code>name</code>: Human-readable label</li> <li><code>description</code>: Longer textual description</li> <li><code>provided_by</code>: Data source attribution (used for grouping in QC reports)</li> <li><code>in_taxon</code>: Taxonomic context for biological entities (e.g., <code>NCBITaxon:9606</code> for human)</li> <li><code>in_taxon_label</code>: Human-readable taxon name</li> <li><code>xref</code>: Cross-references to other databases</li> <li><code>synonym</code>: Alternative names</li> </ul>"},{"location":"graph-operations/explanation/biolink-compliance/#edge-properties","title":"Edge Properties","text":"<ul> <li><code>category</code>: Edge category (typically <code>biolink:Association</code> or a subclass)</li> <li><code>negated</code>: Boolean indicating negation of the relationship</li> <li><code>knowledge_level</code>: TRAPI knowledge level (e.g., <code>knowledge_assertion</code>, <code>logical_entailment</code>)</li> <li><code>agent_type</code>: TRAPI agent type (e.g., <code>manual_agent</code>, <code>automated_agent</code>)</li> <li><code>aggregator_knowledge_source</code>: Intermediate data aggregators</li> <li><code>publications</code>: Supporting literature references</li> </ul>"},{"location":"graph-operations/explanation/biolink-compliance/#multivalued-fields","title":"Multivalued Fields","text":"<p>Some Biolink fields can contain multiple values. Graph operations handle these as arrays.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#common-multivalued-fields","title":"Common Multivalued Fields","text":"<p>According to the Biolink Model schema, these fields are defined as multivalued:</p> <ul> <li><code>xref</code> / <code>xrefs</code>: Cross-references to external databases</li> <li><code>synonym</code> / <code>synonyms</code>: Alternative names for entities</li> <li><code>publications</code>: Supporting literature citations</li> <li><code>provided_by</code>: Multiple data sources contributing to a record</li> <li><code>qualifiers</code>: Edge qualifiers for nuanced relationships</li> <li><code>knowledge_source</code>: Knowledge attribution chain</li> <li><code>aggregator_knowledge_source</code>: Multiple aggregating sources</li> </ul>"},{"location":"graph-operations/explanation/biolink-compliance/#array-handling-in-graph-operations","title":"Array Handling in Graph Operations","text":"<p>When loading KGX files, graph operations:</p> <ol> <li>Detect array columns using the Biolink Model schema (via LinkML)</li> <li>Parse pipe-delimited values in TSV files (e.g., <code>PMID:123|PMID:456</code>)</li> <li>Store as native arrays in DuckDB</li> <li>Preserve array structure when exporting back to KGX format</li> </ol> <pre><code># Example: How arrays appear in different formats\n# TSV: xref column contains \"UniProtKB:P12345|ENSEMBL:ENSG00000139618\"\n# DuckDB: xref column stored as ['UniProtKB:P12345', 'ENSEMBL:ENSG00000139618']\n# JSONL: \"xref\": [\"UniProtKB:P12345\", \"ENSEMBL:ENSG00000139618\"]\n</code></pre>"},{"location":"graph-operations/explanation/biolink-compliance/#configuration-for-multivalued-fields","title":"Configuration for Multivalued Fields","text":"<p>Some fields that are technically multivalued in Biolink are treated as single-valued in graph operations:</p> <ul> <li><code>category</code>: While nodes can have multiple categories, operations typically use the most specific one</li> <li><code>in_taxon</code>: Entities usually have a single primary taxon</li> <li><code>type</code>: Similar to category, treated as single-valued</li> </ul> <p>This behavior can be customized through schema configuration.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#compliance-checking","title":"Compliance Checking","text":"<p>Graph operations include tools to verify Biolink compliance of your data.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#schema-report-command","title":"Schema Report Command","text":"<p>Generate a schema analysis report using:</p> <pre><code>koza report schema --database my_graph.duckdb --output schema_report.yaml\n</code></pre> <p>This produces a YAML report containing:</p> <pre><code>metadata:\n  operation: schema\n  generated_at: '2024-01-15 10:30:00'\n  report_version: '1.0'\nschema_analysis:\n  summary:\n    nodes:\n      file_count: 5\n      unique_columns: 12\n      all_columns:\n        - id\n        - category\n        - name\n        - in_taxon\n        # ... more columns\n    edges:\n      file_count: 5\n      unique_columns: 15\n      all_columns:\n        - subject\n        - predicate\n        - object\n        # ... more columns\n  tables:\n    nodes:\n      columns:\n        - name: id\n          type: VARCHAR\n        - name: category\n          type: VARCHAR\n      column_count: 12\n      record_count: 50000\n    edges:\n      columns:\n        - name: subject\n          type: VARCHAR\n        - name: predicate\n          type: VARCHAR\n        - name: object\n          type: VARCHAR\n      column_count: 15\n      record_count: 100000\n  biolink_compliance:\n    status: compliant\n    compliance_percentage: 95.5\n    missing_fields: []\n    extension_fields:\n      - custom_score\n      - source_version\n</code></pre>"},{"location":"graph-operations/explanation/biolink-compliance/#what-compliance-checking-validates","title":"What Compliance Checking Validates","text":"<p>The schema report analyzes:</p> <ol> <li>Required fields present: Ensures <code>id</code> for nodes and <code>subject</code>/<code>predicate</code>/<code>object</code> for edges</li> <li>Column data types: Validates appropriate types (VARCHAR for IDs, arrays for multivalued fields)</li> <li>Biolink slot coverage: Identifies which columns map to standard Biolink slots</li> <li>Extension fields: Lists custom columns not defined in the Biolink Model</li> <li>Schema consistency: Detects variations in column structure across source files</li> </ol>"},{"location":"graph-operations/explanation/biolink-compliance/#using-with-join-operations","title":"Using with Join Operations","text":"<p>Schema reports are automatically generated during join operations when <code>schema_reporting=True</code>:</p> <pre><code>koza join --nodes data/*_nodes.tsv --edges data/*_edges.tsv \\\n    --output merged.duckdb\n# Produces: merged_schema_report.yaml\n</code></pre>"},{"location":"graph-operations/explanation/biolink-compliance/#common-compliance-issues","title":"Common Compliance Issues","text":""},{"location":"graph-operations/explanation/biolink-compliance/#missing-required-fields","title":"Missing Required Fields","text":"<p>Problem: Edges missing <code>subject</code>, <code>predicate</code>, or <code>object</code> columns.</p> <pre><code>Error: Required edge columns missing: ['predicate']\n</code></pre> <p>Solution: Ensure your source data includes all required columns before joining.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#invalid-predicates","title":"Invalid Predicates","text":"<p>Problem: Using predicates not defined in the Biolink Model.</p> <pre><code># Problematic:\npredicate: custom:related_to\n\n# Correct:\npredicate: biolink:related_to\n</code></pre> <p>Solution: Map custom predicates to standard Biolink predicates, or use <code>biolink:related_to</code> as a generic fallback.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#non-standard-categories","title":"Non-Standard Categories","text":"<p>Problem: Node categories that don't exist in Biolink.</p> <pre><code># Problematic:\ncategory: MyDatabase:ProteinEntity\n\n# Correct:\ncategory: biolink:Protein\n</code></pre> <p>Solution: Map source categories to appropriate Biolink categories. Use the category hierarchy to find the most specific valid category.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#malformed-curies","title":"Malformed CURIEs","text":"<p>Problem: IDs that don't follow CURIE format.</p> <pre><code># Problematic:\nid: 12345\nid: http://example.org/entity/12345\n\n# Correct:\nid: EXAMPLE:12345\n</code></pre> <p>Solution: Ensure all IDs follow the <code>prefix:local_id</code> format. Use normalization to standardize IDs.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#missing-provenance","title":"Missing Provenance","text":"<p>Problem: Data without source attribution lacks data lineage tracking.</p> <pre><code># Missing provenance:\n- id: HGNC:1234\n  category: biolink:Gene\n  name: BRCA1\n\n# With provenance:\n- id: HGNC:1234\n  category: biolink:Gene\n  name: BRCA1\n  provided_by: infores:hgnc\n</code></pre> <p>Solution: Use the <code>--generate-provided-by</code> flag during join/merge operations to automatically add provenance from filenames.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#schema-mismatches-across-sources","title":"Schema Mismatches Across Sources","text":"<p>Problem: Different source files have different column structures.</p> <pre><code>schema_analysis:\n  # File A has 10 columns, File B has 15 columns\n  # This causes NULL values in merged data\n</code></pre> <p>Solution: The join operation creates a unified schema automatically. Review the schema report to see which columns come from which sources.</p>"},{"location":"graph-operations/explanation/biolink-compliance/#best-practices","title":"Best Practices","text":"<ol> <li>Validate early: Run schema reports on source files before large merge operations</li> <li>Use standard prefixes: Stick to well-known CURIE prefixes (HGNC, HP, MONDO, etc.)</li> <li>Include provenance: Always populate <code>provided_by</code> or <code>primary_knowledge_source</code></li> <li>Document extensions: If using custom columns, document their purpose and expected values</li> <li>Regular compliance checks: Integrate schema validation into your data pipeline</li> </ol>"},{"location":"graph-operations/explanation/biolink-compliance/#related-documentation","title":"Related Documentation","text":"<ul> <li>Schema Handling - How graph operations manage schema evolution</li> <li>Data Integrity - Ensuring data quality during operations</li> <li>Generate Reports How-To - Practical guide to report generation</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/","title":"Data Integrity","text":""},{"location":"graph-operations/explanation/data-integrity/#overview","title":"Overview","text":"<p>Koza graph operations use non-destructive data operations. When problems are detected in graph data (duplicate entries, dangling edges, or isolated nodes), the problematic records are moved to archive tables rather than deleted. These archived records remain accessible for analysis, debugging, and recovery.</p> <p>\"Problem\" data often contains useful information:</p> <ul> <li>Duplicates may reveal integration issues between data sources</li> <li>Dangling edges may indicate missing node files or ID mismatches</li> <li>Singletons may represent valid entities that lack relationships in the current dataset</li> </ul> <p>Preserving this data enables quality control analysis without irreversible data loss.</p>"},{"location":"graph-operations/explanation/data-integrity/#move-dont-delete","title":"Move, Don't Delete","text":"<p>The core principle: move problem data to archive tables rather than deleting it.</p> <p>Debugging and QC: When edges reference non-existent nodes, the archived edges show which nodes are missing and from which source files. This helps identify upstream data issues.</p> <p>Recovery: If data is archived incorrectly (due to misconfiguration or upstream bugs), it can be recovered from archive tables without re-running the entire pipeline.</p> <p>Audit trail: Archive tables document what was removed and when. This supports reproducibility and explains changes between pipeline runs.</p> <p>No data loss: Cleaning operations preserve all original data. You can inspect what was removed and verify that the cleaning logic behaved correctly.</p>"},{"location":"graph-operations/explanation/data-integrity/#archive-tables","title":"Archive Tables","text":"<p>Koza creates several archive tables during graph operations:</p>"},{"location":"graph-operations/explanation/data-integrity/#dangling_edges","title":"dangling_edges","text":"<p>Contains edges where the <code>subject</code> or <code>object</code> ID does not exist in the nodes table.</p> <pre><code>-- Example: View dangling edges\nSELECT * FROM dangling_edges LIMIT 10;\n\n-- Find which nodes are missing\nSELECT DISTINCT subject as missing_node\nFROM dangling_edges e\nLEFT JOIN nodes n ON e.subject = n.id\nWHERE n.id IS NULL;\n</code></pre> <p>Dangling edges typically indicate:</p> <ul> <li>Node files that failed to load</li> <li>ID mismatches between node and edge files</li> <li>Normalization that mapped to non-existent canonical IDs</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/#duplicate_nodes","title":"duplicate_nodes","text":"<p>Contains all rows that had duplicate IDs in the nodes table. When multiple rows share the same <code>id</code>, all are copied here. Only the first occurrence (ordered by <code>file_source</code>) is kept in the main <code>nodes</code> table.</p> <pre><code>-- Example: View duplicate nodes\nSELECT id, COUNT(*) as occurrence_count\nFROM duplicate_nodes\nGROUP BY id\nORDER BY occurrence_count DESC;\n\n-- Compare duplicates for a specific ID\nSELECT * FROM duplicate_nodes WHERE id = 'HGNC:1234';\n</code></pre> <p>Duplicate nodes may indicate:</p> <ul> <li>The same entity appearing in multiple source files</li> <li>Version conflicts between data sources</li> <li>Intentional overlaps that need resolution</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/#duplicate_edges","title":"duplicate_edges","text":"<p>Contains all rows that had duplicate <code>id</code> values in the edges table. All duplicates are archived. Only the first occurrence is retained.</p> <pre><code>-- Example: View duplicate edges by source\nSELECT file_source, COUNT(*) as duplicates\nFROM duplicate_edges\nGROUP BY file_source\nORDER BY duplicates DESC;\n</code></pre>"},{"location":"graph-operations/explanation/data-integrity/#singleton_nodes","title":"singleton_nodes","text":"<p>Contains nodes that do not appear as <code>subject</code> or <code>object</code> in any edge. This table is only populated when you explicitly request singleton removal with <code>--remove-singletons</code>.</p> <pre><code>-- Example: Analyze singleton nodes by category\nSELECT category, COUNT(*) as count\nFROM singleton_nodes\nGROUP BY category\nORDER BY count DESC;\n</code></pre> <p>Singleton nodes may represent:</p> <ul> <li>Valid entities that simply lack relationships in your dataset</li> <li>Nodes from incomplete data sources</li> <li>Orphaned entries from failed edge loading</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/#provenance-tracking","title":"Provenance Tracking","text":"<p>Koza tracks the source of each record through provenance columns. This enables source-aware deduplication and QC analysis.</p>"},{"location":"graph-operations/explanation/data-integrity/#file_source-column","title":"file_source Column","text":"<p>When loading files, Koza adds a <code>file_source</code> column containing the source identifier:</p> <pre><code># Source name is derived from filename by default\nkoza join --nodes gene_nodes.tsv --edges gene_edges.tsv -d graph.duckdb\n\n# Or specify explicitly\nkoza join --nodes gene_nodes.tsv:gene_source --edges gene_edges.tsv:gene_source -d graph.duckdb\n</code></pre> <p>The <code>file_source</code> column provides:</p> <ul> <li>Record-to-file traceability</li> <li>Deterministic ordering during deduplication</li> <li>Source-specific QC reports</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/#provided_by-column","title":"provided_by Column","text":"<p>The <code>provided_by</code> column is a standard KGX provenance field. It may already exist in your source data. If present, Koza preserves it. If absent, Koza can generate it from the source name.</p>"},{"location":"graph-operations/explanation/data-integrity/#ordering-during-deduplication","title":"Ordering During Deduplication","text":"<p>When duplicates are found, Koza keeps the \"first\" occurrence based on ordering by provenance columns:</p> <ol> <li><code>file_source</code> (preferred) - Added by Koza during loading</li> <li><code>provided_by</code> (fallback) - Standard KGX provenance column</li> <li>Constant (last resort) - Arbitrary but deterministic ordering</li> </ol> <p>This means you can control which version of a duplicate is kept by ordering your input files appropriately or by setting source names that sort in your preferred order.</p>"},{"location":"graph-operations/explanation/data-integrity/#original-value-preservation","title":"Original Value Preservation","text":"<p>When normalizing identifiers using SSSOM mappings, Koza preserves the original values. This allows tracing back to the source data.</p>"},{"location":"graph-operations/explanation/data-integrity/#original_subject-and-original_object-columns","title":"original_subject and original_object Columns","text":"<p>After normalization, edges gain two new columns:</p> <ul> <li><code>original_subject</code>: The subject ID before normalization (NULL if unchanged)</li> <li><code>original_object</code>: The object ID before normalization (NULL if unchanged)</li> </ul> <pre><code>-- Example: Find edges that were normalized\nSELECT subject, original_subject, object, original_object\nFROM edges\nWHERE original_subject IS NOT NULL\n   OR original_object IS NOT NULL;\n\n-- Compare before and after\nSELECT\n    original_subject as before,\n    subject as after,\n    COUNT(*) as edge_count\nFROM edges\nWHERE original_subject IS NOT NULL\nGROUP BY original_subject, subject\nORDER BY edge_count DESC;\n</code></pre>"},{"location":"graph-operations/explanation/data-integrity/#capabilities-of-preserving-originals","title":"Capabilities of Preserving Originals","text":"<ul> <li>Debugging: Check what IDs an edge had before normalization</li> <li>Validation: Compare normalized IDs against expected mappings</li> <li>Reversibility: Reconstruct the original graph if needed</li> <li>Provenance: Full audit trail of transformations applied to each record</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/#recovery-via-sql","title":"Recovery via SQL","text":"<p>Archive tables are standard DuckDB tables. You can query them directly and recover data if needed.</p>"},{"location":"graph-operations/explanation/data-integrity/#query-archive-tables","title":"Query Archive Tables","text":"<pre><code>-- Connect to the database\n-- duckdb graph.duckdb\n\n-- View all archive tables\nSHOW TABLES;\n\n-- Count records in each archive\nSELECT 'dangling_edges' as table_name, COUNT(*) as count FROM dangling_edges\nUNION ALL\nSELECT 'duplicate_nodes', COUNT(*) FROM duplicate_nodes\nUNION ALL\nSELECT 'duplicate_edges', COUNT(*) FROM duplicate_edges\nUNION ALL\nSELECT 'singleton_nodes', COUNT(*) FROM singleton_nodes;\n</code></pre>"},{"location":"graph-operations/explanation/data-integrity/#re-insert-recovered-data","title":"Re-insert Recovered Data","text":"<p>If you determine that archived data should be restored:</p> <pre><code>-- Restore specific dangling edges (perhaps after adding missing nodes)\nINSERT INTO edges\nSELECT * FROM dangling_edges\nWHERE file_source = 'my_source';\n\n-- Restore singleton nodes\nINSERT INTO nodes\nSELECT * FROM singleton_nodes\nWHERE category = 'biolink:Gene';\n</code></pre>"},{"location":"graph-operations/explanation/data-integrity/#analyze-patterns-in-problem-data","title":"Analyze Patterns in Problem Data","text":"<pre><code>-- Find common patterns in dangling edges\nSELECT\n    file_source,\n    COUNT(*) as dangling_count,\n    COUNT(DISTINCT subject) as unique_subjects,\n    COUNT(DISTINCT object) as unique_objects\nFROM dangling_edges\nGROUP BY file_source\nORDER BY dangling_count DESC;\n\n-- Identify which source files contribute most duplicates\nSELECT\n    file_source,\n    COUNT(DISTINCT id) as duplicate_ids,\n    COUNT(*) as total_duplicate_rows\nFROM duplicate_nodes\nGROUP BY file_source\nORDER BY duplicate_ids DESC;\n</code></pre>"},{"location":"graph-operations/explanation/data-integrity/#why-this-matters","title":"Why This Matters","text":"<p>Non-destructive data operations support production knowledge graph workflows:</p>"},{"location":"graph-operations/explanation/data-integrity/#qc-analysis","title":"QC Analysis","text":"<p>Archive tables support quality control analysis:</p> <pre><code>-- Generate a QC summary\nSELECT\n    (SELECT COUNT(*) FROM nodes) as active_nodes,\n    (SELECT COUNT(*) FROM edges) as active_edges,\n    (SELECT COUNT(*) FROM dangling_edges) as dangling_edges,\n    (SELECT COUNT(*) FROM duplicate_nodes) as duplicate_node_rows,\n    (SELECT COUNT(*) FROM singleton_nodes) as singleton_nodes;\n</code></pre>"},{"location":"graph-operations/explanation/data-integrity/#debugging-data-issues","title":"Debugging Data Issues","text":"<p>When something looks wrong in your graph, archive tables help answer:</p> <ul> <li>Why are certain edges missing? (Check <code>dangling_edges</code>)</li> <li>Why is this node's data different than expected? (Check <code>duplicate_nodes</code>)</li> <li>Why are some entities disconnected? (Check <code>singleton_nodes</code>)</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/#compliance-and-auditing","title":"Compliance and Auditing","text":"<p>For regulated environments or reproducible science:</p> <ul> <li>Full provenance: Every record can be traced to its source file</li> <li>Complete audit trail: Archive tables document all removals</li> <li>Reproducibility: Re-running with the same inputs produces the same outputs</li> <li>Transparency: QC reports based on archive tables explain exactly what was cleaned</li> </ul>"},{"location":"graph-operations/explanation/data-integrity/#safe-iteration","title":"Safe Iteration","text":"<p>Non-destructive operations allow iteration on pipelines:</p> <ul> <li>Try aggressive cleaning, inspect results, adjust parameters</li> <li>Compare archive table contents between runs</li> <li>Recover from mistakes without re-running upstream processing</li> </ul> <p>This approach ensures that Koza graph operations preserve data while allowing cleaning and transformation.</p>"},{"location":"graph-operations/explanation/schema-handling/","title":"Schema Handling","text":""},{"location":"graph-operations/explanation/schema-handling/#overview","title":"Overview","text":"<p>When building knowledge graphs from multiple sources, handling different schemas is a common challenge. Each data source may use different columns, different naming conventions, and different data types. Graph operations use DuckDB's schema harmonization features to combine these heterogeneous files into a unified graph.</p> <p>This document explains how schema handling works, from format detection through schema evolution, so you can understand what happens when your files have different columns.</p>"},{"location":"graph-operations/explanation/schema-handling/#the-schema-challenge","title":"The Schema Challenge","text":""},{"location":"graph-operations/explanation/schema-handling/#why-different-kgx-files-have-different-columns","title":"Why Different KGX Files Have Different Columns","text":"<p>Knowledge graph data comes from many different sources, and each source may have different characteristics:</p> <p>Different data sources: A gene annotation database exports different fields than a disease ontology. One might include <code>taxon</code> and <code>chromosome</code>, while another includes <code>definition</code> and <code>synonyms</code>.</p> <p>Different Biolink slots used: The Biolink model defines many optional slots for nodes and edges. A protein interaction dataset might use <code>has_evidence</code> and <code>supporting_publications</code>, while a chemical database uses <code>has_chemical_formula</code> and <code>molecular_weight</code>.</p> <p>Optional vs required fields: While KGX defines core fields like <code>id</code>, <code>name</code>, <code>category</code> for nodes and <code>subject</code>, <code>predicate</code>, <code>object</code> for edges, most other fields are optional. Different sources populate different subsets.</p> <p>Custom extensions: Some data sources include custom columns beyond the Biolink model, such as source-specific identifiers, confidence scores, or annotation metadata.</p>"},{"location":"graph-operations/explanation/schema-handling/#example-schema-variation-in-practice","title":"Example: Schema Variation in Practice","text":"<p>Consider combining three node files:</p> <pre><code># gene_nodes.tsv (5 columns)\nid    name    category    taxon    chromosome\n\n# disease_nodes.tsv (4 columns)\nid    name    category    definition\n\n# chemical_nodes.tsv (6 columns)\nid    name    category    formula    smiles    molecular_weight\n</code></pre> <p>When these files are combined, the result must include all columns from all sources, with appropriate handling for missing values.</p>"},{"location":"graph-operations/explanation/schema-handling/#union-all-by-name","title":"UNION ALL BY NAME","text":""},{"location":"graph-operations/explanation/schema-handling/#duckdbs-schema-harmonization-strategy","title":"DuckDB's Schema Harmonization Strategy","text":"<p>Graph operations use DuckDB's <code>UNION ALL BY NAME</code> feature to combine files with different schemas. This is the key mechanism that makes heterogeneous file handling possible.</p>"},{"location":"graph-operations/explanation/schema-handling/#how-it-works","title":"How It Works","text":"<p>Columns matched by name, not position: Unlike traditional <code>UNION ALL</code> which requires identical column order, <code>UNION ALL BY NAME</code> matches columns by their names. This means files can have columns in any order.</p> <p>Missing columns filled with NULL: When a file lacks a column that exists in another file, DuckDB fills those cells with <code>NULL</code>. No data is lost.</p> <p>All columns from all sources preserved: The final table includes every column from every source file. No manual schema mapping is required.</p>"},{"location":"graph-operations/explanation/schema-handling/#example","title":"Example","text":"<pre><code>-- DuckDB automatically handles schema differences\nCREATE TABLE nodes AS\nSELECT * FROM temp_genes\nUNION ALL BY NAME\nSELECT * FROM temp_diseases\nUNION ALL BY NAME\nSELECT * FROM temp_chemicals\n</code></pre> <p>Result schema: <code>id</code>, <code>name</code>, <code>category</code>, <code>taxon</code>, <code>chromosome</code>, <code>definition</code>, <code>formula</code>, <code>smiles</code>, <code>molecular_weight</code></p> <p>For genes: <code>definition</code>, <code>formula</code>, <code>smiles</code>, <code>molecular_weight</code> will be <code>NULL</code> For diseases: <code>taxon</code>, <code>chromosome</code>, <code>formula</code>, <code>smiles</code>, <code>molecular_weight</code> will be <code>NULL</code> For chemicals: <code>taxon</code>, <code>chromosome</code>, <code>definition</code> will be <code>NULL</code></p>"},{"location":"graph-operations/explanation/schema-handling/#format-auto-detection","title":"Format Auto-Detection","text":""},{"location":"graph-operations/explanation/schema-handling/#how-format-is-detected","title":"How Format Is Detected","text":"<p>Graph operations detect file formats automatically. The <code>FileSpec</code> model handles this through field validators.</p>"},{"location":"graph-operations/explanation/schema-handling/#file-extension-detection","title":"File Extension Detection","text":"<p>The format is detected from the file extension:</p> Extension Format <code>.tsv</code>, <code>.txt</code> TSV (tab-separated values) <code>.jsonl</code>, <code>.json</code> JSONL (JSON Lines) <code>.parquet</code> Parquet"},{"location":"graph-operations/explanation/schema-handling/#compression-detection","title":"Compression Detection","text":"<p>The system handles compressed files by first stripping the compression extension, then detecting the underlying format:</p> File Detected Format <code>nodes.tsv.gz</code> TSV (compressed) <code>edges.jsonl.bz2</code> JSONL (compressed) <code>data.parquet.xz</code> Parquet (compressed) <p>DuckDB reads compressed files without manual decompression.</p>"},{"location":"graph-operations/explanation/schema-handling/#file-type-detection","title":"File Type Detection","text":"<p>The system also auto-detects whether a file contains nodes or edges based on filename patterns:</p> <ul> <li>Files containing <code>_nodes.</code> or starting with <code>nodes.</code> are detected as node files</li> <li>Files containing <code>_edges.</code> or starting with <code>edges.</code> are detected as edge files</li> </ul>"},{"location":"graph-operations/explanation/schema-handling/#type-inference","title":"Type Inference","text":""},{"location":"graph-operations/explanation/schema-handling/#how-column-types-are-determined","title":"How Column Types Are Determined","text":"<p>DuckDB performs type inference when reading files. The behavior varies by format:</p> <p>TSV files: By default, graph operations read TSV files with <code>all_varchar=true</code>, treating all columns as text. This avoids data loss from type mismatches and handles mixed-type columns consistently.</p> <p>JSONL files: DuckDB infers types from the JSON structure. Arrays become <code>VARCHAR[]</code> (array of strings), numbers become appropriate numeric types, and strings remain <code>VARCHAR</code>. The option <code>convert_strings_to_integers=false</code> prevents UUID-like strings from being incorrectly parsed as integers.</p> <p>Parquet files: Types are preserved exactly as stored in the Parquet schema, since Parquet is a typed format.</p>"},{"location":"graph-operations/explanation/schema-handling/#array-detection-for-multivalued-fields","title":"Array Detection for Multivalued Fields","text":"<p>Graph operations identify multivalued fields (like <code>synonym</code>, <code>xref</code>, <code>publications</code>) and convert them appropriately:</p> <ol> <li>Schema-based detection: The system consults the Biolink model schema to determine which fields are defined as multivalued</li> <li>Fallback list: When the schema is unavailable, a hardcoded list of common KGX multivalued fields is used</li> <li>Transformation: For TSV files, pipe-delimited values (<code>value1|value2|value3</code>) are converted to arrays</li> </ol> <pre><code># Fields that are always treated as single-valued (overrides schema)\nFORCE_SINGLE_VALUED_FIELDS = {\"category\", \"in_taxon\", \"type\"}\n\n# Fallback multivalued fields when schema unavailable\nKGX_MULTIVALUED_FIELDS_FALLBACK = {\n    \"xref\", \"synonym\", \"publications\", \"provided_by\",\n    \"qualifiers\", \"knowledge_source\", \"has_evidence\", ...\n}\n</code></pre>"},{"location":"graph-operations/explanation/schema-handling/#schema-evolution-with-append","title":"Schema Evolution with Append","text":""},{"location":"graph-operations/explanation/schema-handling/#how-new-columns-are-added","title":"How New Columns Are Added","text":"<p>The append operation supports schema evolution, allowing you to add files with new columns to an existing database.</p>"},{"location":"graph-operations/explanation/schema-handling/#the-process","title":"The Process","text":"<ol> <li>Load new file into temp table: The new file is loaded with its full schema</li> <li>Compare schemas: The temp table schema is compared to the existing table</li> <li>ALTER TABLE ADD COLUMN: Any columns in the new file that do not exist in the target table are added via <code>ALTER TABLE</code></li> <li>Insert with UNION ALL BY NAME: Data is inserted using schema-compatible union</li> </ol> <pre><code># Schema evolution in append operation\nnew_columns = set(file_columns.keys()) - set(existing_schema.keys())\nfor col_name in new_columns:\n    col_type = file_columns[col_name]\n    db.conn.execute(f\"ALTER TABLE {table_type} ADD COLUMN {col_name} {col_type}\")\n</code></pre>"},{"location":"graph-operations/explanation/schema-handling/#null-backfill-for-existing-rows","title":"NULL Backfill for Existing Rows","text":"<p>When a new column is added to an existing table, all existing rows receive <code>NULL</code> for that column. DuckDB does this automatically.</p>"},{"location":"graph-operations/explanation/schema-handling/#example_1","title":"Example","text":"<pre><code># Original database has nodes with: id, name, category\n\n# New file has: id, name, category, taxon, description\n\n# After append:\n# - taxon and description columns are added\n# - Original nodes have NULL for taxon and description\n# - New nodes have all five fields populated\n</code></pre>"},{"location":"graph-operations/explanation/schema-handling/#null-handling","title":"NULL Handling","text":""},{"location":"graph-operations/explanation/schema-handling/#what-happens-with-missing-values","title":"What Happens with Missing Values","text":"<p>NULL values occur during schema harmonization. Here is how they flow through operations:</p> <p>During join: Missing columns are filled with <code>NULL</code> via <code>UNION ALL BY NAME</code></p> <p>During append: New columns in existing rows are <code>NULL</code>; missing columns in new rows are <code>NULL</code></p> <p>During export: <code>NULL</code> values are preserved in the output format: - TSV: Empty cell (no value between tabs) - JSONL: Field omitted from the JSON object - Parquet: Native NULL representation</p> <p>In queries: Use <code>IS NULL</code> / <code>IS NOT NULL</code> to filter, or <code>COALESCE()</code> to provide defaults</p>"},{"location":"graph-operations/explanation/schema-handling/#nulls-vs-empty-arrays","title":"NULLs vs Empty Arrays","text":"<p>For multivalued fields, there is a distinction:</p> <ul> <li><code>NULL</code>: The field has no value (not provided)</li> <li><code>[]</code> (empty array): The field was provided but contains no values</li> <li><code>['value']</code>: The field contains one or more values</li> </ul> <p>Graph operations preserve this distinction when possible. TSV format cannot distinguish between <code>NULL</code> and empty string.</p>"},{"location":"graph-operations/explanation/schema-handling/#best-practices","title":"Best Practices","text":""},{"location":"graph-operations/explanation/schema-handling/#tips-for-maintaining-clean-schemas","title":"Tips for Maintaining Clean Schemas","text":"<p>Use consistent column naming: Follow Biolink naming conventions (<code>snake_case</code>) across all your source files.</p> <p>Document your extensions: If you add custom columns beyond Biolink, document what they contain.</p> <p>Validate before joining: Use the schema report feature (<code>--schema-reporting</code>) to analyze schemas before combining files.</p> <p>Consider output format: If schema consistency matters for downstream tools, consider exporting to Parquet which preserves exact types.</p> <p>Handle multivalued fields consistently: Use pipe-delimited format (<code>value1|value2</code>) in TSV files for multivalued fields.</p> <p>Review NULL patterns: After joining, query for NULL patterns to understand data coverage:</p> <pre><code>-- Find columns with high NULL rates\nSELECT\n    'taxon' as column_name,\n    COUNT(*) as total,\n    COUNT(taxon) as non_null,\n    ROUND(100.0 * COUNT(taxon) / COUNT(*), 1) as coverage_pct\nFROM nodes\n</code></pre> <p>Use provided_by for provenance: Enable <code>generate_provided_by</code> (default) to track which source each record came from. This allows investigation of schema differences by source.</p>"},{"location":"graph-operations/how-to/","title":"How-to Guides","text":"<p>Step-by-step instructions for specific graph operation tasks. Each guide focuses on a single goal.</p>"},{"location":"graph-operations/how-to/#data-loading-combination","title":"Data Loading &amp; Combination","text":""},{"location":"graph-operations/how-to/#join-files","title":"Join Files","text":"<p>Combine multiple KGX files into a unified database. Covers basic joins, mixed formats, glob patterns, and schema reporting.</p>"},{"location":"graph-operations/how-to/#incremental-updates","title":"Incremental Updates","text":"<p>Add new data to an existing database, with options for schema updates and deduplication.</p>"},{"location":"graph-operations/how-to/#data-transformation","title":"Data Transformation","text":""},{"location":"graph-operations/how-to/#split-graphs","title":"Split Graphs","text":"<p>Divide a graph into subsets based on field values such as source, category, or other attributes.</p>"},{"location":"graph-operations/how-to/#normalize-ids","title":"Normalize IDs","text":"<p>Apply SSSOM mappings to harmonize identifiers across different naming conventions and ontologies.</p>"},{"location":"graph-operations/how-to/#clean-graphs","title":"Clean Graphs","text":"<p>Remove duplicates, dangling edges, and optionally singleton nodes from a graph.</p>"},{"location":"graph-operations/how-to/#reporting-analysis","title":"Reporting &amp; Analysis","text":""},{"location":"graph-operations/how-to/#generate-reports","title":"Generate Reports","text":"<p>Create QC reports, graph statistics, schema compliance reports, and tabular summaries.</p>"},{"location":"graph-operations/how-to/#export-formats","title":"Export Formats","text":"<p>Export your graph to TSV, JSONL, or Parquet format, with optional archiving and compression.</p>"},{"location":"graph-operations/how-to/#guide-format","title":"Guide Format","text":"<p>Each how-to guide follows a consistent structure:</p> <ol> <li>Goal: What you'll accomplish</li> <li>Prerequisites: What you need before starting</li> <li>Steps: Numbered instructions with examples</li> <li>Verification: How to confirm success</li> <li>Variations: Alternative approaches and options</li> </ol>"},{"location":"graph-operations/how-to/#quick-reference","title":"Quick Reference","text":"Task Command Guide Combine files <code>koza join</code> Join Files Add to existing <code>koza append</code> Incremental Updates Extract subset <code>koza split</code> Split Graphs Harmonize IDs <code>koza normalize</code> Normalize IDs Remove issues <code>koza prune</code> / <code>koza deduplicate</code> Clean Graphs Quality reports <code>koza report</code> Generate Reports Format conversion <code>koza split --format</code> Export Formats"},{"location":"graph-operations/how-to/clean-graph/","title":"How to Clean Graphs","text":""},{"location":"graph-operations/how-to/clean-graph/#goal","title":"Goal","text":"<p>Remove duplicates, dangling edges, and optionally singleton nodes from your knowledge graph. This guide covers cleanup operations that identify and archive problematic data while preserving it for later inspection.</p>"},{"location":"graph-operations/how-to/clean-graph/#prerequisites","title":"Prerequisites","text":"<ul> <li>A DuckDB database containing your graph (created via <code>koza join</code>, <code>koza merge</code>, or <code>koza append</code>)</li> <li>Koza installed and available in your PATH</li> </ul>"},{"location":"graph-operations/how-to/clean-graph/#removing-dangling-edges","title":"Removing Dangling Edges","text":"<p>Dangling edges are edges that reference nodes which do not exist in the graph. This commonly occurs when:</p> <ul> <li>Node and edge files come from different sources with incomplete overlap</li> <li>Data has been filtered or subset, leaving orphaned edge references</li> <li>There are ID mismatches between node definitions and edge references</li> </ul> <p>Use the <code>prune</code> command to identify and remove dangling edges:</p> <pre><code>koza prune graph.duckdb --keep-singletons\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#how-it-works","title":"How It Works","text":"<p>The prune operation:</p> <ol> <li>Identifies edges where the <code>subject</code> does not match any node <code>id</code></li> <li>Identifies edges where the <code>object</code> does not match any node <code>id</code></li> <li>Moves these dangling edges to the <code>dangling_edges</code> archive table</li> <li>Reports statistics on what was found and moved</li> </ol>"},{"location":"graph-operations/how-to/clean-graph/#example-output","title":"Example Output","text":"<pre><code>Prune completed successfully\n  - 156 dangling edges moved to dangling_edges table\n  - 23 singleton nodes preserved (--keep-singletons)\n  - Main graph: 174,844 connected edges remain\nDangling edges by source:\n  - source_a: 89 edges (missing 12 target nodes)\n  - source_b: 67 edges (missing 8 target nodes)\n</code></pre> <p>Non-Destructive Operation</p> <p>Dangling edges are moved to the <code>dangling_edges</code> table, not deleted. You can always inspect or recover this data later.</p>"},{"location":"graph-operations/how-to/clean-graph/#handling-singleton-nodes","title":"Handling Singleton Nodes","text":"<p>Singleton nodes are nodes that have no edges connecting them to other nodes. Depending on your use case, you may want to keep or remove them.</p>"},{"location":"graph-operations/how-to/clean-graph/#keep-singletons-default","title":"Keep Singletons (Default)","text":"<p>Use <code>--keep-singletons</code> to preserve isolated nodes in your graph:</p> <pre><code>koza prune graph.duckdb --keep-singletons\n</code></pre> <p>Use this option when:</p> <ul> <li>Nodes have standalone meaning (e.g., ontology terms, reference data)</li> <li>Edges connecting these nodes may be added later</li> <li>All node metadata should be preserved regardless of connectivity</li> </ul>"},{"location":"graph-operations/how-to/clean-graph/#remove-singletons","title":"Remove Singletons","text":"<p>Use <code>--remove-singletons</code> to move isolated nodes to an archive table:</p> <pre><code>koza prune graph.duckdb --remove-singletons\n</code></pre> <p>Use this option when:</p> <ul> <li>Only nodes that participate in relationships are needed</li> <li>The graph will be used primarily for traversal queries</li> <li>Isolated nodes represent incomplete data</li> </ul> <p>When singletons are removed, they are moved to the <code>singleton_nodes</code> table for later inspection.</p>"},{"location":"graph-operations/how-to/clean-graph/#deduplicating-nodes-and-edges","title":"Deduplicating Nodes and Edges","text":"<p>Deduplication is performed automatically as part of the <code>merge</code> or <code>append</code> pipelines. There is no standalone <code>koza deduplicate</code> command.</p>"},{"location":"graph-operations/how-to/clean-graph/#how-it-works_1","title":"How It Works","text":"<p>For nodes:</p> <ol> <li>Nodes are grouped by their <code>id</code> field</li> <li>For duplicate IDs, the first occurrence is kept (ordered by <code>file_source</code> or <code>provided_by</code>)</li> <li>All other occurrences are moved to the <code>duplicate_nodes</code> archive table</li> </ol> <p>For edges:</p> <ol> <li>Edges are grouped by their <code>id</code> field (or by subject-predicate-object if no ID)</li> <li>For duplicates, the first occurrence is kept</li> <li>All other occurrences are moved to the <code>duplicate_edges</code> archive table</li> </ol> <p>This applies when:</p> <ul> <li>Multiple source files define the same node with different attributes</li> <li>Appended data overlaps with existing nodes</li> <li>Deterministic, unique node records are required</li> </ul>"},{"location":"graph-operations/how-to/clean-graph/#using-deduplication-via-append","title":"Using Deduplication via Append","text":"<p>To deduplicate an existing database, use the <code>--deduplicate</code> flag with append:</p> <pre><code>koza append graph.duckdb --deduplicate\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#inspecting-archived-data","title":"Inspecting Archived Data","text":"<p>All cleanup operations preserve data in archive tables. You can inspect these using SQL queries.</p>"},{"location":"graph-operations/how-to/clean-graph/#view-dangling-edges","title":"View Dangling Edges","text":"<pre><code>-- Connect to the database\n-- duckdb graph.duckdb\n\n-- View sample of dangling edges\nSELECT * FROM dangling_edges LIMIT 10;\n\n-- Count dangling edges by source\nSELECT\n    file_source,\n    COUNT(*) as count\nFROM dangling_edges\nGROUP BY file_source\nORDER BY count DESC;\n\n-- Find which nodes are missing\nSELECT DISTINCT subject\nFROM dangling_edges\nWHERE subject NOT IN (SELECT id FROM nodes);\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#view-duplicate-nodes","title":"View Duplicate Nodes","text":"<pre><code>-- View sample of duplicate nodes\nSELECT * FROM duplicate_nodes LIMIT 10;\n\n-- Count duplicates by category\nSELECT\n    category,\n    COUNT(*) as duplicate_count\nFROM duplicate_nodes\nGROUP BY category\nORDER BY duplicate_count DESC;\n\n-- See all versions of a specific duplicated node\nSELECT * FROM duplicate_nodes WHERE id = 'MONDO:0005148';\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#view-duplicate-edges","title":"View Duplicate Edges","text":"<pre><code>-- View sample of duplicate edges\nSELECT * FROM duplicate_edges LIMIT 10;\n\n-- Count duplicates by predicate\nSELECT\n    predicate,\n    COUNT(*) as duplicate_count\nFROM duplicate_edges\nGROUP BY predicate\nORDER BY duplicate_count DESC;\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#view-singleton-nodes-if-removed","title":"View Singleton Nodes (if removed)","text":"<pre><code>-- View sample of singleton nodes\nSELECT * FROM singleton_nodes LIMIT 10;\n\n-- Count singletons by category\nSELECT\n    category,\n    COUNT(*) as singleton_count\nFROM singleton_nodes\nGROUP BY category\nORDER BY singleton_count DESC;\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#combined-cleanup-with-merge","title":"Combined Cleanup with Merge","text":"<p>For a complete cleanup pipeline, the <code>merge</code> command combines all cleanup operations in sequence:</p> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output clean_graph.duckdb\n</code></pre> <p>The merge pipeline runs: join -&gt; deduplicate -&gt; normalize -&gt; prune</p>"},{"location":"graph-operations/how-to/clean-graph/#selective-steps","title":"Selective Steps","text":"<p>You can skip steps you do not need:</p> <pre><code># Skip normalization (no SSSOM mappings needed)\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output clean_graph.duckdb \\\n  --skip-normalize\n\n# Skip deduplication\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output clean_graph.duckdb \\\n  --skip-deduplicate\n\n# Skip pruning\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output clean_graph.duckdb \\\n  --skip-prune\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#example-merge-output","title":"Example Merge Output","text":"<pre><code>Starting merge pipeline...\nPipeline: join -&gt; deduplicate -&gt; normalize -&gt; prune\nOutput database: clean_graph.duckdb\nStep 1: Join - Loading input files...\nJoin completed: 6 files | 125,340 nodes | 298,567 edges\nStep 2: Deduplicate - Removing duplicate nodes/edges...\nDeduplicate completed: 45 duplicate nodes, 123 duplicate edges removed\nStep 3: Normalize - Applying SSSOM mappings...\nNormalize completed: 3 mapping files | 15,234 edge references normalized\nStep 4: Prune - Cleaning graph structure...\nPrune completed: 156 dangling edges moved | 23 singleton nodes handled\nMerge pipeline completed successfully!\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#verification","title":"Verification","text":"<p>After cleanup, verify your graph integrity with reports.</p>"},{"location":"graph-operations/how-to/clean-graph/#generate-qc-report","title":"Generate QC Report","text":"<pre><code>koza report qc -d graph.duckdb -o qc_report.yaml\n</code></pre> <p>This will show:</p> <ul> <li>Total node and edge counts</li> <li>Breakdown by source/category/predicate</li> <li>Any remaining integrity issues</li> </ul>"},{"location":"graph-operations/how-to/clean-graph/#generate-graph-statistics","title":"Generate Graph Statistics","text":"<pre><code>koza report graph-stats -d graph.duckdb -o graph_stats.yaml\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#check-archive-tables","title":"Check Archive Tables","text":"<p>Verify archive tables exist and contain expected data:</p> <pre><code>-- Check what archive tables exist\nSELECT table_name\nFROM information_schema.tables\nWHERE table_name IN ('dangling_edges', 'duplicate_nodes', 'duplicate_edges', 'singleton_nodes');\n\n-- Get counts from each archive table\nSELECT 'dangling_edges' as table_name, COUNT(*) as count FROM dangling_edges\nUNION ALL\nSELECT 'duplicate_nodes', COUNT(*) FROM duplicate_nodes\nUNION ALL\nSELECT 'duplicate_edges', COUNT(*) FROM duplicate_edges;\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#verify-no-dangling-edges-remain","title":"Verify No Dangling Edges Remain","text":"<pre><code>-- This should return 0 rows after prune\nSELECT COUNT(*) as dangling_count\nFROM edges e\nLEFT JOIN nodes n1 ON e.subject = n1.id\nLEFT JOIN nodes n2 ON e.object = n2.id\nWHERE n1.id IS NULL OR n2.id IS NULL;\n</code></pre>"},{"location":"graph-operations/how-to/clean-graph/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Complete command documentation</li> <li>How to Join Files - Creating the initial database</li> <li>How to Normalize IDs - SSSOM-based identifier normalization</li> <li>How to Generate Reports - QC and statistics reporting</li> </ul>"},{"location":"graph-operations/how-to/export-formats/","title":"How to Export Formats","text":""},{"location":"graph-operations/how-to/export-formats/#goal","title":"Goal","text":"<p>Export graph data to TSV, JSONL, or Parquet format with optional archiving. This is useful for:</p> <ul> <li>Sharing graph data with collaborators or downstream tools</li> <li>Converting between formats for different analysis workflows</li> <li>Creating distributable archives for data releases</li> <li>Preparing data for analytics platforms that prefer Parquet</li> </ul>"},{"location":"graph-operations/how-to/export-formats/#prerequisites","title":"Prerequisites","text":"<ul> <li>A DuckDB database containing graph data (created via <code>koza join</code> or <code>koza merge</code>)</li> <li>Target directory for exported files</li> </ul>"},{"location":"graph-operations/how-to/export-formats/#export-to-tsv","title":"Export to TSV","text":"<p>TSV (Tab-Separated Values) is the standard KGX format, widely supported by knowledge graph tools.</p>"},{"location":"graph-operations/how-to/export-formats/#using-split-for-simple-export","title":"Using split for simple export","text":"<p>The <code>split</code> command can export an entire graph without splitting when you export all records:</p> <pre><code>koza split graph.duckdb id --output-dir ./export --format tsv\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#using-merge-with-export","title":"Using merge with export","text":"<p>The most common approach is to use <code>merge</code> with export options, which processes and exports in one step:</p> <pre><code>koza merge \\\n  --nodes *.nodes.tsv \\\n  --edges *.edges.tsv \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --format tsv\n</code></pre> <p>This creates:</p> <pre><code>output/\n  merged_graph_nodes.tsv\n  merged_graph_edges.tsv\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#export-to-jsonl","title":"Export to JSONL","text":"<p>JSON Lines format stores one JSON object per line, useful for streaming and JavaScript-based tools.</p> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --format jsonl\n</code></pre> <p>Output:</p> <pre><code>output/\n  merged_graph_nodes.jsonl\n  merged_graph_edges.jsonl\n</code></pre> <p>JSONL is suited for:</p> <ul> <li>Working with JavaScript or Node.js applications</li> <li>Streaming large files line by line</li> <li>Preserving complex nested structures</li> <li>Integration with document databases</li> </ul>"},{"location":"graph-operations/how-to/export-formats/#export-to-parquet","title":"Export to Parquet","text":"<p>Parquet is a columnar format optimized for analytical workloads and large datasets.</p> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --format parquet\n</code></pre> <p>Output:</p> <pre><code>output/\n  merged_graph_nodes.parquet\n  merged_graph_edges.parquet\n</code></pre> <p>Parquet supports:</p> <ul> <li>Large-scale analytics with tools like Spark, Pandas, or Polars</li> <li>Storage with automatic compression</li> <li>Column-based queries (selecting specific fields)</li> <li>Integration with data warehouses and cloud analytics platforms</li> </ul>"},{"location":"graph-operations/how-to/export-formats/#creating-archives","title":"Creating Archives","text":"<p>Use the <code>--archive</code> flag to bundle exported files into a tar archive:</p> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --archive\n</code></pre> <p>This creates a single archive file:</p> <pre><code>output/\n  merged_graph.tar\n</code></pre> <p>The archive contains the nodes and edges files in the specified format (TSV by default).</p> <p>Archives are useful for:</p> <ul> <li>Data releases and distribution</li> <li>Preserving file relationships</li> <li>Single-file management and transfer</li> <li>Versioned snapshots of graph data</li> </ul>"},{"location":"graph-operations/how-to/export-formats/#compressed-archives","title":"Compressed Archives","text":"<p>Add <code>--compress</code> to create a gzip-compressed tar archive:</p> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --archive \\\n  --compress\n</code></pre> <p>This creates:</p> <pre><code>output/\n  merged_graph.tar.gz\n</code></pre> <p>Compressed archives reduce file size, especially for TSV and JSONL formats. Parquet files are already compressed internally, so the size reduction is smaller.</p> <p>Compression requires archive</p> <p>The <code>--compress</code> flag requires <code>--archive</code> to be enabled. You cannot create a compressed archive without first enabling archiving.</p>"},{"location":"graph-operations/how-to/export-formats/#custom-graph-naming","title":"Custom Graph Naming","text":"<p>Use <code>--graph-name</code> to specify custom names for exported files:</p> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --graph-name monarch_kg_2024_01\n</code></pre> <p>Output with custom naming:</p> <pre><code>output/\n  monarch_kg_2024_01_nodes.tsv\n  monarch_kg_2024_01_edges.tsv\n</code></pre> <p>Or with archiving:</p> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output \\\n  --graph-name monarch_kg_2024_01 \\\n  --archive \\\n  --compress\n</code></pre> <p>Creates: <code>output/monarch_kg_2024_01.tar.gz</code></p> <p>Custom naming is useful for:</p> <ul> <li>Version-specific releases (<code>monarch_kg_v2024_01</code>)</li> <li>Environment identification (<code>kg_production</code>, <code>kg_staging</code>)</li> <li>Source attribution (<code>hgnc_gene_graph</code>)</li> <li>Date-stamped snapshots</li> </ul>"},{"location":"graph-operations/how-to/export-formats/#loose-files-vs-archives","title":"Loose Files vs Archives","text":"<p>Choose the appropriate export strategy based on your use case.</p>"},{"location":"graph-operations/how-to/export-formats/#use-loose-files-when","title":"Use Loose Files When","text":"<ul> <li>Directly loading into analysis tools (Pandas, R, DuckDB)</li> <li>Incremental updates to individual files</li> <li>Quick inspection and validation</li> <li>Development and testing workflows</li> </ul> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./output\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#use-archives-when","title":"Use Archives When","text":"<ul> <li>Creating official data releases</li> <li>Distributing to external users</li> <li>Long-term storage and backup</li> <li>Ensuring file integrity during transfer</li> </ul> <pre><code>koza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --export \\\n  --export-dir ./releases \\\n  --graph-name monarch_kg_v2024_01 \\\n  --archive \\\n  --compress\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#use-compressed-archives-when","title":"Use Compressed Archives When","text":"<ul> <li>Minimizing storage costs</li> <li>Transferring over networks</li> <li>Publishing to data repositories</li> <li>Creating distributable packages</li> </ul>"},{"location":"graph-operations/how-to/export-formats/#using-merge-for-export","title":"Using merge for Export","text":"<p>The <code>merge</code> command combines data processing with export in a single pipeline.</p>"},{"location":"graph-operations/how-to/export-formats/#complete-pipeline-with-export","title":"Complete Pipeline with Export","text":"<pre><code>koza merge \\\n  --nodes data/*.nodes.tsv \\\n  --edges data/*.edges.tsv \\\n  --mappings sssom/*.sssom.tsv \\\n  --output processed_graph.duckdb \\\n  --export \\\n  --export-dir ./release \\\n  --graph-name my_knowledge_graph \\\n  --format parquet \\\n  --archive \\\n  --compress\n</code></pre> <p>This runs the complete pipeline (join, deduplicate, normalize, prune) and exports the final clean data.</p>"},{"location":"graph-operations/how-to/export-formats/#export-only-merge","title":"Export-Only Merge","text":"<p>If you only want to export from an existing database, you can skip all processing steps:</p> <pre><code>koza merge \\\n  --nodes empty.tsv \\\n  --edges empty.tsv \\\n  --output existing_graph.duckdb \\\n  --skip-normalize \\\n  --skip-prune \\\n  --skip-deduplicate \\\n  --export \\\n  --export-dir ./output\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#selective-pipeline-with-export","title":"Selective Pipeline with Export","text":"<p>Run specific pipeline steps and export:</p> <pre><code># Join and deduplicate only, then export\nkoza merge \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb \\\n  --skip-normalize \\\n  --skip-prune \\\n  --export \\\n  --export-dir ./output \\\n  --format tsv\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#cli-options-reference","title":"CLI Options Reference","text":"Option Description <code>--export</code> Enable export of final data to files <code>--export-dir</code> Directory for exported files (required with <code>--export</code>) <code>--format</code>, <code>-f</code> Output format: <code>tsv</code>, <code>jsonl</code>, or <code>parquet</code> (default: <code>tsv</code>) <code>--archive</code> Export as tar archive instead of loose files <code>--compress</code> Compress archive as tar.gz (requires <code>--archive</code>) <code>--graph-name</code> Custom name for graph files (default: <code>merged_graph</code>)"},{"location":"graph-operations/how-to/export-formats/#verification","title":"Verification","text":"<p>After exporting, verify the output files.</p>"},{"location":"graph-operations/how-to/export-formats/#check-exported-files","title":"Check exported files","text":"<pre><code>ls -la ./output/\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#verify-record-counts","title":"Verify record counts","text":"<p>For TSV:</p> <pre><code>wc -l ./output/*_nodes.tsv ./output/*_edges.tsv\n</code></pre> <p>For Parquet (using DuckDB):</p> <pre><code>duckdb -c \"SELECT COUNT(*) FROM read_parquet('./output/*_nodes.parquet')\"\nduckdb -c \"SELECT COUNT(*) FROM read_parquet('./output/*_edges.parquet')\"\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#inspect-archive-contents","title":"Inspect archive contents","text":"<pre><code># List tar contents\ntar -tvf ./output/my_graph.tar\n\n# List compressed tar contents\ntar -tzvf ./output/my_graph.tar.gz\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#validate-exported-data","title":"Validate exported data","text":"<pre><code># Check first few records\nhead -5 ./output/my_graph_nodes.tsv\n\n# For JSONL, validate JSON structure\nhead -1 ./output/my_graph_nodes.jsonl | jq .\n</code></pre>"},{"location":"graph-operations/how-to/export-formats/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Complete CLI documentation</li> <li>How to Join Files - Creating DuckDB databases to export from</li> <li>How to Split Graphs - Alternative export with field-based splitting</li> <li>Schema Handling - Format details and type inference</li> </ul>"},{"location":"graph-operations/how-to/generate-reports/","title":"How to Generate Reports","text":""},{"location":"graph-operations/how-to/generate-reports/#goal","title":"Goal","text":"<p>Generate quality control reports, statistics, and data summaries from your knowledge graph databases. Koza provides multiple reporting commands for different analysis needs: QC reports for data quality assessment, graph statistics for structural analysis, schema compliance for biolink validation, and tabular reports for detailed breakdowns.</p>"},{"location":"graph-operations/how-to/generate-reports/#prerequisites","title":"Prerequisites","text":"<ul> <li>A DuckDB database created by <code>koza join</code>, <code>koza merge</code>, or <code>koza append</code></li> <li>Alternatively, KGX files (TSV, JSONL, or Parquet) for file-based reports</li> </ul>"},{"location":"graph-operations/how-to/generate-reports/#qc-report","title":"QC Report","text":"<p>The <code>koza report qc</code> command generates a quality control report with node/edge statistics grouped by data source.</p>"},{"location":"graph-operations/how-to/generate-reports/#basic-usage","title":"Basic Usage","text":"<pre><code>koza report qc -d graph.duckdb -o qc_report.yaml\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#console-only-analysis","title":"Console-Only Analysis","text":"<p>For quick QC analysis without saving to a file:</p> <pre><code>koza report qc -d graph.duckdb\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#output-format","title":"Output Format","text":"<p>The QC report is saved as YAML with the following structure:</p> <pre><code>summary:\n  total_nodes: 125340\n  total_edges: 298567\n  dangling_edges: 156\n  duplicate_nodes: 0\n  singleton_nodes: 23\nnodes:\n  - provided_by: gene_source\n    category: biolink:Gene\n    count: 50000\n  - provided_by: disease_source\n    category: biolink:Disease\n    count: 25340\nedges:\n  - provided_by: interaction_source\n    predicate: biolink:interacts_with\n    count: 150000\n  - provided_by: association_source\n    predicate: biolink:associated_with\n    count: 148567\n</code></pre> <p>The report includes:</p> <ul> <li>Total entity counts and data distribution</li> <li>Potential integrity issues (dangling edges, duplicates)</li> <li>Breakdown by source for multi-source graphs</li> </ul>"},{"location":"graph-operations/how-to/generate-reports/#graph-statistics","title":"Graph Statistics","text":"<p>The <code>koza report graph-stats</code> command generates graph statistics similar to the <code>merged_graph_stats.yaml</code> output from cat-merge.</p>"},{"location":"graph-operations/how-to/generate-reports/#basic-usage_1","title":"Basic Usage","text":"<pre><code>koza report graph-stats -d graph.duckdb -o graph_stats.yaml\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#output-format_1","title":"Output Format","text":"<pre><code>graph_name: Graph Statistics\nnode_stats:\n  total_nodes: 125340\n  count_by_category:\n    biolink:Gene:\n      count: 50000\n      provided_by:\n        gene_source: 50000\n    biolink:Disease:\n      count: 25340\n      provided_by:\n        disease_source: 25340\n  count_by_id_prefixes:\n    HGNC: 45000\n    NCBIGene: 5000\n    MONDO: 15340\n    OMIM: 10000\n  node_categories:\n    - biolink:Gene\n    - biolink:Disease\n    - biolink:Phenotype\n  node_id_prefixes:\n    - HGNC\n    - NCBIGene\n    - MONDO\n    - OMIM\n  provided_by:\n    - gene_source\n    - disease_source\nedge_stats:\n  total_edges: 298567\n  count_by_predicates:\n    biolink:interacts_with:\n      count: 150000\n      provided_by:\n        interaction_source: 150000\n    biolink:associated_with:\n      count: 148567\n      provided_by:\n        association_source: 148567\n  predicates:\n    - biolink:interacts_with\n    - biolink:associated_with\n  provided_by:\n    - interaction_source\n    - association_source\n</code></pre> <p>Statistics generated include:</p> <ul> <li>Node counts by category and ID prefix</li> <li>Edge counts by predicate</li> <li>Attribution breakdown by <code>provided_by</code> source</li> <li>Complete enumeration of categories, predicates, and prefixes</li> </ul>"},{"location":"graph-operations/how-to/generate-reports/#schema-compliance","title":"Schema Compliance","text":"<p>The <code>koza report schema</code> command analyzes database schema and checks biolink model compliance.</p>"},{"location":"graph-operations/how-to/generate-reports/#basic-usage_2","title":"Basic Usage","text":"<pre><code>koza report schema -d graph.duckdb -o schema_report.yaml\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#output-format_2","title":"Output Format","text":"<pre><code>metadata:\n  operation: schema_analysis\n  generated_at: '2024-01-15 10:30:45'\n  report_version: '1.0'\nschema_analysis:\n  summary:\n    nodes:\n      file_count: 4\n      unique_columns: 23\n      all_columns:\n        - id\n        - category\n        - name\n        - description\n        - xref\n        - provided_by\n    edges:\n      file_count: 2\n      unique_columns: 18\n      all_columns:\n        - id\n        - subject\n        - predicate\n        - object\n        - category\n        - primary_knowledge_source\n  files:\n    - filename: genes_nodes.tsv\n      table_type: nodes\n      column_count: 12\n      columns:\n        - id\n        - category\n        - name\n        - symbol\n</code></pre> <p>The schema report includes:</p> <ul> <li>Column coverage across source files</li> <li>Schema harmonization applied during join</li> <li>Biolink-compliant vs. extension columns</li> <li>Data type consistency</li> </ul>"},{"location":"graph-operations/how-to/generate-reports/#node-reports","title":"Node Reports","text":"<p>The <code>koza node-report</code> command generates tabular reports with node counts grouped by categorical columns.</p>"},{"location":"graph-operations/how-to/generate-reports/#from-database","title":"From Database","text":"<pre><code>koza node-report -d graph.duckdb -o node_report.tsv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#from-file","title":"From File","text":"<pre><code>koza node-report -f nodes.tsv -o node_report.tsv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#custom-columns","title":"Custom Columns","text":"<p>Specify which categorical columns to group by:</p> <pre><code>koza node-report -d graph.duckdb -o report.tsv \\\n    -c namespace -c category -c provided_by\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#output-example","title":"Output Example","text":"<pre><code>namespace   category    provided_by count\nHGNC    biolink:Gene    gene_source 45000\nNCBIGene    biolink:Gene    gene_source 5000\nMONDO   biolink:Disease disease_source  15340\nOMIM    biolink:Disease disease_source  10000\n</code></pre> <p>The default grouping columns include <code>namespace</code>, <code>category</code>, and <code>provided_by</code> when present in the data.</p>"},{"location":"graph-operations/how-to/generate-reports/#edge-reports","title":"Edge Reports","text":"<p>The <code>koza edge-report</code> command generates tabular reports with edge counts, including denormalized node information.</p>"},{"location":"graph-operations/how-to/generate-reports/#from-database_1","title":"From Database","text":"<pre><code>koza edge-report -d graph.duckdb -o edge_report.tsv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#from-files","title":"From Files","text":"<p>When working with separate node and edge files, provide both for denormalization:</p> <pre><code>koza edge-report -n nodes.tsv -e edges.tsv -o edge_report.tsv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#custom-columns_1","title":"Custom Columns","text":"<pre><code>koza edge-report -d graph.duckdb -o report.tsv \\\n    -c subject_category -c predicate -c object_category -c primary_knowledge_source\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#output-example_1","title":"Output Example","text":"<pre><code>subject_category    predicate   object_category primary_knowledge_source    count\nbiolink:Gene    biolink:interacts_with  biolink:Gene    string_db   85000\nbiolink:Gene    biolink:interacts_with  biolink:Protein biogrid 65000\nbiolink:Disease biolink:associated_with biolink:Phenotype   hpo 98567\nbiolink:Gene    biolink:associated_with biolink:Disease disgenet    50000\n</code></pre> <p>The edge report automatically joins edges to nodes to derive <code>subject_category</code> and <code>object_category</code> from the referenced nodes.</p>"},{"location":"graph-operations/how-to/generate-reports/#node-examples","title":"Node Examples","text":"<p>The <code>koza node-examples</code> command extracts sample nodes for each category or other grouping column.</p>"},{"location":"graph-operations/how-to/generate-reports/#basic-usage_3","title":"Basic Usage","text":"<pre><code>koza node-examples -d graph.duckdb -o node_examples.tsv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#custom-sample-size","title":"Custom Sample Size","text":"<pre><code>koza node-examples -d graph.duckdb -o examples.tsv -n 10\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#group-by-different-column","title":"Group by Different Column","text":"<pre><code>koza node-examples -d graph.duckdb -o examples.tsv -t provided_by\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#from-file_1","title":"From File","text":"<pre><code>koza node-examples -f nodes.tsv -o examples.tsv -n 5\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#output-example_2","title":"Output Example","text":"<p>The output contains N sample rows for each distinct value in the type column:</p> <pre><code>id  name    category    provided_by ...\nHGNC:1234   BRCA1   biolink:Gene    gene_source ...\nHGNC:5678   TP53    biolink:Gene    gene_source ...\nMONDO:0005148   diabetes mellitus   biolink:Disease disease_source  ...\nMONDO:0004975   Alzheimer disease   biolink:Disease disease_source  ...\n</code></pre> <p>The output can be used for:</p> <ul> <li>Data validation and spot-checking</li> <li>Documentation and examples</li> <li>Investigating data quality issues</li> </ul>"},{"location":"graph-operations/how-to/generate-reports/#edge-examples","title":"Edge Examples","text":"<p>The <code>koza edge-examples</code> command extracts sample edges for each predicate pattern or custom grouping.</p>"},{"location":"graph-operations/how-to/generate-reports/#basic-usage_4","title":"Basic Usage","text":"<pre><code>koza edge-examples -d graph.duckdb -o edge_examples.tsv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#custom-sample-size_1","title":"Custom Sample Size","text":"<pre><code>koza edge-examples -d graph.duckdb -o examples.tsv -s 10\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#custom-type-columns","title":"Custom Type Columns","text":"<p>By default, edges are grouped by <code>subject_category</code>, <code>predicate</code>, and <code>object_category</code>. Customize this:</p> <pre><code>koza edge-examples -d graph.duckdb -o examples.tsv \\\n    -t predicate -t primary_knowledge_source\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#from-files_1","title":"From Files","text":"<pre><code>koza edge-examples -n nodes.tsv -e edges.tsv -o examples.tsv -s 5\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#output-example_3","title":"Output Example","text":"<pre><code>subject predicate   object  subject_name    object_name primary_knowledge_source    ...\nHGNC:1234   biolink:interacts_with  HGNC:5678   BRCA1   TP53    string_db   ...\nHGNC:9999   biolink:interacts_with  HGNC:8888   MYC MAX biogrid ...\nMONDO:0005148   biolink:associated_with HP:0001943  diabetes    Hypoglycemia    hpo ...\n</code></pre> <p>Edge examples show:</p> <ul> <li>Subject/object relationships</li> <li>Predicate usage patterns</li> <li>Knowledge source attribution</li> </ul>"},{"location":"graph-operations/how-to/generate-reports/#output-formats","title":"Output Formats","text":"<p>All tabular reports (<code>node-report</code>, <code>edge-report</code>, <code>node-examples</code>, <code>edge-examples</code>) support multiple output formats.</p>"},{"location":"graph-operations/how-to/generate-reports/#tsv-default","title":"TSV (Default)","text":"<pre><code>koza node-report -d graph.duckdb -o report.tsv --format tsv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#parquet","title":"Parquet","text":"<p>For large reports or downstream analytics:</p> <pre><code>koza node-report -d graph.duckdb -o report.parquet --format parquet\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#csv","title":"CSV","text":"<pre><code>koza node-report -d graph.duckdb -o report.csv --format csv\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#json","title":"JSON","text":"<pre><code>koza node-report -d graph.duckdb -o report.json --format json\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#common-options","title":"Common Options","text":"<p>All report commands support these options:</p> Option Description <code>-d, --database</code> Path to DuckDB database file <code>-o, --output</code> Path to output file <code>-q, --quiet</code> Suppress progress output"},{"location":"graph-operations/how-to/generate-reports/#workflow-examples","title":"Workflow Examples","text":""},{"location":"graph-operations/how-to/generate-reports/#complete-qc-pipeline","title":"Complete QC Pipeline","text":"<p>Generate all reports for a merged graph:</p> <pre><code># Generate all report types\nkoza report qc -d merged.duckdb -o qc_report.yaml\nkoza report graph-stats -d merged.duckdb -o graph_stats.yaml\nkoza report schema -d merged.duckdb -o schema_report.yaml\n\n# Generate tabular breakdowns\nkoza node-report -d merged.duckdb -o node_report.tsv\nkoza edge-report -d merged.duckdb -o edge_report.tsv\n\n# Extract examples for documentation\nkoza node-examples -d merged.duckdb -o node_examples.tsv -n 3\nkoza edge-examples -d merged.duckdb -o edge_examples.tsv -s 3\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#post-merge-validation","title":"Post-Merge Validation","text":"<p>After running <code>koza merge</code>, validate the results:</p> <pre><code># Check for data quality issues\nkoza report qc -d merged.duckdb -o post_merge_qc.yaml\n\n# Verify expected categories and predicates\nkoza report graph-stats -d merged.duckdb -o post_merge_stats.yaml\n\n# Spot-check examples\nkoza edge-examples -d merged.duckdb -o edge_samples.tsv -s 5\n</code></pre>"},{"location":"graph-operations/how-to/generate-reports/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Complete command documentation</li> <li>Configuration Reference - Report configuration options</li> <li>Data Integrity - Understanding archive tables in reports</li> </ul>"},{"location":"graph-operations/how-to/incremental-updates/","title":"How to Do Incremental Updates","text":""},{"location":"graph-operations/how-to/incremental-updates/#goal","title":"Goal","text":"<p>Add new data to an existing DuckDB database with automatic schema evolution. The <code>append</code> operation lets you incrementally grow your knowledge graph without rebuilding from scratch, automatically handling schema differences between old and new data.</p>"},{"location":"graph-operations/how-to/incremental-updates/#prerequisites","title":"Prerequisites","text":"<ul> <li>An existing DuckDB database (created via <code>koza join</code>, <code>koza merge</code>, or a previous <code>koza append</code>)</li> <li>New KGX files to append (nodes and/or edges in TSV, JSONL, or Parquet format)</li> <li>Koza installed and available in your PATH</li> </ul>"},{"location":"graph-operations/how-to/incremental-updates/#basic-append","title":"Basic Append","text":"<p>The simplest append adds new node and edge files to an existing database.</p> <pre><code>koza append \\\n  --database existing_graph.duckdb \\\n  --nodes new_genes.tsv \\\n  --edges new_interactions.tsv\n</code></pre> <p>You can append multiple files at once:</p> <pre><code>koza append \\\n  --database existing_graph.duckdb \\\n  --nodes new_genes.tsv updated_pathways.jsonl additional_proteins.parquet \\\n  --edges new_interactions.tsv new_associations.jsonl\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#mixed-format-support","title":"Mixed Format Support","text":"<p>Like other Koza operations, append handles mixed formats:</p> <pre><code>koza append \\\n  --database graph.duckdb \\\n  --nodes genes.tsv proteins.jsonl pathways.parquet \\\n  --edges interactions.tsv.gz associations.jsonl.bz2\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#schema-evolution","title":"Schema Evolution","text":"<p>When new files contain columns that do not exist in the database, append automatically adds these columns to the schema.</p> <pre><code>koza append \\\n  --database graph.duckdb \\\n  --nodes genes_with_new_fields.tsv \\\n  --edges interactions_with_confidence.tsv \\\n  --schema-report\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#how-schema-evolution-works","title":"How Schema Evolution Works","text":"<ol> <li>New columns detected: Append compares incoming file schemas with existing table schemas</li> <li>Columns added: New columns are added to the database tables using <code>ALTER TABLE</code></li> <li>Backward compatibility: Existing rows get NULL values for the new columns</li> <li>Type safety: DuckDB handles type inference and conversion automatically</li> </ol>"},{"location":"graph-operations/how-to/incremental-updates/#example-output","title":"Example Output","text":"<pre><code>Append completed successfully\n  Files processed: 2 (2 successful)\n  Records added: 15,234\n  Schema evolution: 2 new columns added\n    - Added 1 new column to nodes: custom_score\n    - Added 1 new column to edges: confidence_value\n  Database growth:\n    - Nodes: 125,340 -&gt; 138,574 (+13,234)\n    - Edges: 298,567 -&gt; 300,567 (+2,000)\n  Total time: 8.2s\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#deduplication-during-append","title":"Deduplication During Append","text":"<p>When appending data that may overlap with existing records, use the <code>--deduplicate</code> flag to remove duplicates:</p> <pre><code>koza append \\\n  --database graph.duckdb \\\n  --nodes updated_genes.tsv \\\n  --edges updated_interactions.tsv \\\n  --deduplicate\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#how-deduplication-works","title":"How Deduplication Works","text":"<ol> <li>Append first: New data is added to the tables</li> <li>Identify duplicates: Records with the same ID are identified</li> <li>Keep first occurrence: The first occurrence (by load order) is kept</li> <li>Archive duplicates: Duplicate records are moved to <code>duplicate_nodes</code> and <code>duplicate_edges</code> tables</li> </ol>"},{"location":"graph-operations/how-to/incremental-updates/#example-with-deduplication","title":"Example with Deduplication","text":"<pre><code>koza append \\\n  --database graph.duckdb \\\n  --nodes genes_v2.tsv \\\n  --deduplicate \\\n  --show-progress\n</code></pre> <p>Output:</p> <pre><code>Append completed successfully\n  Files processed: 1 (1 successful)\n  Records added: 5,234\n  Duplicates removed: 45\n  Database: graph.duckdb (4.8 MB)\n  Total time: 4.1s\n</code></pre> <p>Deduplication Preserves Data</p> <p>Duplicate records are moved to archive tables (<code>duplicate_nodes</code>, <code>duplicate_edges</code>), not deleted. You can inspect these tables to understand what was deduplicated.</p>"},{"location":"graph-operations/how-to/incremental-updates/#tracking-schema-changes","title":"Tracking Schema Changes","text":"<p>Use the <code>--schema-report</code> flag to generate a detailed report of schema changes:</p> <pre><code>koza append \\\n  --database graph.duckdb \\\n  --nodes new_data.tsv \\\n  --schema-report\n</code></pre> <p>This creates a YAML file (e.g., <code>graph_schema_report_append.yaml</code>) containing:</p> <ul> <li>File analysis: Format detection, column counts, record counts per file</li> <li>Schema changes: List of new columns added to each table</li> <li>Column details: Data types, null percentages, example values</li> </ul>"},{"location":"graph-operations/how-to/incremental-updates/#inspecting-schema-changes-via-sql","title":"Inspecting Schema Changes via SQL","text":"<p>After appending, you can also check the schema directly:</p> <pre><code># View current table schemas\nduckdb graph.duckdb \"DESCRIBE nodes\"\nduckdb graph.duckdb \"DESCRIBE edges\"\n\n# Check which columns have NULL values (potentially new columns)\nduckdb graph.duckdb \"SELECT COUNT(*) - COUNT(custom_score) as nulls FROM nodes\"\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#when-to-use-append-vs-join","title":"When to Use Append vs Join","text":"<p>Choose the right operation based on your use case:</p> Scenario Use Append Use Join Adding new data to existing database Yes No Preserving existing database state Yes No (overwrites) Incremental daily/weekly updates Yes No Full rebuild from all sources No Yes Starting fresh with new data No Yes Schema evolution needed Yes (automatic) Yes (automatic) Combining many files initially No Yes"},{"location":"graph-operations/how-to/incremental-updates/#append-use-cases","title":"Append Use Cases","text":"<ul> <li>Daily data ingestion: Add new records from a data pipeline each day</li> <li>Incremental updates: Add corrections or updates without full rebuild</li> <li>Data augmentation: Add new sources to an existing graph</li> <li>Schema extension: Add new properties to existing entities</li> </ul>"},{"location":"graph-operations/how-to/incremental-updates/#join-use-cases","title":"Join Use Cases","text":"<ul> <li>Initial graph creation: Combine multiple source files into a new database</li> <li>Full rebuild: Replace existing data with a fresh build from sources</li> <li>Format conversion: Load data into DuckDB for the first time</li> </ul>"},{"location":"graph-operations/how-to/incremental-updates/#complete-pipeline-comparison","title":"Complete Pipeline Comparison","text":"<p>Incremental approach (append): <pre><code># Initial build\nkoza join --nodes *.nodes.* --edges *.edges.* --output graph.duckdb\n\n# Later: add new data\nkoza append --database graph.duckdb --nodes new_data.tsv --deduplicate\nkoza append --database graph.duckdb --nodes more_data.tsv --deduplicate\n</code></pre></p> <p>Full rebuild approach (join): <pre><code># Rebuild everything each time\nkoza merge \\\n  --nodes *.nodes.* new_data.tsv more_data.tsv \\\n  --edges *.edges.* \\\n  --output graph.duckdb\n</code></pre></p>"},{"location":"graph-operations/how-to/incremental-updates/#verification","title":"Verification","text":"<p>After appending, verify the operation succeeded.</p>"},{"location":"graph-operations/how-to/incremental-updates/#check-record-counts","title":"Check Record Counts","text":"<pre><code># Compare before and after counts\nduckdb graph.duckdb \"SELECT COUNT(*) AS node_count FROM nodes\"\nduckdb graph.duckdb \"SELECT COUNT(*) AS edge_count FROM edges\"\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#verify-new-data-is-present","title":"Verify New Data Is Present","text":"<pre><code># Check for records from the new source\nduckdb graph.duckdb \"SELECT COUNT(*) FROM nodes WHERE file_source LIKE '%new_data%'\"\n\n# Or by provided_by if set\nduckdb graph.duckdb \"SELECT provided_by, COUNT(*) FROM nodes GROUP BY provided_by ORDER BY COUNT(*) DESC\"\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#check-schema-evolution","title":"Check Schema Evolution","text":"<pre><code># View the current schema\nduckdb graph.duckdb \"DESCRIBE nodes\"\n\n# Check for new columns with mostly NULL values (recently added)\nduckdb graph.duckdb \"\nSELECT\n    column_name,\n    COUNT(*) - COUNT(column_name) as null_count,\n    COUNT(*) as total_count\nFROM nodes\nUNPIVOT (value FOR column_name IN (*))\nGROUP BY column_name\nORDER BY null_count DESC\n\"\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#verify-deduplication-if-used","title":"Verify Deduplication (if used)","text":"<pre><code># Check duplicate archive tables\nduckdb graph.duckdb \"SELECT COUNT(*) as duplicate_nodes FROM duplicate_nodes\"\nduckdb graph.duckdb \"SELECT COUNT(*) as duplicate_edges FROM duplicate_edges\"\n\n# View sample duplicates\nduckdb graph.duckdb \"SELECT * FROM duplicate_nodes LIMIT 5\"\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#generate-qc-report","title":"Generate QC Report","text":"<p>For comprehensive verification:</p> <pre><code>koza report qc --database graph.duckdb --output qc_report.yaml\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#complete-example","title":"Complete Example","text":"<p>A typical incremental update workflow:</p> <pre><code># Step 1: Initial graph build\nkoza merge \\\n  --nodes source_a.nodes.tsv source_b.nodes.tsv \\\n  --edges source_a.edges.tsv source_b.edges.tsv \\\n  --output knowledge_graph.duckdb\n\n# Step 2: Check initial counts\nduckdb knowledge_graph.duckdb \"SELECT COUNT(*) FROM nodes\"\n# Returns: 125340\n\n# Step 3: Append new data with deduplication and schema tracking\nkoza append \\\n  --database knowledge_graph.duckdb \\\n  --nodes new_source_c.nodes.tsv \\\n  --edges new_source_c.edges.tsv \\\n  --deduplicate \\\n  --schema-report \\\n  --show-progress\n\n# Step 4: Verify the update\nduckdb knowledge_graph.duckdb \"SELECT COUNT(*) FROM nodes\"\n# Returns: 138574\n\n# Step 5: Generate updated QC report\nkoza report qc --database knowledge_graph.duckdb --output qc_report.yaml\n</code></pre>"},{"location":"graph-operations/how-to/incremental-updates/#see-also","title":"See Also","text":"<ul> <li>CLI Reference: koza append</li> <li>How to Join Files - Creating the initial database</li> <li>How to Clean Graphs - Deduplication and pruning operations</li> <li>Schema Handling - Schema evolution and harmonization</li> </ul>"},{"location":"graph-operations/how-to/join-files/","title":"How to Join Files","text":""},{"location":"graph-operations/how-to/join-files/#goal","title":"Goal","text":"<p>Combine multiple KGX files into a unified DuckDB database. The join operation automatically handles schema harmonization across files with different column structures and supports multiple input formats.</p>"},{"location":"graph-operations/how-to/join-files/#prerequisites","title":"Prerequisites","text":"<ul> <li>Koza installed (<code>uvx koza</code>, <code>uv add koza</code>, <code>poetry add koza</code>, or <code>pip install koza</code>)</li> <li>KGX files (nodes and/or edges in TSV, JSONL, or Parquet format)</li> </ul>"},{"location":"graph-operations/how-to/join-files/#basic-join","title":"Basic Join","text":"<p>The simplest case combines a node file and an edge file into a single database.</p> <pre><code>koza join \\\n  --nodes genes.tsv \\\n  --edges gene_interactions.tsv \\\n  --output gene_graph.duckdb\n</code></pre> <p>This creates <code>gene_graph.duckdb</code> containing two tables: <code>nodes</code> and <code>edges</code>.</p>"},{"location":"graph-operations/how-to/join-files/#joining-multiple-files","title":"Joining Multiple Files","text":"<p>You can specify multiple files for each table type by repeating the options:</p> <pre><code>koza join \\\n  -n genes.tsv -n proteins.tsv -n pathways.tsv \\\n  -e gene_protein.tsv -e protein_pathway.tsv \\\n  --output combined_graph.duckdb\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#mixed-formats","title":"Mixed Formats","text":"<p>The join operation handles files in different formats. DuckDB automatically detects formats based on file extensions.</p> <pre><code>koza join \\\n  -n genes.tsv -n proteins.jsonl -n pathways.parquet \\\n  -e interactions.tsv -e associations.jsonl.gz \\\n  --output mixed_format_graph.duckdb\n</code></pre> <p>Supported formats:</p> <ul> <li>TSV: Tab-separated values (<code>.tsv</code>, <code>.tsv.gz</code>)</li> <li>JSONL: JSON Lines (<code>.jsonl</code>, <code>.jsonl.gz</code>, <code>.jsonl.bz2</code>)</li> <li>Parquet: Apache Parquet (<code>.parquet</code>)</li> </ul> <p>Compressed files (<code>.gz</code>, <code>.bz2</code>) are automatically decompressed during processing.</p>"},{"location":"graph-operations/how-to/join-files/#using-glob-patterns","title":"Using Glob Patterns","text":"<p>For directories with many files following naming conventions, use wildcard patterns:</p> <pre><code># Join all node and edge files\nkoza join \\\n  --nodes *.nodes.tsv \\\n  --edges *.edges.tsv \\\n  --output graph.duckdb\n</code></pre> <p>More flexible patterns:</p> <pre><code># Match multiple extensions\nkoza join \\\n  --nodes *.nodes.* \\\n  --edges *.edges.* \\\n  --output graph.duckdb\n\n# Match files in subdirectories\nkoza join \\\n  --nodes data/**/nodes.tsv \\\n  --edges data/**/edges.tsv \\\n  --output graph.duckdb\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#schema-reporting","title":"Schema Reporting","text":"<p>Use the <code>--schema-report</code> flag to generate a detailed YAML report analyzing the schema across all input files:</p> <pre><code>koza join \\\n  -n genes.tsv -n proteins.jsonl \\\n  -e interactions.tsv \\\n  --output graph.duckdb \\\n  --schema-report\n</code></pre> <p>This creates <code>graph_schema_report.yaml</code> containing:</p> <ul> <li>File analysis: Format detection, column counts, record counts per file</li> <li>Schema summary: Unique columns across all files</li> <li>Column details: Data types, null percentages, example values</li> <li>Schema harmonization: How differences between files were resolved</li> </ul> <p>Schema harmonization handles:</p> <ul> <li>Missing columns: Filled with NULL values</li> <li>Extra columns: Preserved in the final schema</li> <li>Type conflicts: Resolved using DuckDB's type inference</li> <li>Multi-valued fields: Detected and converted to arrays where appropriate</li> </ul>"},{"location":"graph-operations/how-to/join-files/#source-attribution","title":"Source Attribution","text":"<p>The join operation automatically tracks which file each record came from using the <code>file_source</code> column. This enables provenance tracking and later splitting by source. No additional flags are needed - source attribution happens automatically during the join operation.</p>"},{"location":"graph-operations/how-to/join-files/#persistent-vs-in-memory","title":"Persistent vs In-Memory","text":""},{"location":"graph-operations/how-to/join-files/#persistent-database-recommended","title":"Persistent Database (Recommended)","text":"<p>Use <code>--output</code> to create a persistent DuckDB file:</p> <pre><code>koza join \\\n  --nodes genes.tsv \\\n  --edges interactions.tsv \\\n  --output graph.duckdb\n</code></pre> <p>Use persistent databases when:</p> <ul> <li>Processing large datasets</li> <li>Running multiple subsequent operations (prune, split, normalize)</li> <li>Sharing results with others</li> <li>Preserving work for later analysis</li> </ul>"},{"location":"graph-operations/how-to/join-files/#in-memory-database","title":"In-Memory Database","text":"<p>Omit <code>--output</code> for an in-memory database (useful for quick analysis or piping to other operations):</p> <pre><code>koza join \\\n  --nodes genes.tsv \\\n  --edges interactions.tsv\n</code></pre> <p>In-Memory Limitations</p> <p>In-memory databases are lost when the process exits. Always use <code>--output</code> for data you want to keep.</p>"},{"location":"graph-operations/how-to/join-files/#progress-tracking","title":"Progress Tracking","text":"<p>Progress tracking is enabled by default. Use <code>--quiet</code> to suppress all output:</p> <pre><code>koza join \\\n  --nodes \"*.nodes.*\" \\\n  --edges \"*.edges.*\" \\\n  --output graph.duckdb \\\n  --quiet\n</code></pre> <p>Use <code>-p</code> or <code>--progress</code> to explicitly control progress bars (enabled by default).</p>"},{"location":"graph-operations/how-to/join-files/#verification","title":"Verification","text":"<p>After joining, verify the operation succeeded using SQL queries or the report commands.</p>"},{"location":"graph-operations/how-to/join-files/#quick-verification-with-duckdb-cli","title":"Quick Verification with DuckDB CLI","text":"<pre><code>duckdb graph.duckdb \"SELECT COUNT(*) AS node_count FROM nodes\"\nduckdb graph.duckdb \"SELECT COUNT(*) AS edge_count FROM edges\"\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#check-schema","title":"Check Schema","text":"<pre><code>duckdb graph.duckdb \"DESCRIBE nodes\"\nduckdb graph.duckdb \"DESCRIBE edges\"\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#generate-qc-report","title":"Generate QC Report","text":"<pre><code>koza report qc \\\n  -d graph.duckdb \\\n  -o qc_report.yaml\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#view-sample-records","title":"View Sample Records","text":"<pre><code>duckdb graph.duckdb \"SELECT * FROM nodes LIMIT 5\"\nduckdb graph.duckdb \"SELECT * FROM edges LIMIT 5\"\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#verify-source-attribution","title":"Verify Source Attribution","text":"<p>Check file source tracking:</p> <pre><code>duckdb graph.duckdb \"SELECT file_source, COUNT(*) FROM nodes GROUP BY file_source\"\nduckdb graph.duckdb \"SELECT file_source, COUNT(*) FROM edges GROUP BY file_source\"\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#complete-example","title":"Complete Example","text":"<p>A typical workflow combining multiple options:</p> <pre><code>koza join \\\n  -n data/hgnc_genes.tsv -n data/uniprot_proteins.jsonl -n data/reactome_pathways.parquet \\\n  -e data/gene_protein_interactions.tsv -e data/protein_pathway_associations.jsonl \\\n  --output knowledge_graph.duckdb \\\n  --schema-report\n</code></pre> <p>Expected output:</p> <pre><code>Join completed successfully\n  Files processed: 5 (5 successful)\n  Records loaded: 125,340 nodes, 298,567 edges\n  Schema harmonization: 3 missing columns filled, 2 extra preserved\n  Database created: knowledge_graph.duckdb (4.2 MB)\n  Total time: 12.4s\n</code></pre>"},{"location":"graph-operations/how-to/join-files/#next-steps","title":"Next Steps","text":"<p>After joining files, you might want to:</p> <ul> <li>Clean the graph to remove dangling edges</li> <li>Normalize identifiers using SSSOM mappings</li> <li>Split the graph by source or category</li> <li>Generate reports for quality control</li> </ul>"},{"location":"graph-operations/how-to/join-files/#see-also","title":"See Also","text":"<ul> <li>CLI Reference: koza join</li> <li>Schema Handling - Format detection and schema harmonization</li> <li>Generate Reports - Schema analysis and QC reporting</li> </ul>"},{"location":"graph-operations/how-to/normalize-ids/","title":"How to Normalize IDs","text":""},{"location":"graph-operations/how-to/normalize-ids/#goal","title":"Goal","text":"<p>Apply SSSOM mappings to harmonize identifiers in edge references. This allows you to consolidate equivalent identifiers from different sources (e.g., mapping OMIM disease IDs to MONDO) so that edges reference consistent node identifiers.</p>"},{"location":"graph-operations/how-to/normalize-ids/#prerequisites","title":"Prerequisites","text":"<ul> <li>A DuckDB database with edges (created via <code>koza join</code>, <code>koza merge</code>, or <code>koza append</code>)</li> <li>One or more SSSOM mapping files that define the identifier mappings you want to apply</li> </ul>"},{"location":"graph-operations/how-to/normalize-ids/#understanding-sssom","title":"Understanding SSSOM","text":"<p>SSSOM (Simple Standard for Sharing Ontological Mappings) is a standard format for representing mappings between ontology terms and other identifiers.</p>"},{"location":"graph-operations/how-to/normalize-ids/#sssom-file-format","title":"SSSOM File Format","text":"<p>An SSSOM TSV file typically has an optional YAML header (lines starting with <code>#</code>) followed by tab-separated data:</p> <pre><code>#curie_map:\n#  MONDO: http://purl.obolibrary.org/obo/MONDO_\n#  OMIM: https://omim.org/entry/\n#  HP: http://purl.obolibrary.org/obo/HP_\n#mapping_set_id: https://example.org/mappings/mondo-omim\nsubject_id  predicate_id    object_id   mapping_justification\nMONDO:0005148   skos:exactMatch OMIM:222100 semapv:ManualMappingCuration\nMONDO:0007455   skos:exactMatch OMIM:114500 semapv:ManualMappingCuration\nMONDO:0008199   skos:exactMatch OMIM:176000 semapv:ManualMappingCuration\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#how-koza-uses-sssom-for-normalization","title":"How Koza Uses SSSOM for Normalization","text":"<p>Koza uses a simplifying assumption when applying SSSOM mappings: it maps FROM the <code>object_id</code> TO the <code>subject_id</code>. This is a practical simplification for identifier normalization and does not reflect the full semantic meaning of SSSOM mapping files (where subject/object semantics depend on the predicate).</p> <p>The relevant columns for normalization are:</p> <ul> <li>subject_id: The target identifier (what Koza normalizes TO)</li> <li>object_id: The source identifier (what Koza normalizes FROM)</li> <li>predicate_id: The mapping relationship (e.g., <code>skos:exactMatch</code>) - used for filtering but not for determining direction</li> <li>mapping_justification: How the mapping was created (optional but recommended)</li> </ul> <p>Important: Normalization changes edge references (the <code>subject</code> and <code>object</code> columns in the edges table), not the node IDs themselves. If an edge references an identifier that appears in the SSSOM <code>object_id</code> column, that reference is updated to the corresponding <code>subject_id</code>.</p>"},{"location":"graph-operations/how-to/normalize-ids/#basic-normalization","title":"Basic Normalization","text":"<p>Apply a single SSSOM file to normalize identifiers in your database.</p>"},{"location":"graph-operations/how-to/normalize-ids/#step-1-verify-your-database","title":"Step 1: Verify Your Database","text":"<p>First, check the current state of your edges:</p> <pre><code># Count edges and check identifier patterns\nduckdb graph.duckdb -c \"\n  SELECT COUNT(*) as edge_count FROM edges;\n\"\n\n# See sample identifiers\nduckdb graph.duckdb -c \"\n  SELECT DISTINCT subject FROM edges LIMIT 10;\n\"\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#step-2-apply-the-mapping","title":"Step 2: Apply the Mapping","text":"<pre><code>koza normalize graph.duckdb \\\n  --mappings mondo-omim.sssom.tsv\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#example-output","title":"Example Output","text":"<pre><code>Loading SSSOM mappings...\n  Loaded 45,678 unique mappings from 1 file(s)\nNormalizing edge references...\n  Normalized 12,345 edge subject/object references\nNormalization completed successfully\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#multiple-mapping-files","title":"Multiple Mapping Files","text":"<p>When you have mappings from multiple sources, you can apply them all at once.</p>"},{"location":"graph-operations/how-to/normalize-ids/#using-multiple-files","title":"Using Multiple Files","text":"<pre><code>koza normalize graph.duckdb \\\n  -m mondo-omim.sssom.tsv \\\n  -m mondo-orphanet.sssom.tsv \\\n  -m hp-mp.sssom.tsv\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#using-a-mappings-directory","title":"Using a Mappings Directory","text":"<p>If all your SSSOM files are in one directory:</p> <pre><code>koza normalize graph.duckdb \\\n  --mappings-dir ./mappings/\n</code></pre> <p>This loads all <code>.sssom.tsv</code> files from the specified directory.</p>"},{"location":"graph-operations/how-to/normalize-ids/#order-of-application","title":"Order of Application","text":"<p>When using multiple mapping files:</p> <ol> <li>Files are processed in the order specified (or alphabetically for <code>--mappings-dir</code>)</li> <li>All mappings are loaded into a single mappings table before normalization</li> <li>If the same <code>object_id</code> appears in multiple files, the first occurrence is kept</li> </ol>"},{"location":"graph-operations/how-to/normalize-ids/#original-value-preservation","title":"Original Value Preservation","text":"<p>Normalization preserves the original identifier values so you can trace back to the source data.</p>"},{"location":"graph-operations/how-to/normalize-ids/#how-it-works","title":"How It Works","text":"<p>When an edge's <code>subject</code> or <code>object</code> is normalized:</p> <ul> <li>The new (normalized) identifier is written to <code>subject</code> or <code>object</code></li> <li>The original identifier is stored in <code>original_subject</code> or <code>original_object</code></li> </ul>"},{"location":"graph-operations/how-to/normalize-ids/#example","title":"Example","text":"<p>Before normalization:</p> subject object predicate HGNC:1234 OMIM:222100 biolink:gene_associated_with_condition <p>After normalization (with OMIM to MONDO mapping):</p> subject object predicate original_subject original_object HGNC:1234 MONDO:0005148 biolink:gene_associated_with_condition NULL OMIM:222100 <p>Note: <code>original_subject</code> is NULL because <code>HGNC:1234</code> was not in the mappings and was not changed.</p>"},{"location":"graph-operations/how-to/normalize-ids/#querying-original-values","title":"Querying Original Values","text":"<p>You can find all normalized edges:</p> <pre><code>SELECT subject, object, original_subject, original_object\nFROM edges\nWHERE original_subject IS NOT NULL\n   OR original_object IS NOT NULL;\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#duplicate-mapping-handling","title":"Duplicate Mapping Handling","text":"<p>SSSOM files sometimes contain one-to-many mappings where a single <code>object_id</code> maps to multiple <code>subject_id</code> values. Koza handles this to prevent edge duplication.</p>"},{"location":"graph-operations/how-to/normalize-ids/#the-problem","title":"The Problem","text":"<p>If your SSSOM file contains:</p> <pre><code>subject_id  predicate_id    object_id   mapping_justification\nMONDO:0005148   skos:exactMatch OMIM:222100 semapv:ManualMappingCuration\nMONDO:0005149   skos:closeMatch OMIM:222100 semapv:LexicalMatching\n</code></pre> <p>Without deduplication, an edge referencing <code>OMIM:222100</code> could become two edges.</p>"},{"location":"graph-operations/how-to/normalize-ids/#how-koza-handles-it","title":"How Koza Handles It","text":"<p>Koza keeps only one mapping per object_id:</p> <ol> <li>Mappings are ordered by source file, then by <code>subject_id</code></li> <li>The first mapping for each <code>object_id</code> is kept</li> <li>Subsequent duplicates are discarded with a warning</li> </ol>"},{"location":"graph-operations/how-to/normalize-ids/#warning-message","title":"Warning Message","text":"<p>When duplicates are detected:</p> <pre><code>Loading SSSOM mappings...\n  Loaded 45,678 unique mappings from 2 file(s)\n  Found 234 duplicate mappings (one object_id mapped to multiple subject_ids).\n  Keeping only one mapping per object_id.\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#best-practice","title":"Best Practice","text":"<p>To control which mapping is used, ensure your preferred mappings come first:</p> <ol> <li>Order the <code>--mappings</code> arguments with preferred files first</li> <li>Or curate your SSSOM files to have one mapping per identifier</li> </ol>"},{"location":"graph-operations/how-to/normalize-ids/#verification","title":"Verification","text":"<p>After normalization, verify the results.</p>"},{"location":"graph-operations/how-to/normalize-ids/#check-normalization-statistics","title":"Check Normalization Statistics","text":"<pre><code># Count how many edges were normalized\nduckdb graph.duckdb -c \"\n  SELECT\n    COUNT(*) as total_edges,\n    COUNT(original_subject) as normalized_subjects,\n    COUNT(original_object) as normalized_objects\n  FROM edges;\n\"\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#compare-before-and-after","title":"Compare Before and After","text":"<p>Save identifiers before normalization for comparison:</p> <pre><code># Before normalization - export unique identifiers\nduckdb graph.duckdb -c \"\n  SELECT DISTINCT object FROM edges WHERE object LIKE 'OMIM:%'\n\" &gt; before_omim_ids.txt\n\n# Run normalization\nkoza normalize graph.duckdb -m mondo-omim.sssom.tsv\n\n# After normalization - check OMIM IDs are gone\nduckdb graph.duckdb -c \"\n  SELECT DISTINCT object FROM edges WHERE object LIKE 'OMIM:%'\n\" &gt; after_omim_ids.txt\n\n# Compare\ndiff before_omim_ids.txt after_omim_ids.txt\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#verify-mapping-coverage","title":"Verify Mapping Coverage","text":"<p>Check which identifiers were not mapped:</p> <pre><code>-- Find object identifiers that match a pattern but were not normalized\nSELECT DISTINCT object\nFROM edges\nWHERE object LIKE 'OMIM:%'\n  AND original_object IS NULL\nLIMIT 20;\n</code></pre> <p>This shows OMIM IDs that remain in the <code>object</code> column (not normalized), likely because they were not in your mapping file.</p>"},{"location":"graph-operations/how-to/normalize-ids/#generate-a-report","title":"Generate a Report","text":"<p>Use <code>koza report</code> to get overall statistics after normalization:</p> <pre><code>koza report qc -d graph.duckdb -o post_normalize_qc.yaml\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#variations","title":"Variations","text":""},{"location":"graph-operations/how-to/normalize-ids/#using-normalize-in-the-merge-pipeline","title":"Using Normalize in the Merge Pipeline","text":"<p>For new graphs, <code>koza merge</code> includes normalization as part of its pipeline:</p> <pre><code>koza merge \\\n  --nodes \"*.nodes.*\" \\\n  --edges \"*.edges.*\" \\\n  --mappings \"mappings/*.sssom.tsv\" \\\n  --output merged_graph.duckdb\n</code></pre> <p>This runs: join -&gt; deduplicate -&gt; normalize -&gt; prune</p> <p>See the merge command reference for details.</p>"},{"location":"graph-operations/how-to/normalize-ids/#skip-normalization-in-merge","title":"Skip Normalization in Merge","text":"<p>If you want to run merge without normalization:</p> <pre><code>koza merge \\\n  --nodes \"*.nodes.*\" \\\n  --edges \"*.edges.*\" \\\n  --output merged_graph.duckdb \\\n  --skip-normalize\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#normalize-after-other-operations","title":"Normalize After Other Operations","text":"<p>You can run normalize at any point after your database has edges:</p> <pre><code># First, join your files\nkoza join --nodes \"*.nodes.*\" --edges \"*.edges.*\" --output graph.duckdb\n\n# Later, normalize with new mappings\nkoza normalize graph.duckdb -m new_mappings.sssom.tsv\n</code></pre>"},{"location":"graph-operations/how-to/normalize-ids/#see-also","title":"See Also","text":"<ul> <li>CLI Reference: normalize - Full command options</li> <li>Merge Pipeline - Complete merge workflow including normalization</li> <li>SSSOM Specification - Official SSSOM documentation</li> </ul>"},{"location":"graph-operations/how-to/split-graph/","title":"How to Split Graphs","text":""},{"location":"graph-operations/how-to/split-graph/#goal","title":"Goal","text":"<p>Divide a graph into subsets based on field values with optional format conversion. This is useful for:</p> <ul> <li>Separating nodes or edges by data source (<code>provided_by</code>)</li> <li>Creating category-specific subsets (<code>category</code>)</li> <li>Partitioning large graphs for parallel processing</li> <li>Converting between formats (TSV, JSONL, Parquet) during extraction</li> </ul>"},{"location":"graph-operations/how-to/split-graph/#prerequisites","title":"Prerequisites","text":"<ul> <li>A KGX file to split (TSV, JSONL, or Parquet format)</li> <li>Knowledge of which field(s) to split on (e.g., <code>provided_by</code>, <code>category</code>, <code>predicate</code>)</li> </ul>"},{"location":"graph-operations/how-to/split-graph/#output-filename-pattern","title":"Output Filename Pattern","text":"<p>Split generates output filenames automatically using this pattern:</p> <pre><code>{input_basename}_{field_value}_{nodes|edges}.{format}\n</code></pre> <p>For example, splitting <code>monarch_nodes.tsv</code> by <code>provided_by</code> where one value is <code>infores:hgnc</code> produces:</p> <pre><code>monarch_infores_hgnc_nodes.tsv\n</code></pre> <p>The field value has special characters (like <code>:</code>) replaced with underscores.</p>"},{"location":"graph-operations/how-to/split-graph/#split-by-single-field","title":"Split by Single Field","text":"<p>The most common use case is splitting by a single field like <code>provided_by</code> or <code>category</code>.</p>"},{"location":"graph-operations/how-to/split-graph/#split-nodes-by-source","title":"Split nodes by source","text":"<pre><code>koza split monarch_nodes.tsv provided_by --output-dir ./split_by_source\n</code></pre> <p>This creates one file per unique <code>provided_by</code> value in <code>./split_by_source/</code>:</p> <ul> <li><code>monarch_infores_hgnc_nodes.tsv</code> (nodes where <code>provided_by</code> = <code>infores:hgnc</code>)</li> <li><code>monarch_infores_omim_nodes.tsv</code> (nodes where <code>provided_by</code> = <code>infores:omim</code>)</li> <li><code>monarch_infores_mondo_nodes.tsv</code> (nodes where <code>provided_by</code> = <code>infores:mondo</code>)</li> </ul>"},{"location":"graph-operations/how-to/split-graph/#split-edges-by-predicate","title":"Split edges by predicate","text":"<pre><code>koza split monarch_edges.tsv predicate --output-dir ./split_by_predicate\n</code></pre> <p>Output files are named <code>{input}_{predicate_value}_edges.tsv</code>:</p> <pre><code>split_by_predicate/\n  monarch_biolink_has_phenotype_edges.tsv\n  monarch_biolink_causes_edges.tsv\n  monarch_biolink_interacts_with_edges.tsv\n  ...\n</code></pre>"},{"location":"graph-operations/how-to/split-graph/#split-by-multiple-fields","title":"Split by Multiple Fields","text":"<p>Split on multiple fields by providing a comma-separated list. Records are grouped by the unique combination of all specified field values.</p> <pre><code>koza split monarch_edges.tsv predicate,provided_by --output-dir ./split_multi\n</code></pre> <p>This creates files for each unique combination:</p> <pre><code>split_multi/\n  monarch_biolink_has_phenotype_infores_hpoa_edges.tsv\n  monarch_biolink_causes_infores_mondo_edges.tsv\n  ...\n</code></pre>"},{"location":"graph-operations/how-to/split-graph/#format-conversion","title":"Format Conversion","text":"<p>Convert between formats during the split operation using the <code>--format</code> option.</p>"},{"location":"graph-operations/how-to/split-graph/#convert-tsv-to-parquet","title":"Convert TSV to Parquet","text":"<pre><code>koza split monarch_nodes.tsv provided_by \\\n  --output-dir ./parquet_split \\\n  --format parquet\n</code></pre>"},{"location":"graph-operations/how-to/split-graph/#convert-to-jsonl","title":"Convert to JSONL","text":"<pre><code>koza split monarch_edges.tsv predicate \\\n  --output-dir ./jsonl_split \\\n  --format jsonl\n</code></pre> <p>Supported formats:</p> <ul> <li><code>tsv</code> - Tab-separated values (KGX standard)</li> <li><code>jsonl</code> - JSON Lines (one JSON object per line)</li> <li><code>parquet</code> - Apache Parquet (columnar, efficient for analytics)</li> </ul> <p>If <code>--format</code> is not specified, the output format matches the input format.</p>"},{"location":"graph-operations/how-to/split-graph/#handling-multivalued-fields","title":"Handling Multivalued Fields","text":"<p>When splitting on array-type fields (like <code>category</code> or <code>provided_by</code> which can contain multiple values), records may appear in multiple output files.</p> <p>For example, if a node has:</p> <pre><code>{\n  \"id\": \"HGNC:1234\",\n  \"category\": [\"biolink:Gene\", \"biolink:NamedThing\"]\n}\n</code></pre> <p>This node will appear in both:</p> <ul> <li><code>monarch_Gene_nodes.tsv</code></li> <li><code>monarch_NamedThing_nodes.tsv</code></li> </ul> <p>With this behavior, every record appears in every subset it belongs to.</p> <p>The split operation automatically detects array fields and uses appropriate filtering (via <code>list_contains()</code>) to handle them correctly.</p>"},{"location":"graph-operations/how-to/split-graph/#prefix-removal","title":"Prefix Removal","text":"<p>Use <code>--remove-prefixes</code> to strip CURIE prefixes from field values when generating filenames.</p>"},{"location":"graph-operations/how-to/split-graph/#without-prefix-removal-default","title":"Without prefix removal (default)","text":"<pre><code>koza split monarch_nodes.tsv provided_by --output-dir ./output\n</code></pre> <p>Output: <code>monarch_infores_hgnc_nodes.tsv</code></p>"},{"location":"graph-operations/how-to/split-graph/#with-prefix-removal","title":"With prefix removal","text":"<pre><code>koza split monarch_nodes.tsv provided_by \\\n  --output-dir ./output \\\n  --remove-prefixes\n</code></pre> <p>Output: <code>monarch_hgnc_nodes.tsv</code></p> <p>This option applies when:</p> <ul> <li>Field values use common prefixes like <code>infores:</code>, <code>biolink:</code>, or <code>HP:</code></li> <li>Shorter filenames are needed</li> <li>Files should be organized by the local part of the identifier</li> </ul>"},{"location":"graph-operations/how-to/split-graph/#output-directory","title":"Output Directory","text":"<p>Specify a custom output location with <code>--output-dir</code>:</p> <pre><code># Output to current directory\nkoza split data.tsv provided_by --output-dir .\n\n# Output to nested directory (created if it doesn't exist)\nkoza split data.tsv category --output-dir ./processed/split/by_category\n\n# Absolute path\nkoza split data.tsv provided_by --output-dir /data/graphs/split\n</code></pre> <p>The output directory is created automatically if it does not exist.</p>"},{"location":"graph-operations/how-to/split-graph/#verification","title":"Verification","text":"<p>After splitting, verify the operation completed successfully.</p>"},{"location":"graph-operations/how-to/split-graph/#check-output-files-exist","title":"Check output files exist","text":"<pre><code>ls -la ./split_output/\n</code></pre>"},{"location":"graph-operations/how-to/split-graph/#count-records-across-all-split-files","title":"Count records across all split files","text":"<p>For TSV files:</p> <pre><code># Count total lines (excluding headers)\nwc -l ./split_output/*.tsv | tail -1\n\n# Compare to original\nwc -l original_nodes.tsv\n</code></pre> <p>Note: When splitting on multivalued fields, the total across split files may exceed the original count since records can appear in multiple outputs.</p>"},{"location":"graph-operations/how-to/split-graph/#verify-specific-split","title":"Verify specific split","text":"<pre><code># Check a specific split file\nhead -5 ./split_output/monarch_hgnc_nodes.tsv\n\n# Verify records have the expected field value\ngrep \"infores:hgnc\" ./split_output/monarch_hgnc_nodes.tsv | head -3\n</code></pre>"},{"location":"graph-operations/how-to/split-graph/#use-duckdb-for-detailed-verification","title":"Use DuckDB for detailed verification","text":"<pre><code># Load and inspect a split file\nduckdb -c \"SELECT COUNT(*) FROM read_csv_auto('./split_output/monarch_hgnc_nodes.tsv')\"\n\n# Verify field values\nduckdb -c \"SELECT DISTINCT provided_by FROM read_csv_auto('./split_output/monarch_hgnc_nodes.tsv')\"\n</code></pre>"},{"location":"graph-operations/how-to/split-graph/#cli-options-reference","title":"CLI Options Reference","text":"Option Short Description <code>--output-dir</code> <code>-o</code> Output directory for split files (default: <code>./output</code>) <code>--format</code> <code>-f</code> Output format: <code>tsv</code>, <code>jsonl</code>, or <code>parquet</code> (default: preserve input) <code>--remove-prefixes</code> Remove CURIE prefixes from values in filenames <code>--progress</code> <code>-p</code> Show progress bars (default: enabled) <code>--quiet</code> <code>-q</code> Suppress output messages"},{"location":"graph-operations/how-to/split-graph/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Complete CLI documentation</li> <li>How to Join Files - Combine split files back together</li> <li>How to Export Formats - More on format conversion</li> </ul>"},{"location":"graph-operations/reference/","title":"Reference","text":"<p>Technical reference documentation for graph operations. Use these pages to look up specific details about commands, APIs, and configuration options.</p>"},{"location":"graph-operations/reference/#reference-sections","title":"Reference Sections","text":""},{"location":"graph-operations/reference/#cli-reference","title":"CLI Reference","text":"<p>Documentation for all graph operation CLI commands:</p> <ul> <li><code>koza join</code> - Combine KGX files</li> <li><code>koza split</code> - Split by field values</li> <li><code>koza merge</code> - Complete pipeline</li> <li><code>koza normalize</code> - Apply SSSOM mappings</li> <li><code>koza deduplicate</code> - Remove duplicates</li> <li><code>koza prune</code> - Clean graph integrity</li> <li><code>koza append</code> - Add to existing database</li> <li><code>koza report</code> - Generate reports</li> <li><code>koza node-report</code> / <code>koza edge-report</code> - Tabular reports</li> <li><code>koza node-examples</code> / <code>koza edge-examples</code> - Sample extraction</li> </ul> <p>Each command includes:</p> <ul> <li>Synopsis and description</li> <li>All options with types and defaults</li> <li>Usage examples</li> <li>Output description</li> </ul>"},{"location":"graph-operations/reference/#python-api","title":"Python API","text":"<p>Auto-generated documentation from source code docstrings:</p> <ul> <li><code>join_graphs()</code> - Join operation</li> <li><code>split_graph()</code> - Split operation</li> <li><code>merge_graphs()</code> - Pipeline orchestration</li> <li><code>normalize_graph()</code> - SSSOM normalization</li> <li><code>deduplicate_graph()</code> - Deduplication</li> <li><code>prune_graph()</code> - Graph cleaning</li> <li><code>append_graphs()</code> - Incremental updates</li> <li>Report generation functions</li> </ul>"},{"location":"graph-operations/reference/#configuration","title":"Configuration","text":"<p>Detailed documentation for all configuration models:</p> <ul> <li><code>FileSpec</code> - File specification with format detection</li> <li><code>JoinConfig</code> - Join operation settings</li> <li><code>SplitConfig</code> - Split operation settings</li> <li><code>MergeConfig</code> - Pipeline configuration</li> <li><code>NormalizeConfig</code> - Normalization settings</li> <li><code>DeduplicateConfig</code> - Deduplication settings</li> <li><code>PruneConfig</code> - Pruning settings</li> <li><code>AppendConfig</code> - Append settings</li> <li>Report configuration models</li> </ul> <p>Each model includes:</p> <ul> <li>All fields with types</li> <li>Default values</li> <li>Validation rules</li> <li>Usage examples</li> </ul>"},{"location":"graph-operations/reference/#file-formats","title":"File Formats","text":""},{"location":"graph-operations/reference/#supported-input-formats","title":"Supported Input Formats","text":"Format Extension Description TSV <code>.tsv</code> Tab-separated values (KGX standard) JSONL <code>.jsonl</code> JSON Lines (one record per line) Parquet <code>.parquet</code> Columnar binary format <p>Compression supported: <code>.gz</code>, <code>.bz2</code>, <code>.xz</code></p>"},{"location":"graph-operations/reference/#kgx-file-types","title":"KGX File Types","text":"Type Required Columns Optional Columns Nodes <code>id</code> <code>category</code>, <code>name</code>, <code>provided_by</code>, ... Edges <code>subject</code>, <code>predicate</code>, <code>object</code> <code>id</code>, <code>category</code>, <code>provided_by</code>, ..."},{"location":"graph-operations/reference/api/","title":"Python API Reference","text":"<p>This reference documentation is auto-generated from the source code docstrings using mkdocstrings. Each function includes parameter descriptions, return types, and usage examples.</p>"},{"location":"graph-operations/reference/api/#core-operations","title":"Core Operations","text":"<p>These are the primary graph transformation functions. Each operation takes a configuration object and returns a result object with statistics and output information.</p>"},{"location":"graph-operations/reference/api/#join_graphs","title":"join_graphs","text":"<p>Combine multiple KGX files into a single DuckDB database.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.join.join_graphs","title":"<code>join_graphs</code>","text":"<p>Join multiple KGX files into a unified DuckDB database.</p> <p>This operation loads node and edge files from various formats (TSV, JSONL, Parquet) into a single DuckDB database, combining them using UNION ALL BY NAME to handle schema differences across files. Each file's records are tagged with a source identifier for provenance tracking.</p> <p>The join process: 1. Creates or opens a DuckDB database (in-memory if no path specified) 2. Loads each node file into a temporary table with format auto-detection 3. Loads each edge file into a temporary table with format auto-detection 4. Optionally generates a schema report analyzing column types and values 5. Combines all temporary tables into final 'nodes' and 'edges' tables</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>JoinConfig</code> <p>JoinConfig containing: - node_files: List of FileSpec objects for node files - edge_files: List of FileSpec objects for edge files - database_path: Optional path for persistent database (None for in-memory) - schema_reporting: Whether to generate schema analysis report - generate_provided_by: Whether to add provided_by column from source names - quiet: Suppress console output - show_progress: Display progress bars during loading</p> required <p>Returns:</p> Type Description <code>JoinResult</code> <p>JoinResult containing: - files_loaded: List of FileLoadResult with per-file statistics - final_stats: DatabaseStats with node/edge counts and database size - schema_report: Optional schema analysis if schema_reporting enabled - total_time_seconds: Operation duration - database_path: Path to the created database</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If file loading fails or database operations error</p>"},{"location":"graph-operations/reference/api/#split_graph","title":"split_graph","text":"<p>Split a graph database into separate files based on column values.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.split.split_graph","title":"<code>split_graph</code>","text":"<p>Split a KGX file into multiple output files based on field values.</p> <p>This operation partitions a single KGX file (nodes or edges) into separate files based on the unique values of one or more specified fields. Supports format conversion between TSV, JSONL, and Parquet during the split.</p> <p>The split process: 1. Loads the input file into an in-memory DuckDB database 2. Identifies all unique value combinations for the specified split fields 3. For each unique combination, exports matching records to a separate file 4. Output filenames are generated from the split field values</p> <p>Handles multivalued fields (arrays) by using list_contains() for filtering, allowing records to appear in multiple output files if they contain multiple values in the split field.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SplitConfig</code> <p>SplitConfig containing: - input_file: FileSpec for the input KGX file - split_fields: List of column names to split on (e.g., [\"provided_by\"]) - output_directory: Path where split files will be written - output_format: Target format (TSV, JSONL, Parquet); defaults to input format - remove_prefixes: Strip CURIE prefixes from values in output filenames - quiet: Suppress console output - show_progress: Display progress bar during splitting</p> required <p>Returns:</p> Type Description <code>SplitResult</code> <p>SplitResult containing: - input_file: The original input FileSpec - output_files: List of Path objects for created files - total_records_split: Total number of records processed - split_values: List of dicts showing the field value combinations - total_time_seconds: Operation duration</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If input file does not exist</p> <code>Exception</code> <p>If loading or export operations fail</p>"},{"location":"graph-operations/reference/api/#merge_graphs","title":"merge_graphs","text":"<p>End-to-end pipeline combining join, normalize, deduplicate, and prune operations.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.merge.merge_graphs","title":"<code>merge_graphs</code>","text":"<p>Execute the complete merge pipeline: join \u2192 deduplicate \u2192 normalize \u2192 prune.</p> <p>This composite operation orchestrates multiple graph operations in sequence to create a clean, normalized, and validated graph database from multiple source files. It's the recommended way to build a production-ready knowledge graph from raw KGX files and SSSOM mappings.</p> <p>The pipeline steps: 1. Join: Load all node/edge files into a unified DuckDB database 2. Deduplicate: Remove duplicate nodes/edges by ID (optional, skip with skip_deduplicate) 3. Normalize: Apply SSSOM mappings to harmonize identifiers (optional, skip with skip_normalize) 4. Prune: Handle dangling edges and singleton nodes (optional, skip with skip_prune) 5. Export: Optionally export final graph to TSV/JSONL/Parquet files or archive</p> <p>Each step can be skipped via config flags. If a step fails, the pipeline can either abort (default) or continue with remaining steps (continue_on_pipeline_step_error).</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MergeConfig</code> <p>MergeConfig containing: - node_files: List of FileSpec objects for node files - edge_files: List of FileSpec objects for edge files - mapping_files: List of FileSpec objects for SSSOM mapping files - output_database: Path for output database (temp if not specified) - skip_deduplicate: Skip deduplication step - skip_normalize: Skip normalization step (also skipped if no mapping files) - skip_prune: Skip pruning step - keep_singletons/remove_singletons: Singleton node handling in prune step - export_final: Whether to export final graph to files - export_directory: Where to write exported files - output_format: Format for exported files (TSV, JSONL, Parquet) - archive: Create a tar archive instead of loose files - compress: Gzip compress the archive - graph_name: Name prefix for exported files - continue_on_pipeline_step_error: Continue pipeline if a step fails - schema_reporting: Generate schema analysis report - quiet: Suppress console output - show_progress: Display progress bars</p> required <p>Returns:</p> Type Description <code>MergeResult</code> <p>MergeResult containing: - success: Whether the full pipeline completed successfully - join_result/deduplicate_result/normalize_result/prune_result: Per-step results - operations_completed: List of steps that completed successfully - operations_skipped: List of steps that were skipped - final_stats: DatabaseStats with final node/edge counts - database_path: Path to output database (None if temp was used) - exported_files: List of paths to exported files - total_time_seconds: Total pipeline duration - summary: OperationSummary with overall status - errors: List of error messages from failed steps - warnings: List of warning messages</p>"},{"location":"graph-operations/reference/api/#normalize_graph","title":"normalize_graph","text":"<p>Apply SSSOM mappings to normalize identifiers in a graph.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.normalize.normalize_graph","title":"<code>normalize_graph</code>","text":"<p>Apply SSSOM mappings to normalize node identifiers in edge references.</p> <p>This operation uses SSSOM (Simple Standard for Sharing Ontological Mappings) files to replace node identifiers in the edges table with their canonical equivalents. This is useful for harmonizing identifiers from different sources to a common namespace.</p> <p>The normalization process: 1. Loads SSSOM mapping files (TSV format with YAML header) 2. Creates a mappings table, deduplicating by object_id to prevent edge duplication 3. Updates edge subject/object columns using the mappings (object_id -&gt; subject_id) 4. Preserves original identifiers in original_subject/original_object columns</p> <p>Note: Only edge references are normalized. Node IDs in the nodes table are not modified - use the mappings to update node IDs separately if needed.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>NormalizeConfig</code> <p>NormalizeConfig containing: - database_path: Path to the DuckDB database to normalize - mapping_files: List of FileSpec objects for SSSOM mapping files - quiet: Suppress console output - show_progress: Display progress bars during loading</p> required <p>Returns:</p> Type Description <code>NormalizeResult</code> <p>NormalizeResult containing: - success: Whether the operation completed successfully - mappings_loaded: List of FileLoadResult with per-file statistics - edges_normalized: Count of edge references that were updated - final_stats: DatabaseStats with node/edge counts - total_time_seconds: Operation duration - summary: OperationSummary with status and messages - errors: List of error messages if any - warnings: List of warnings (e.g., duplicate mappings found)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no nodes/edges tables exist or no mapping files load</p>"},{"location":"graph-operations/reference/api/#deduplicate_graph","title":"deduplicate_graph","text":"<p>Remove duplicate nodes and edges from a graph database.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.deduplicate.deduplicate_graph","title":"<code>deduplicate_graph</code>","text":"<p>Deduplicate nodes and edges in a graph database.</p> <p>This operation: 1. Identifies nodes/edges with duplicate IDs 2. Copies ALL duplicate rows to duplicate_nodes/duplicate_edges tables 3. Keeps only the first occurrence in main tables (ordered by file_source)</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DeduplicateConfig</code> <p>DeduplicateConfig with database path and options</p> required <p>Returns:</p> Type Description <code>DeduplicateResult</code> <p>DeduplicateResult with deduplication statistics</p>"},{"location":"graph-operations/reference/api/#prune_graph","title":"prune_graph","text":"<p>Remove dangling edges and optionally singleton nodes from a graph.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.prune.prune_graph","title":"<code>prune_graph</code>","text":"<p>Clean up graph integrity issues by handling dangling edges and singleton nodes.</p> <p>This operation identifies and handles two common graph quality issues: - Dangling edges: Edges where subject or object IDs don't exist in the nodes table - Singleton nodes: Nodes that don't appear as subject or object in any edge</p> <p>The prune process: 1. Identifies dangling edges and moves them to a 'dangling_edges' table 2. Based on config, either keeps or moves singleton nodes to 'singleton_nodes' table 3. Optionally filters by minimum connected component size (not yet implemented)</p> <p>Dangling edges and singleton nodes are preserved in separate tables for QC analysis rather than being deleted, allowing investigation of data issues.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PruneConfig</code> <p>PruneConfig containing: - database_path: Path to the DuckDB database to prune - keep_singletons: If True, preserve singleton nodes in main table (default) - remove_singletons: If True, move singleton nodes to singleton_nodes table - min_component_size: Minimum connected component size (not yet implemented) - quiet: Suppress console output - show_progress: Display progress during operations</p> required <p>Returns:</p> Type Description <code>PruneResult</code> <p>PruneResult containing: - database_path: Path to the pruned database - dangling_edges_moved: Count of edges moved to dangling_edges table - singleton_nodes_moved: Count of nodes moved to singleton_nodes table - singleton_nodes_kept: Count of singleton nodes preserved in main table - final_stats: DatabaseStats with final node/edge counts - dangling_edges_by_source: Breakdown of dangling edges by file_source - missing_nodes_by_source: Count of missing node IDs by source - total_time_seconds: Operation duration - success: Whether the operation completed successfully</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If database operations fail</p>"},{"location":"graph-operations/reference/api/#append_graphs","title":"append_graphs","text":"<p>Append additional KGX files to an existing graph database.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.append.append_graphs","title":"<code>append_graphs</code>","text":"<p>Append new KGX files to an existing DuckDB database with schema evolution.</p> <p>This operation adds records from new KGX files to an existing database, automatically handling schema differences. New columns in the input files are added to the existing tables, allowing incremental updates to a graph without re-processing all source files.</p> <p>The append process: 1. Connects to an existing DuckDB database 2. Records the initial schema and statistics 3. For each new file, loads into a temp table and compares schema 4. Adds any new columns to the main table (schema evolution) 5. Inserts records using UNION ALL BY NAME for schema compatibility 6. Optionally deduplicates after appending 7. Generates a schema report if requested</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AppendConfig</code> <p>AppendConfig containing: - database_path: Path to the existing DuckDB database - node_files: List of FileSpec objects for new node files - edge_files: List of FileSpec objects for new edge files - deduplicate: Whether to remove duplicates after appending - schema_reporting: Whether to generate schema analysis report - quiet: Suppress console output - show_progress: Display progress bars during loading</p> required <p>Returns:</p> Type Description <code>AppendResult</code> <p>AppendResult containing: - database_path: Path to the updated database - files_loaded: List of FileLoadResult with per-file statistics - records_added: Net change in record count (nodes + edges) - new_columns_added: Count of new columns added via schema evolution - schema_changes: List of descriptions of schema changes - final_stats: DatabaseStats with updated counts - schema_report: Optional schema analysis if enabled - duplicates_handled: Count of duplicates removed (if deduplication enabled) - total_time_seconds: Operation duration</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If database connection fails or file operations error</p>"},{"location":"graph-operations/reference/api/#reporting-functions","title":"Reporting Functions","text":"<p>These functions generate various reports and statistics about graph databases.</p>"},{"location":"graph-operations/reference/api/#generate_qc_report","title":"generate_qc_report","text":"<p>Generate a quality control report.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.report.generate_qc_report","title":"<code>generate_qc_report</code>","text":"<p>Generate a comprehensive quality control report for a graph database.</p> <p>This operation analyzes a graph database and produces a detailed QC report including node/edge counts by source, duplicate detection, dangling edge analysis, and singleton node counts. The report can be grouped by different columns (e.g., provided_by, file_source).</p> <p>The QC report includes: - Summary: Total nodes, edges, duplicates, dangling edges, singletons - Nodes by source: Count and category breakdown per source - Edges by source: Count and predicate breakdown per source - Dangling edges by source: Edges pointing to non-existent nodes - Duplicate analysis: Nodes/edges with duplicate IDs</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>QCReportConfig</code> <p>QCReportConfig containing: - database_path: Path to the DuckDB database to analyze - output_file: Optional path to write YAML report - group_by: Column to group statistics by (default: \"provided_by\") - quiet: Suppress console output</p> required <p>Returns:</p> Type Description <code>QCReportResult</code> <p>QCReportResult containing: - qc_report: QCReport with all analysis data - output_file: Path where report was written (if specified) - total_time_seconds: Report generation duration</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If database does not exist</p> <code>Exception</code> <p>If database analysis fails</p>"},{"location":"graph-operations/reference/api/#generate_graph_stats","title":"generate_graph_stats","text":"<p>Generate graph statistics.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.report.generate_graph_stats","title":"<code>generate_graph_stats</code>","text":"<p>Generate comprehensive statistical analysis of a graph database.</p> <p>This operation produces detailed statistics about a graph's structure, including node category distributions, edge predicate distributions, degree statistics, and biolink model compliance analysis.</p> <p>The statistics report includes: - Node statistics: Total count, unique categories, category distribution - Edge statistics: Total count, unique predicates, predicate distribution - Predicate details: Subject/object category pairs for each predicate - Biolink compliance: Validation against biolink model categories/predicates</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GraphStatsConfig</code> <p>GraphStatsConfig containing: - database_path: Path to the DuckDB database to analyze - output_file: Optional path to write YAML report - quiet: Suppress console output</p> required <p>Returns:</p> Type Description <code>GraphStatsResult</code> <p>GraphStatsResult containing: - stats_report: GraphStatsReport with all statistics - output_file: Path where report was written (if specified) - total_time_seconds: Report generation duration</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If database does not exist</p> <code>Exception</code> <p>If database analysis fails</p>"},{"location":"graph-operations/reference/api/#generate_schema_compliance_report","title":"generate_schema_compliance_report","text":"<p>Generate a schema analysis and biolink compliance report.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.report.generate_schema_compliance_report","title":"<code>generate_schema_compliance_report</code>","text":"<p>Generate a schema analysis and biolink compliance report.</p> <p>This operation analyzes the schema (columns and data types) of the nodes and edges tables, comparing them against expected biolink model properties and identifying any non-standard or missing columns.</p> <p>The schema report includes: - Table schemas: Column names and DuckDB data types for nodes/edges - Biolink compliance: Which columns match biolink model slots - Non-standard columns: Columns not in the biolink model - Data type analysis: Column type distributions and potential issues</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SchemaReportConfig</code> <p>SchemaReportConfig containing: - database_path: Path to the DuckDB database to analyze - output_file: Optional path to write YAML report - quiet: Suppress console output</p> required <p>Returns:</p> Type Description <code>SchemaReportResult</code> <p>SchemaReportResult containing: - schema_report: SchemaAnalysisReport with all analysis - output_file: Path where report was written (if specified) - total_time_seconds: Report generation duration</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If database does not exist</p> <code>Exception</code> <p>If schema analysis fails</p>"},{"location":"graph-operations/reference/api/#generate_node_report","title":"generate_node_report","text":"<p>Generate a tabular node report grouped by categorical columns.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.report.generate_node_report","title":"<code>generate_node_report</code>","text":"<p>Generate a tabular node report grouped by categorical columns.</p> <p>This operation creates a summary report of nodes grouped by specified categorical columns (e.g., category, provided_by, namespace), with counts for each unique combination. Useful for understanding the distribution and composition of nodes in a graph.</p> <p>Can operate on either an existing DuckDB database or load nodes directly from a KGX file into an in-memory database.</p> <p>Special handling: - \"namespace\" column: Extracts the CURIE prefix from the id column</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>NodeReportConfig</code> <p>NodeReportConfig containing: - database_path: Path to existing database (or None to load from file) - node_file: FileSpec for direct file loading (if no database_path) - categorical_columns: List of columns to group by (e.g., [\"category\", \"provided_by\"]) - output_file: Path to write the report (TSV, CSV, or Parquet) - output_format: Format for output file - quiet: Suppress console output</p> required <p>Returns:</p> Type Description <code>NodeReportResult</code> <p>NodeReportResult containing: - output_file: Path where report was written - total_rows: Number of unique combinations in the report - total_time_seconds: Report generation duration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid columns found for the report</p> <code>Exception</code> <p>If database operations fail</p>"},{"location":"graph-operations/reference/api/#generate_edge_report","title":"generate_edge_report","text":"<p>Generate a tabular edge report with denormalized node information.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.report.generate_edge_report","title":"<code>generate_edge_report</code>","text":"<p>Generate a tabular edge report with denormalized node information.</p> <p>This operation creates a summary report of edges grouped by specified categorical columns, with counts for each unique combination. The report can include denormalized node information (subject_category, object_category) by joining edges with the nodes table.</p> <p>Creates a 'denormalized_edges' view that joins edges with nodes to provide subject and object category information alongside edge predicates.</p> <p>Special handling: - \"subject_namespace\" / \"object_namespace\": Extract CURIE prefixes from subject/object</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EdgeReportConfig</code> <p>EdgeReportConfig containing: - database_path: Path to existing database (or None to load from files) - node_file: FileSpec for node file (if no database_path) - edge_file: FileSpec for edge file (if no database_path) - categorical_columns: List of columns to group by   (e.g., [\"predicate\", \"subject_category\", \"object_category\"]) - output_file: Path to write the report (TSV, CSV, or Parquet) - output_format: Format for output file - quiet: Suppress console output</p> required <p>Returns:</p> Type Description <code>EdgeReportResult</code> <p>EdgeReportResult containing: - output_file: Path where report was written - total_rows: Number of unique combinations in the report - total_time_seconds: Report generation duration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid columns found for the report</p> <code>Exception</code> <p>If database operations fail</p>"},{"location":"graph-operations/reference/api/#generate_node_examples","title":"generate_node_examples","text":"<p>Generate sample nodes for each node type in the graph.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.report.generate_node_examples","title":"<code>generate_node_examples</code>","text":"<p>Generate sample nodes for each node type (category) in the graph.</p> <p>This operation extracts N representative examples for each unique value of the specified type column (typically \"category\"). Useful for data exploration, QC review, and documentation purposes.</p> <p>Uses DuckDB window functions to efficiently sample N rows per type: ROW_NUMBER() OVER (PARTITION BY type_column ORDER BY id) &lt;= sample_size</p> <p>Can operate on either an existing DuckDB database or load nodes directly from a KGX file into an in-memory database.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>NodeExamplesConfig</code> <p>NodeExamplesConfig containing: - database_path: Path to existing database (or None to load from file) - node_file: FileSpec for direct file loading (if no database_path) - type_column: Column to partition by (default: \"category\") - sample_size: Number of examples per type (default: 5) - output_file: Path to write the examples (TSV, CSV, or Parquet) - output_format: Format for output file - quiet: Suppress console output</p> required <p>Returns:</p> Type Description <code>NodeExamplesResult</code> <p>NodeExamplesResult containing: - output_file: Path where examples were written - types_sampled: Number of unique types found - total_examples: Total number of example rows written - total_time_seconds: Report generation duration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If type_column not found in nodes table</p> <code>Exception</code> <p>If database operations fail</p>"},{"location":"graph-operations/reference/api/#generate_edge_examples","title":"generate_edge_examples","text":"<p>Generate sample edges for each edge type pattern in the graph.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.report.generate_edge_examples","title":"<code>generate_edge_examples</code>","text":"<p>Generate sample edges for each edge type pattern in the graph.</p> <p>This operation extracts N representative examples for each unique combination of (subject_category, predicate, object_category). Useful for data exploration, QC review, and understanding the relationship patterns in a knowledge graph.</p> <p>Creates a denormalized view joining edges with nodes to include subject and object category information, then uses DuckDB window functions to efficiently sample N rows per edge type pattern.</p> <p>Can operate on either an existing DuckDB database or load nodes and edges directly from KGX files into an in-memory database.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EdgeExamplesConfig</code> <p>EdgeExamplesConfig containing: - database_path: Path to existing database (or None to load from files) - node_file: FileSpec for node file (if no database_path) - edge_file: FileSpec for edge file (if no database_path) - sample_size: Number of examples per edge type (default: 5) - output_file: Path to write the examples (TSV, CSV, or Parquet) - output_format: Format for output file - quiet: Suppress console output</p> required <p>Returns:</p> Type Description <code>EdgeExamplesResult</code> <p>EdgeExamplesResult containing: - output_file: Path where examples were written - types_sampled: Number of unique edge type patterns found - total_examples: Total number of example rows written - total_time_seconds: Report generation duration</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If database operations fail</p>"},{"location":"graph-operations/reference/api/#helper-functions","title":"Helper Functions","text":"<p>These utility functions simplify common tasks like converting file paths to FileSpec objects.</p>"},{"location":"graph-operations/reference/api/#prepare_file_specs_from_paths","title":"prepare_file_specs_from_paths","text":"<p>Convert file paths to FileSpec objects with format auto-detection.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.join.prepare_file_specs_from_paths","title":"<code>prepare_file_specs_from_paths</code>","text":"<p>Convert file paths to FileSpec objects with format auto-detection.</p> <p>This CLI helper expands glob patterns and creates FileSpec objects for each matched file. The file format (TSV, JSONL, Parquet) is auto-detected from the file extension. Each file's stem is used as its source_name for provenance tracking.</p> <p>Parameters:</p> Name Type Description Default <code>node_paths</code> <code>list[str]</code> <p>List of node file paths or glob patterns (e.g., \"data/*.tsv\")</p> required <code>edge_paths</code> <code>list[str]</code> <p>List of edge file paths or glob patterns (e.g., \"data/*_edges.jsonl\")</p> required <p>Returns:</p> Type Description <code>list[FileSpec]</code> <p>Tuple of (node_file_specs, edge_file_specs) with FileSpec objects</p> <code>list[FileSpec]</code> <p>configured for the appropriate file type (NODES or EDGES)</p>"},{"location":"graph-operations/reference/api/#prepare_merge_config_from_paths","title":"prepare_merge_config_from_paths","text":"<p>Create a MergeConfig from file paths with automatic FileSpec generation.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.merge.prepare_merge_config_from_paths","title":"<code>prepare_merge_config_from_paths</code>","text":"<p>Create a MergeConfig from file paths with automatic FileSpec generation.</p> <p>This CLI helper converts Path objects to FileSpec objects and assembles a complete MergeConfig. File formats are auto-detected from extensions, and file stems are used as source names for provenance tracking.</p> <p>Parameters:</p> Name Type Description Default <code>node_files</code> <code>list[Path]</code> <p>List of Path objects for node KGX files</p> required <code>edge_files</code> <code>list[Path]</code> <p>List of Path objects for edge KGX files</p> required <code>mapping_files</code> <code>list[Path]</code> <p>List of Path objects for SSSOM mapping files</p> required <code>output_database</code> <code>Path | None</code> <p>Optional path for persistent output database</p> <code>None</code> <code>skip_normalize</code> <code>bool</code> <p>If True, skip the normalization step</p> <code>False</code> <code>skip_prune</code> <code>bool</code> <p>If True, skip the pruning step</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional MergeConfig parameters (e.g., quiet, show_progress, export_final, export_directory, archive, compress, graph_name)</p> <code>{}</code> <p>Returns:</p> Type Description <code>MergeConfig</code> <p>Fully configured MergeConfig ready for merge_graphs()</p>"},{"location":"graph-operations/reference/api/#prepare_mapping_file_specs_from_paths","title":"prepare_mapping_file_specs_from_paths","text":"<p>Convert SSSOM mapping file paths to FileSpec objects.</p>"},{"location":"graph-operations/reference/api/#koza.graph_operations.normalize.prepare_mapping_file_specs_from_paths","title":"<code>prepare_mapping_file_specs_from_paths</code>","text":"<p>Convert a list of SSSOM mapping file paths to FileSpec objects.</p> <p>This CLI helper creates FileSpec objects for SSSOM mapping files, which are always in TSV format. Each file's stem is used as its source_name for tracking which mappings came from which file.</p> <p>Parameters:</p> Name Type Description Default <code>mapping_paths</code> <code>list[Path]</code> <p>List of Path objects pointing to SSSOM mapping files</p> required <code>source_name</code> <code>str | None</code> <p>Optional source name to apply to all files (overrides per-file names)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[FileSpec]</code> <p>List of FileSpec objects configured for SSSOM mapping files</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any mapping file does not exist</p>"},{"location":"graph-operations/reference/cli/","title":"CLI Reference","text":"<p>Complete documentation for all graph operation CLI commands.</p>"},{"location":"graph-operations/reference/cli/#koza-join","title":"koza join","text":"<p>Combine multiple KGX files into a unified DuckDB database with automatic schema harmonization.</p>"},{"location":"graph-operations/reference/cli/#synopsis","title":"Synopsis","text":"<pre><code>koza join [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description","title":"Description","text":"<p>The <code>join</code> command loads multiple KGX files (TSV, JSONL, or Parquet) into a single DuckDB database. It automatically handles schema differences between files, filling missing columns with NULL values and preserving extra columns. Supports glob patterns for file discovery and automatic file detection from directories.</p>"},{"location":"graph-operations/reference/cli/#options","title":"Options","text":"Option Short Type Default Description <code>--nodes</code> <code>-n</code> List[str] None Node files or glob patterns (can specify multiple) <code>--edges</code> <code>-e</code> List[str] None Edge files or glob patterns (can specify multiple) <code>--input-dir</code> <code>-d</code> Path None Directory to auto-discover KGX files <code>--output</code> <code>-o</code> str None Path to output database file (default: in-memory) <code>--format</code> <code>-f</code> KGXFormat <code>tsv</code> Output format for any exported files <code>--schema-report</code> bool False Generate schema compliance report <code>--progress</code> <code>-p</code> bool True Show progress bars <code>--quiet</code> <code>-q</code> bool False Suppress output"},{"location":"graph-operations/reference/cli/#examples","title":"Examples","text":"<pre><code># Auto-discover files in directory\nkoza join --input-dir ./data/ -o graph.duckdb\n\n# Use glob patterns for node and edge files\nkoza join -n \"data/*_nodes.tsv\" -e \"data/*_edges.tsv\" -o graph.duckdb\n\n# Mix directory discovery with additional files\nkoza join --input-dir ./data/ -n extra_nodes.tsv -o graph.duckdb\n\n# Multiple individual files from different formats\nkoza join -n genes.tsv -n proteins.jsonl -e interactions.parquet -o graph.duckdb\n\n# Generate schema compliance report\nkoza join -n \"*.nodes.*\" -e \"*.edges.*\" -o graph.duckdb --schema-report\n</code></pre>"},{"location":"graph-operations/reference/cli/#output","title":"Output","text":"<ul> <li>Database file: DuckDB database with <code>nodes</code> and <code>edges</code> tables</li> <li>Schema report: <code>{database}_schema_report.yaml</code> (if <code>--schema-report</code> enabled)</li> <li>CLI summary: File counts, record counts, and schema harmonization details</li> </ul> <p>See also: How to Join KGX Files</p>"},{"location":"graph-operations/reference/cli/#koza-split","title":"koza split","text":"<p>Split a KGX file by specified fields with format conversion support.</p>"},{"location":"graph-operations/reference/cli/#synopsis_1","title":"Synopsis","text":"<pre><code>koza split FILE FIELDS [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_1","title":"Description","text":"<p>The <code>split</code> command extracts subsets of data from a KGX file, creating separate output files for each unique value (or combination of values) in the specified fields. Supports format conversion during split operations.</p>"},{"location":"graph-operations/reference/cli/#arguments","title":"Arguments","text":"Argument Type Description <code>FILE</code> str Path to the KGX file to split (required) <code>FIELDS</code> str Comma-separated list of fields to split on (required)"},{"location":"graph-operations/reference/cli/#options_1","title":"Options","text":"Option Short Type Default Description <code>--output-dir</code> <code>-o</code> Path <code>./output</code> Output directory for split files <code>--format</code> <code>-f</code> KGXFormat None Output format (default: preserve input format) <code>--remove-prefixes</code> bool False Remove prefixes from values in filenames <code>--progress</code> <code>-p</code> bool True Show progress bars <code>--quiet</code> <code>-q</code> bool False Suppress output"},{"location":"graph-operations/reference/cli/#examples_1","title":"Examples","text":"<pre><code># Split nodes by category\nkoza split nodes.tsv category -o ./split_output\n\n# Split edges by predicate and convert to Parquet\nkoza split edges.tsv predicate -o ./parquet_output -f parquet\n\n# Split by multiple fields (creates files per combination)\nkoza split nodes.tsv namespace,category -o ./split_output\n\n# Remove CURIE prefixes from output filenames\nkoza split nodes.tsv category --remove-prefixes -o ./clean_output\n\n# Split with progress tracking\nkoza split large_nodes.tsv provided_by -o ./split_output -p\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_1","title":"Output","text":"<p>For each unique value (or combination) in the split fields, creates: - <code>{value}_nodes.{format}</code> or <code>{value}_edges.{format}</code> depending on input file type</p> <p>When splitting on array-type fields (e.g., <code>category</code>), records may appear in multiple output files if they have multiple values in that field.</p> <p>See also: How to Split a Graph</p>"},{"location":"graph-operations/reference/cli/#koza-merge","title":"koza merge","text":"<p>Run the complete merge pipeline: join, deduplicate, normalize, and prune.</p>"},{"location":"graph-operations/reference/cli/#synopsis_2","title":"Synopsis","text":"<pre><code>koza merge [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_2","title":"Description","text":"<p>The <code>merge</code> command orchestrates a complete graph processing pipeline in sequence:</p> <ol> <li>Join: Load and combine multiple KGX files into a unified database</li> <li>Deduplicate: Remove duplicate nodes and edges by ID</li> <li>Normalize: Apply SSSOM mappings to edge subject/object references</li> <li>Prune: Remove dangling edges and handle singleton nodes</li> </ol> <p>This command runs the complete pipeline for creating a production-ready knowledge graph from multiple sources.</p>"},{"location":"graph-operations/reference/cli/#options_2","title":"Options","text":"Option Short Type Default Description <code>--nodes</code> <code>-n</code> List[str] None Node files or glob patterns (can specify multiple) <code>--edges</code> <code>-e</code> List[str] None Edge files or glob patterns (can specify multiple) <code>--mappings</code> <code>-m</code> List[str] None SSSOM mapping files or glob patterns <code>--input-dir</code> <code>-d</code> Path None Directory to auto-discover KGX files <code>--mappings-dir</code> Path None Directory containing SSSOM mapping files <code>--output</code> <code>-o</code> str None Path to output database file (default: temporary) <code>--export</code> bool False Export final clean data to files <code>--export-dir</code> Path None Directory for exported files (required if <code>--export</code>) <code>--format</code> <code>-f</code> KGXFormat <code>tsv</code> Output format for exported files <code>--archive</code> bool False Export as archive (tar) instead of loose files <code>--compress</code> bool False Compress archive as tar.gz (requires <code>--archive</code>) <code>--graph-name</code> str <code>merged_graph</code> Name for graph files in archive <code>--skip-normalize</code> bool False Skip normalization step <code>--skip-prune</code> bool False Skip pruning step <code>--keep-singletons</code> bool True Keep singleton nodes (default) <code>--remove-singletons</code> bool False Move singleton nodes to separate table <code>--progress</code> <code>-p</code> bool True Show progress bars <code>--quiet</code> <code>-q</code> bool False Suppress output"},{"location":"graph-operations/reference/cli/#examples_2","title":"Examples","text":"<pre><code># Full pipeline with auto-discovery\nkoza merge --input-dir ./data/ --mappings-dir ./sssom/ -o clean_graph.duckdb\n\n# Specific files with export to Parquet\nkoza merge -n nodes.tsv -e edges.tsv -m mappings.sssom.tsv \\\n  --export --export-dir ./output/ -f parquet\n\n# Skip normalization (no SSSOM mappings needed)\nkoza merge -n \"*.nodes.tsv\" -e \"*.edges.tsv\" --skip-normalize -o graph.duckdb\n\n# Create compressed archive for distribution\nkoza merge --input-dir ./data/ -m \"*.sssom.tsv\" \\\n  --export --export-dir ./dist/ --archive --compress --graph-name my_kg\n\n# Custom singleton handling\nkoza merge --input-dir ./data/ -m \"*.sssom.tsv\" --remove-singletons -o graph.duckdb\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_2","title":"Output","text":"<ul> <li>Database file: DuckDB database with cleaned <code>nodes</code> and <code>edges</code> tables</li> <li>Archive tables: <code>duplicate_nodes</code>, <code>duplicate_edges</code>, <code>dangling_edges</code>, <code>singleton_nodes</code> (if applicable)</li> <li>Exported files: KGX files in specified format (if <code>--export</code> enabled)</li> <li>CLI summary: Progress and statistics for each pipeline step</li> </ul> <p>See also: How to Join KGX Files, How to Normalize Identifiers, How to Clean a Graph</p>"},{"location":"graph-operations/reference/cli/#koza-normalize","title":"koza normalize","text":"<p>Apply SSSOM mappings to normalize edge subject/object references.</p>"},{"location":"graph-operations/reference/cli/#synopsis_3","title":"Synopsis","text":"<pre><code>koza normalize DATABASE [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_3","title":"Description","text":"<p>The <code>normalize</code> command loads SSSOM (Simple Standard for Sharing Ontological Mappings) files and applies them to rewrite edge subject and object identifiers to their canonical/equivalent forms. Node identifiers themselves are not changed - only edge references are normalized. Original values are preserved in <code>original_subject</code> and <code>original_object</code> columns.</p>"},{"location":"graph-operations/reference/cli/#arguments_1","title":"Arguments","text":"Argument Type Description <code>DATABASE</code> str Path to existing DuckDB database file (required)"},{"location":"graph-operations/reference/cli/#options_3","title":"Options","text":"Option Short Type Default Description <code>--mappings</code> <code>-m</code> List[str] None SSSOM mapping files or glob patterns (can specify multiple) <code>--mappings-dir</code> <code>-d</code> Path None Directory containing SSSOM mapping files <code>--progress</code> <code>-p</code> bool True Show progress bars <code>--quiet</code> <code>-q</code> bool False Suppress output"},{"location":"graph-operations/reference/cli/#examples_3","title":"Examples","text":"<pre><code># Apply specific mapping files\nkoza normalize graph.duckdb -m gene_mappings.sssom.tsv -m mondo.sssom.tsv\n\n# Auto-discover SSSOM files in directory\nkoza normalize graph.duckdb --mappings-dir ./sssom/\n\n# Apply mappings with glob pattern\nkoza normalize graph.duckdb -m \"mappings/*.sssom.tsv\"\n\n# Quiet operation for automation\nkoza normalize graph.duckdb -m mappings.sssom.tsv -q\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_3","title":"Output","text":"<ul> <li>Modified edges table: <code>subject</code> and <code>object</code> columns updated with mapped identifiers</li> <li>Preservation columns: <code>original_subject</code> and <code>original_object</code> store pre-normalization values</li> <li>CLI summary: Count of loaded mappings and normalized references</li> </ul> <p>Note: When one <code>object_id</code> maps to multiple <code>subject_id</code> values in SSSOM files, only the first mapping is kept to prevent edge duplication.</p> <p>See also: How to Normalize Identifiers</p>"},{"location":"graph-operations/reference/cli/#koza-deduplicate","title":"koza deduplicate","text":"<p>Remove duplicate nodes and edges by ID.</p>"},{"location":"graph-operations/reference/cli/#synopsis_4","title":"Synopsis","text":"<p>The <code>deduplicate</code> operation is included in the <code>merge</code> pipeline but is not exposed as a standalone CLI command. Use <code>koza merge</code> with appropriate options, or <code>koza append --deduplicate</code> for incremental deduplication.</p>"},{"location":"graph-operations/reference/cli/#description_4","title":"Description","text":"<p>Deduplication identifies nodes and edges with duplicate IDs, archives all duplicates to separate tables (<code>duplicate_nodes</code>, <code>duplicate_edges</code>), and keeps only the first occurrence in the main tables. Order is determined by <code>file_source</code> or <code>provided_by</code> fields.</p>"},{"location":"graph-operations/reference/cli/#usage-via-merge","title":"Usage via Merge","text":"<pre><code># Merge includes deduplication by default\nkoza merge -n \"*.nodes.tsv\" -e \"*.edges.tsv\" --skip-normalize -o graph.duckdb\n</code></pre>"},{"location":"graph-operations/reference/cli/#usage-via-append","title":"Usage via Append","text":"<pre><code># Deduplicate during append operation\nkoza append graph.duckdb -n new_nodes.tsv --deduplicate\n</code></pre>"},{"location":"graph-operations/reference/cli/#archive-tables","title":"Archive Tables","text":"<p>After deduplication, inspect removed duplicates:</p> <pre><code>-- View duplicate nodes\nSELECT * FROM duplicate_nodes LIMIT 10;\n\n-- Count duplicates by source\nSELECT file_source, COUNT(*) FROM duplicate_edges GROUP BY file_source;\n</code></pre> <p>See also: How to Perform Incremental Updates</p>"},{"location":"graph-operations/reference/cli/#koza-prune","title":"koza prune","text":"<p>Prune graph by removing dangling edges and handling singleton nodes.</p>"},{"location":"graph-operations/reference/cli/#synopsis_5","title":"Synopsis","text":"<pre><code>koza prune DATABASE [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_5","title":"Description","text":"<p>The <code>prune</code> command cleans up graph integrity issues by identifying and moving dangling edges (edges pointing to non-existent nodes) to a separate table. It can also optionally move singleton nodes (nodes with no edges) to a separate table. Data is never deleted - only moved to archive tables for preservation.</p>"},{"location":"graph-operations/reference/cli/#arguments_2","title":"Arguments","text":"Argument Type Description <code>DATABASE</code> str Path to the DuckDB database file to prune (required)"},{"location":"graph-operations/reference/cli/#options_4","title":"Options","text":"Option Short Type Default Description <code>--keep-singletons</code> bool True* Keep singleton nodes in main table <code>--remove-singletons</code> bool False Move singleton nodes to separate table <code>--min-component-size</code> int None Minimum connected component size (experimental) <code>--progress</code> <code>-p</code> bool True Show progress bars <code>--quiet</code> <code>-q</code> bool False Suppress output <p>*Default behavior: if neither <code>--keep-singletons</code> nor <code>--remove-singletons</code> is specified, singletons are kept.</p>"},{"location":"graph-operations/reference/cli/#examples_4","title":"Examples","text":"<pre><code># Keep singleton nodes, move dangling edges (default)\nkoza prune graph.duckdb\n\n# Explicitly keep singletons\nkoza prune graph.duckdb --keep-singletons\n\n# Remove singleton nodes to separate table\nkoza prune graph.duckdb --remove-singletons\n\n# Experimental: filter small components\nkoza prune graph.duckdb --min-component-size 10\n\n# Quiet operation for automation\nkoza prune graph.duckdb --keep-singletons -q\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_4","title":"Output","text":"<p>Creates archive tables for data preservation: - <code>dangling_edges</code>: Edges pointing to non-existent nodes - <code>singleton_nodes</code>: Isolated nodes (if <code>--remove-singletons</code>) - CLI summary: Counts of edges/nodes moved, integrity statistics</p> <p>See also: How to Clean a Graph</p>"},{"location":"graph-operations/reference/cli/#koza-append","title":"koza append","text":"<p>Append new KGX files to an existing graph database.</p>"},{"location":"graph-operations/reference/cli/#synopsis_6","title":"Synopsis","text":"<pre><code>koza append DATABASE [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_6","title":"Description","text":"<p>The <code>append</code> command adds new data to an existing DuckDB database with automatic schema evolution. New columns in appended files are automatically added to existing tables, with existing records receiving NULL values for new columns. Optional deduplication removes exact duplicates after appending.</p>"},{"location":"graph-operations/reference/cli/#arguments_3","title":"Arguments","text":"Argument Type Description <code>DATABASE</code> str Path to existing DuckDB database file (required)"},{"location":"graph-operations/reference/cli/#options_5","title":"Options","text":"Option Short Type Default Description <code>--nodes</code> <code>-n</code> List[str] None Node files or glob patterns (can specify multiple) <code>--edges</code> <code>-e</code> List[str] None Edge files or glob patterns (can specify multiple) <code>--input-dir</code> <code>-d</code> Path None Directory to auto-discover KGX files <code>--deduplicate</code> bool False Remove duplicates during append <code>--schema-report</code> bool False Generate schema compliance report <code>--progress</code> <code>-p</code> bool True Show progress bars <code>--quiet</code> <code>-q</code> bool False Suppress output"},{"location":"graph-operations/reference/cli/#examples_5","title":"Examples","text":"<pre><code># Append specific files to existing database\nkoza append graph.duckdb -n new_nodes.tsv -e new_edges.tsv\n\n# Auto-discover files in directory and append\nkoza append graph.duckdb --input-dir ./new_data/\n\n# Append with deduplication and schema reporting\nkoza append graph.duckdb -n \"*.tsv\" --deduplicate --schema-report\n\n# Multiple files with glob patterns\nkoza append graph.duckdb -n \"batch2/*.nodes.*\" -e \"batch2/*.edges.*\"\n\n# Quiet append for automation\nkoza append graph.duckdb -n corrections.tsv -q\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_5","title":"Output","text":"<ul> <li>Schema changes: Reports new columns added and their sources</li> <li>Record counts: Before/after record counts for nodes and edges</li> <li>Duplicate statistics: Number of duplicates removed (if <code>--deduplicate</code>)</li> <li>Schema report: Detailed analysis (if <code>--schema-report</code>)</li> </ul> <p>See also: How to Perform Incremental Updates</p>"},{"location":"graph-operations/reference/cli/#koza-report","title":"koza report","text":"<p>Generate comprehensive reports for KGX graph databases.</p>"},{"location":"graph-operations/reference/cli/#synopsis_7","title":"Synopsis","text":"<pre><code>koza report REPORT_TYPE --database DATABASE [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_7","title":"Description","text":"<p>The <code>report</code> command generates various analysis reports for graph databases. Three report types are available: QC (quality control), graph-stats (comprehensive statistics), and schema (database schema analysis).</p>"},{"location":"graph-operations/reference/cli/#arguments_4","title":"Arguments","text":"Argument Type Description <code>REPORT_TYPE</code> str Type of report: <code>qc</code>, <code>graph-stats</code>, or <code>schema</code> (required)"},{"location":"graph-operations/reference/cli/#options_6","title":"Options","text":"Option Short Type Default Description <code>--database</code> <code>-d</code> Path Required Path to DuckDB database file <code>--output</code> <code>-o</code> Path None Path to output report file (YAML format) <code>--quiet</code> <code>-q</code> bool False Suppress progress output"},{"location":"graph-operations/reference/cli/#report-types","title":"Report Types","text":""},{"location":"graph-operations/reference/cli/#qc-quality-control-report","title":"qc - Quality Control Report","text":"<p>Generates quality control analysis grouped by data source, including node/edge counts, category distributions, and potential issues.</p> <pre><code>koza report qc -d merged.duckdb -o qc_report.yaml\n</code></pre>"},{"location":"graph-operations/reference/cli/#graph-stats-graph-statistics-report","title":"graph-stats - Graph Statistics Report","text":"<p>Generates comprehensive graph statistics similar to <code>merged_graph_stats.yaml</code>, including total counts, degree distributions, and connectivity metrics.</p> <pre><code>koza report graph-stats -d merged.duckdb -o graph_stats.yaml\n</code></pre>"},{"location":"graph-operations/reference/cli/#schema-schema-report","title":"schema - Schema Report","text":"<p>Analyzes database schema and biolink compliance, reporting column types, coverage, and potential schema issues.</p> <pre><code>koza report schema -d merged.duckdb -o schema_report.yaml\n</code></pre>"},{"location":"graph-operations/reference/cli/#examples_6","title":"Examples","text":"<pre><code># Generate QC report with output file\nkoza report qc -d merged.duckdb -o qc_report.yaml\n\n# Generate graph statistics\nkoza report graph-stats -d merged.duckdb -o graph_stats.yaml\n\n# Generate schema report\nkoza report schema -d merged.duckdb -o schema_report.yaml\n\n# Quick QC analysis (console output only)\nkoza report qc -d merged.duckdb\n\n# Quiet operation for scripts\nkoza report graph-stats -d merged.duckdb -o stats.yaml -q\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_6","title":"Output","text":"<p>All reports are generated in YAML format and include: - Timestamp: When the report was generated - Database info: Source database path and size - Statistics: Type-specific metrics and analysis</p> <p>See also: How to Generate Reports</p>"},{"location":"graph-operations/reference/cli/#koza-node-report","title":"koza node-report","text":"<p>Generate tabular node reports with categorical column grouping.</p>"},{"location":"graph-operations/reference/cli/#synopsis_8","title":"Synopsis","text":"<pre><code>koza node-report [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_8","title":"Description","text":"<p>The <code>node-report</code> command generates tabular reports showing node counts grouped by categorical columns (namespace, category, provided_by, etc.). Can read from either a DuckDB database or directly from a node file.</p>"},{"location":"graph-operations/reference/cli/#options_7","title":"Options","text":"Option Short Type Default Description <code>--database</code> <code>-d</code> Path None Path to DuckDB database file <code>--file</code> <code>-f</code> Path None Path to node file (TSV, JSONL, or Parquet) <code>--output</code> <code>-o</code> Path None Path to output report file <code>--format</code> TabularReportFormat <code>tsv</code> Output format: <code>tsv</code>, <code>jsonl</code>, or <code>parquet</code> <code>--column</code> <code>-c</code> List[str] None Categorical columns to group by (can specify multiple) <code>--quiet</code> <code>-q</code> bool False Suppress progress output <p>Note: Must specify either <code>--database</code> or <code>--file</code>.</p>"},{"location":"graph-operations/reference/cli/#examples_7","title":"Examples","text":"<pre><code># From database with default columns\nkoza node-report -d merged.duckdb -o node_report.tsv\n\n# From file with Parquet output\nkoza node-report -f nodes.tsv -o node_report.parquet --format parquet\n\n# Custom categorical columns\nkoza node-report -d merged.duckdb -o report.tsv -c namespace -c category -c provided_by\n\n# JSONL output format\nkoza node-report -d merged.duckdb -o node_report.jsonl --format jsonl\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_7","title":"Output","text":"<p>Tabular report with columns for each categorical field plus a count column. Default categorical columns include <code>namespace</code>, <code>category</code>, and <code>provided_by</code> when present.</p> <p>See also: How to Generate Reports</p>"},{"location":"graph-operations/reference/cli/#koza-edge-report","title":"koza edge-report","text":"<p>Generate tabular edge reports with denormalized node information.</p>"},{"location":"graph-operations/reference/cli/#synopsis_9","title":"Synopsis","text":"<pre><code>koza edge-report [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_9","title":"Description","text":"<p>The <code>edge-report</code> command generates tabular reports showing edge counts grouped by categorical columns. When node information is available, it joins edges to nodes to include <code>subject_category</code> and <code>object_category</code> in the grouping.</p>"},{"location":"graph-operations/reference/cli/#options_8","title":"Options","text":"Option Short Type Default Description <code>--database</code> <code>-d</code> Path None Path to DuckDB database file <code>--nodes</code> <code>-n</code> Path None Path to node file (for denormalization) <code>--edges</code> <code>-e</code> Path None Path to edge file (TSV, JSONL, or Parquet) <code>--output</code> <code>-o</code> Path None Path to output report file <code>--format</code> TabularReportFormat <code>tsv</code> Output format: <code>tsv</code>, <code>jsonl</code>, or <code>parquet</code> <code>--column</code> <code>-c</code> List[str] None Categorical columns to group by (can specify multiple) <code>--quiet</code> <code>-q</code> bool False Suppress progress output <p>Note: Must specify either <code>--database</code> or <code>--edges</code>.</p>"},{"location":"graph-operations/reference/cli/#examples_8","title":"Examples","text":"<pre><code># From database with default columns\nkoza edge-report -d merged.duckdb -o edge_report.tsv\n\n# From files with Parquet output\nkoza edge-report -n nodes.tsv -e edges.tsv -o edge_report.parquet --format parquet\n\n# Custom categorical columns\nkoza edge-report -d merged.duckdb -o report.tsv \\\n  -c subject_category -c predicate -c object_category -c primary_knowledge_source\n\n# Edge report without node denormalization\nkoza edge-report -e edges.tsv -o edge_report.tsv\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_8","title":"Output","text":"<p>Tabular report with columns for each categorical field plus a count column. When node information is available, includes <code>subject_category</code> and <code>object_category</code> derived from joining to the nodes table.</p> <p>See also: How to Generate Reports</p>"},{"location":"graph-operations/reference/cli/#koza-node-examples","title":"koza node-examples","text":"<p>Generate sample rows per node type.</p>"},{"location":"graph-operations/reference/cli/#synopsis_10","title":"Synopsis","text":"<pre><code>koza node-examples [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_10","title":"Description","text":"<p>The <code>node-examples</code> command samples N example rows for each distinct value in a type column (default: <code>category</code>). The output can be used for documentation, debugging, and data exploration.</p>"},{"location":"graph-operations/reference/cli/#options_9","title":"Options","text":"Option Short Type Default Description <code>--database</code> <code>-d</code> Path None Path to DuckDB database file <code>--file</code> <code>-f</code> Path None Path to node file (TSV, JSONL, or Parquet) <code>--output</code> <code>-o</code> Path None Path to output examples file <code>--format</code> TabularReportFormat <code>tsv</code> Output format: <code>tsv</code>, <code>jsonl</code>, or <code>parquet</code> <code>--sample-size</code> <code>-n</code> int 5 Number of examples per type <code>--type-column</code> <code>-t</code> str <code>category</code> Column to partition examples by <code>--quiet</code> <code>-q</code> bool False Suppress progress output <p>Note: Must specify either <code>--database</code> or <code>--file</code>.</p>"},{"location":"graph-operations/reference/cli/#examples_9","title":"Examples","text":"<pre><code># From database (5 examples per category)\nkoza node-examples -d merged.duckdb -o node_examples.tsv\n\n# From file with 10 examples per type\nkoza node-examples -f nodes.tsv -o examples.tsv -n 10\n\n# Group by different column\nkoza node-examples -d merged.duckdb -o examples.tsv -t provided_by\n\n# Output as Parquet\nkoza node-examples -d merged.duckdb -o examples.parquet --format parquet\n\n# More examples per category\nkoza node-examples -d merged.duckdb -o examples.tsv -n 20\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_9","title":"Output","text":"<p>Tabular file containing N sample rows for each unique value in the type column. All node columns are preserved in the output.</p> <p>See also: How to Generate Reports</p>"},{"location":"graph-operations/reference/cli/#koza-edge-examples","title":"koza edge-examples","text":"<p>Generate sample rows per edge type.</p>"},{"location":"graph-operations/reference/cli/#synopsis_11","title":"Synopsis","text":"<pre><code>koza edge-examples [OPTIONS]\n</code></pre>"},{"location":"graph-operations/reference/cli/#description_11","title":"Description","text":"<p>The <code>edge-examples</code> command samples N example rows for each distinct combination of type columns (default: <code>subject_category</code>, <code>predicate</code>, <code>object_category</code>). When node information is available, it joins edges to nodes for category information.</p>"},{"location":"graph-operations/reference/cli/#options_10","title":"Options","text":"Option Short Type Default Description <code>--database</code> <code>-d</code> Path None Path to DuckDB database file <code>--nodes</code> <code>-n</code> Path None Path to node file (for denormalization) <code>--edges</code> <code>-e</code> Path None Path to edge file (TSV, JSONL, or Parquet) <code>--output</code> <code>-o</code> Path None Path to output examples file <code>--format</code> TabularReportFormat <code>tsv</code> Output format: <code>tsv</code>, <code>jsonl</code>, or <code>parquet</code> <code>--sample-size</code> <code>-s</code> int 5 Number of examples per type <code>--type-column</code> <code>-t</code> List[str] None Columns to partition examples by (can specify multiple) <code>--quiet</code> <code>-q</code> bool False Suppress progress output <p>Note: Must specify either <code>--database</code> or <code>--edges</code>.</p>"},{"location":"graph-operations/reference/cli/#examples_10","title":"Examples","text":"<pre><code># From database (5 examples per edge type)\nkoza edge-examples -d merged.duckdb -o edge_examples.tsv\n\n# From files with 10 examples\nkoza edge-examples -n nodes.tsv -e edges.tsv -o examples.tsv -s 10\n\n# Custom type columns\nkoza edge-examples -d merged.duckdb -o examples.tsv -t predicate -t primary_knowledge_source\n\n# Output as Parquet\nkoza edge-examples -d merged.duckdb -o examples.parquet --format parquet\n\n# More examples per edge type\nkoza edge-examples -d merged.duckdb -o examples.tsv -s 20\n</code></pre>"},{"location":"graph-operations/reference/cli/#output_10","title":"Output","text":"<p>Tabular file containing N sample rows for each unique combination of type columns. When node information is available, includes <code>subject_category</code> and <code>object_category</code> columns.</p> <p>See also: How to Generate Reports</p>"},{"location":"graph-operations/reference/cli/#common-patterns","title":"Common Patterns","text":""},{"location":"graph-operations/reference/cli/#file-specification-formats","title":"File Specification Formats","text":"<p>All commands that accept file lists support multiple specification formats:</p>"},{"location":"graph-operations/reference/cli/#glob-patterns","title":"Glob Patterns","text":"<pre><code># Match all node files\n--nodes \"*.nodes.*\"\n\n# Match specific formats\n--nodes \"*.tsv\" --edges \"*.jsonl\"\n\n# Match files in subdirectories\n--nodes \"data/*_nodes.tsv\"\n</code></pre>"},{"location":"graph-operations/reference/cli/#multiple-files","title":"Multiple Files","text":"<pre><code># Specify multiple files individually\n--nodes genes.tsv --nodes proteins.tsv --edges interactions.tsv\n\n# Or as a list\n--nodes genes.tsv proteins.jsonl pathways.parquet\n</code></pre>"},{"location":"graph-operations/reference/cli/#progress-and-output-control","title":"Progress and Output Control","text":""},{"location":"graph-operations/reference/cli/#progress-indicators","title":"Progress Indicators","text":"<ul> <li><code>--progress</code> / <code>-p</code>: Display progress bars (enabled by default for most commands)</li> <li><code>--quiet</code> / <code>-q</code>: Suppress all non-error output</li> </ul>"},{"location":"graph-operations/reference/cli/#output-formats","title":"Output Formats","text":"<p>Supported formats for tabular reports and exports: - TSV: Tab-separated values (KGX standard) - JSONL: JSON Lines format - Parquet: Columnar format for analytics</p>"},{"location":"graph-operations/reference/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error (file not found, permission denied, etc.) 2 Invalid arguments or configuration 130 Interrupted by user (Ctrl+C)"},{"location":"graph-operations/reference/cli/#getting-help","title":"Getting Help","text":"<pre><code># General help\nkoza --help\n\n# Command-specific help\nkoza join --help\nkoza split --help\nkoza merge --help\nkoza normalize --help\nkoza prune --help\nkoza append --help\nkoza report --help\nkoza node-report --help\nkoza edge-report --help\nkoza node-examples --help\nkoza edge-examples --help\n\n# Version information\nkoza --version\n</code></pre>"},{"location":"graph-operations/reference/configuration/","title":"Configuration Reference","text":"<p>This reference documents all Pydantic configuration models used in Koza graph operations.</p>"},{"location":"graph-operations/reference/configuration/#enums","title":"Enums","text":""},{"location":"graph-operations/reference/configuration/#kgxformat","title":"KGXFormat","text":"<p>Supported KGX file formats.</p> Value Description <code>TSV</code> Tab-separated values format <code>JSONL</code> JSON Lines format <code>PARQUET</code> Apache Parquet format"},{"location":"graph-operations/reference/configuration/#kgxfiletype","title":"KGXFileType","text":"<p>KGX file types.</p> Value Description <code>NODES</code> Node file <code>EDGES</code> Edge file"},{"location":"graph-operations/reference/configuration/#tabularreportformat","title":"TabularReportFormat","text":"<p>Supported formats for tabular reports.</p> Value Description <code>TSV</code> Tab-separated values format <code>JSONL</code> JSON Lines format <code>PARQUET</code> Apache Parquet format"},{"location":"graph-operations/reference/configuration/#file-handling","title":"File Handling","text":""},{"location":"graph-operations/reference/configuration/#filespec","title":"FileSpec","text":"<p>Specification for a KGX file with automatic format detection and source attribution.</p>"},{"location":"graph-operations/reference/configuration/#fields","title":"Fields","text":"Field Type Default Description <code>path</code> <code>Path</code> required Path to the file <code>source_name</code> <code>str \\| None</code> file stem Source attribution name (defaults to filename stem) <code>format</code> <code>KGXFormat \\| None</code> auto-detect File format (TSV, JSONL, PARQUET) <code>file_type</code> <code>KGXFileType \\| None</code> auto-detect File type (NODES, EDGES)"},{"location":"graph-operations/reference/configuration/#validation-rules","title":"Validation Rules","text":"<ul> <li>Format Detection: If <code>format</code> is not provided, it is auto-detected from the file extension:<ul> <li><code>.tsv</code>, <code>.txt</code> -&gt; <code>TSV</code></li> <li><code>.jsonl</code>, <code>.json</code> -&gt; <code>JSONL</code></li> <li><code>.parquet</code> -&gt; <code>PARQUET</code></li> <li>Compressed files (<code>.gz</code>, <code>.bz2</code>, <code>.xz</code>) are handled by stripping the compression suffix first</li> </ul> </li> <li>File Type Detection: If <code>file_type</code> is not provided, it is auto-detected from the filename:<ul> <li>Files containing <code>_nodes.</code> or starting with <code>nodes.</code> -&gt; <code>NODES</code></li> <li>Files containing <code>_edges.</code> or starting with <code>edges.</code> -&gt; <code>EDGES</code></li> </ul> </li> <li>Source Name: If not provided, defaults to the stem of the file path</li> </ul>"},{"location":"graph-operations/reference/configuration/#example","title":"Example","text":"<pre><code>from koza.model.graph_operations import FileSpec\n\n# Full specification\nfile_spec = FileSpec(\n    path=Path(\"data/monarch_nodes.tsv\"),\n    source_name=\"monarch\",\n    format=KGXFormat.TSV,\n    file_type=KGXFileType.NODES\n)\n\n# With auto-detection (recommended)\nfile_spec = FileSpec(path=Path(\"data/monarch_nodes.tsv\"))\n# Automatically detects: format=TSV, file_type=NODES, source_name=\"monarch_nodes\"\n</code></pre>"},{"location":"graph-operations/reference/configuration/#operation-configurations","title":"Operation Configurations","text":""},{"location":"graph-operations/reference/configuration/#joinconfig","title":"JoinConfig","text":"<p>Configuration for the join operation, which loads multiple KGX files into a unified database.</p>"},{"location":"graph-operations/reference/configuration/#fields_1","title":"Fields","text":"Field Type Default Description <code>node_files</code> <code>list[FileSpec]</code> <code>[]</code> List of node files to join <code>edge_files</code> <code>list[FileSpec]</code> <code>[]</code> List of edge files to join <code>output_database</code> <code>Path \\| None</code> <code>None</code> Path for output DuckDB database <code>schema_reporting</code> <code>bool</code> <code>True</code> Enable schema analysis reporting <code>preserve_duplicates</code> <code>bool</code> <code>False</code> Keep duplicate records during join <code>generate_provided_by</code> <code>bool</code> <code>True</code> Add <code>provided_by</code> column from filename <code>database_path</code> <code>Path \\| None</code> <code>None</code> Database path (inherited from base) <code>output_format</code> <code>KGXFormat</code> <code>TSV</code> Output format for exports <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <code>show_progress</code> <code>bool</code> <code>True</code> Show progress indicators"},{"location":"graph-operations/reference/configuration/#validation-rules_1","title":"Validation Rules","text":"<ul> <li>If <code>output_database</code> is provided, it automatically sets <code>database_path</code> for compatibility</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_1","title":"Example","text":"<pre><code>from koza.model.graph_operations import JoinConfig, FileSpec\n\nconfig = JoinConfig(\n    node_files=[\n        FileSpec(path=Path(\"data/source1_nodes.tsv\")),\n        FileSpec(path=Path(\"data/source2_nodes.tsv\")),\n    ],\n    edge_files=[\n        FileSpec(path=Path(\"data/source1_edges.tsv\")),\n        FileSpec(path=Path(\"data/source2_edges.tsv\")),\n    ],\n    output_database=Path(\"merged.duckdb\"),\n    generate_provided_by=True\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#joinresult","title":"JoinResult","text":"<p>Result of the join operation.</p>"},{"location":"graph-operations/reference/configuration/#fields_2","title":"Fields","text":"Field Type Default Description <code>files_loaded</code> <code>list[FileLoadResult]</code> required Details for each loaded file <code>final_stats</code> <code>DatabaseStats</code> required Final database statistics <code>schema_report</code> <code>dict[str, Any] \\| None</code> <code>None</code> Schema analysis report <code>total_time_seconds</code> <code>float</code> required Total operation time <code>database_path</code> <code>Path \\| None</code> <code>None</code> Path to the created database"},{"location":"graph-operations/reference/configuration/#splitconfig","title":"SplitConfig","text":"<p>Configuration for the split operation, which splits a KGX file based on field values.</p>"},{"location":"graph-operations/reference/configuration/#fields_3","title":"Fields","text":"Field Type Default Description <code>input_file</code> <code>FileSpec</code> required Input file to split <code>split_fields</code> <code>list[str]</code> required Fields to split on <code>output_directory</code> <code>Path</code> <code>./output</code> Directory for output files <code>remove_prefixes</code> <code>bool</code> <code>False</code> Remove prefixes from split values <code>output_format</code> <code>KGXFormat \\| None</code> <code>None</code> Output format (None = preserve original) <code>database_path</code> <code>Path \\| None</code> <code>None</code> Database path (inherited from base) <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <code>show_progress</code> <code>bool</code> <code>True</code> Show progress indicators"},{"location":"graph-operations/reference/configuration/#example_2","title":"Example","text":"<pre><code>from koza.model.graph_operations import SplitConfig, FileSpec\n\nconfig = SplitConfig(\n    input_file=FileSpec(path=Path(\"data/all_nodes.tsv\")),\n    split_fields=[\"category\", \"provided_by\"],\n    output_directory=Path(\"split_output\"),\n    remove_prefixes=True\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#splitresult","title":"SplitResult","text":"<p>Result of the split operation.</p>"},{"location":"graph-operations/reference/configuration/#fields_4","title":"Fields","text":"Field Type Default Description <code>input_file</code> <code>FileSpec</code> required The input file that was split <code>output_files</code> <code>list[Path]</code> required Paths to created output files <code>total_records_split</code> <code>int</code> required Total records processed <code>split_values</code> <code>list[dict[str, str]]</code> required Unique value combinations found <code>total_time_seconds</code> <code>float</code> required Total operation time"},{"location":"graph-operations/reference/configuration/#mergeconfig","title":"MergeConfig","text":"<p>Configuration for the merge operation, which is a composite pipeline combining join, deduplicate, normalize, and prune operations.</p>"},{"location":"graph-operations/reference/configuration/#fields_5","title":"Fields","text":"Field Type Default Description Input Files <code>node_files</code> <code>list[FileSpec]</code> <code>[]</code> List of node files to merge <code>edge_files</code> <code>list[FileSpec]</code> <code>[]</code> List of edge files to merge <code>mapping_files</code> <code>list[FileSpec]</code> <code>[]</code> SSSOM mapping files for normalization Pipeline Options <code>skip_deduplicate</code> <code>bool</code> <code>False</code> Skip deduplication step <code>skip_normalize</code> <code>bool</code> <code>False</code> Skip normalization step <code>skip_prune</code> <code>bool</code> <code>False</code> Skip pruning step <code>generate_provided_by</code> <code>bool</code> <code>True</code> Add <code>provided_by</code> column from filename <code>continue_on_pipeline_step_error</code> <code>bool</code> <code>True</code> Continue on non-critical errors Prune Options <code>keep_singletons</code> <code>bool</code> <code>True</code> Preserve isolated nodes <code>remove_singletons</code> <code>bool</code> <code>False</code> Move singletons to separate table Output Options <code>output_database</code> <code>Path \\| None</code> <code>None</code> Path for output database (None = temporary) <code>output_format</code> <code>KGXFormat</code> <code>TSV</code> Format for exported files <code>export_final</code> <code>bool</code> <code>False</code> Export final clean data to files <code>export_directory</code> <code>Path \\| None</code> <code>None</code> Directory for exported files <code>archive</code> <code>bool</code> <code>False</code> Export as archive instead of loose files <code>compress</code> <code>bool</code> <code>False</code> Compress archive as tar.gz <code>graph_name</code> <code>str \\| None</code> <code>None</code> Name for graph files in archive General Options <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <code>show_progress</code> <code>bool</code> <code>True</code> Show progress indicators <code>schema_reporting</code> <code>bool</code> <code>True</code> Enable schema analysis reporting"},{"location":"graph-operations/reference/configuration/#validation-rules_2","title":"Validation Rules","text":"<ul> <li>Files Required: At least one node or edge file must be provided</li> <li>Normalize Requirements: If <code>skip_normalize</code> is <code>False</code>, mapping files must be provided</li> <li>Singleton Options: Cannot set both <code>keep_singletons</code> and <code>remove_singletons</code> to <code>True</code></li> <li>Export Requirements: If <code>export_final</code> is <code>True</code>, <code>export_directory</code> must be provided</li> <li>Archive Requirements: <code>compress</code> requires <code>archive</code> to be enabled</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_3","title":"Example","text":"<pre><code>from koza.model.graph_operations import MergeConfig, FileSpec\n\nconfig = MergeConfig(\n    node_files=[\n        FileSpec(path=Path(\"data/source1_nodes.tsv\")),\n        FileSpec(path=Path(\"data/source2_nodes.tsv\")),\n    ],\n    edge_files=[\n        FileSpec(path=Path(\"data/source1_edges.tsv\")),\n        FileSpec(path=Path(\"data/source2_edges.tsv\")),\n    ],\n    mapping_files=[\n        FileSpec(path=Path(\"mappings/disease_mappings.sssom.tsv\")),\n    ],\n    output_database=Path(\"merged.duckdb\"),\n    export_final=True,\n    export_directory=Path(\"output\"),\n    archive=True,\n    compress=True,\n    graph_name=\"my-knowledge-graph\"\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#mergeresult","title":"MergeResult","text":"<p>Result of the merge operation.</p>"},{"location":"graph-operations/reference/configuration/#fields_6","title":"Fields","text":"Field Type Default Description <code>success</code> <code>bool</code> required Whether the operation succeeded <code>join_result</code> <code>JoinResult \\| None</code> <code>None</code> Result from join step <code>deduplicate_result</code> <code>DeduplicateResult \\| None</code> <code>None</code> Result from deduplicate step <code>normalize_result</code> <code>NormalizeResult \\| None</code> <code>None</code> Result from normalize step <code>prune_result</code> <code>PruneResult \\| None</code> <code>None</code> Result from prune step <code>operations_completed</code> <code>list[str]</code> <code>[]</code> Names of completed operations <code>operations_skipped</code> <code>list[str]</code> <code>[]</code> Names of skipped operations <code>final_stats</code> <code>DatabaseStats \\| None</code> <code>None</code> Final database statistics <code>database_path</code> <code>Path \\| None</code> <code>None</code> Path to the database <code>exported_files</code> <code>list[Path]</code> <code>[]</code> Paths to exported files <code>total_time_seconds</code> <code>float</code> required Total operation time <code>summary</code> <code>OperationSummary</code> required Summary for CLI output <code>errors</code> <code>list[str]</code> <code>[]</code> Error messages <code>warnings</code> <code>list[str]</code> <code>[]</code> Warning messages"},{"location":"graph-operations/reference/configuration/#normalizeconfig","title":"NormalizeConfig","text":"<p>Configuration for the normalize operation, which applies SSSOM mappings to normalize identifiers in edges.</p>"},{"location":"graph-operations/reference/configuration/#fields_7","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the DuckDB database <code>mapping_files</code> <code>list[FileSpec]</code> <code>[]</code> SSSOM mapping files <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <code>show_progress</code> <code>bool</code> <code>True</code> Show progress indicators"},{"location":"graph-operations/reference/configuration/#validation-rules_3","title":"Validation Rules","text":"<ul> <li>Database Exists: The database file must exist</li> <li>Mapping Files Required: At least one SSSOM mapping file must be provided</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_4","title":"Example","text":"<pre><code>from koza.model.graph_operations import NormalizeConfig, FileSpec\n\nconfig = NormalizeConfig(\n    database_path=Path(\"merged.duckdb\"),\n    mapping_files=[\n        FileSpec(path=Path(\"mappings/disease_mappings.sssom.tsv\")),\n        FileSpec(path=Path(\"mappings/gene_mappings.sssom.tsv\")),\n    ]\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#normalizeresult","title":"NormalizeResult","text":"<p>Result of the normalize operation.</p>"},{"location":"graph-operations/reference/configuration/#fields_8","title":"Fields","text":"Field Type Default Description <code>success</code> <code>bool</code> required Whether the operation succeeded <code>mappings_loaded</code> <code>list[FileLoadResult]</code> required Details for each loaded mapping file <code>edges_normalized</code> <code>int</code> required Number of edges normalized <code>final_stats</code> <code>DatabaseStats \\| None</code> <code>None</code> Final database statistics <code>total_time_seconds</code> <code>float</code> required Total operation time <code>summary</code> <code>OperationSummary</code> required Summary for CLI output <code>errors</code> <code>list[str]</code> <code>[]</code> Error messages <code>warnings</code> <code>list[str]</code> <code>[]</code> Warning messages"},{"location":"graph-operations/reference/configuration/#deduplicateconfig","title":"DeduplicateConfig","text":"<p>Configuration for the deduplicate operation, which removes duplicate nodes and edges.</p>"},{"location":"graph-operations/reference/configuration/#fields_9","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the DuckDB database <code>deduplicate_nodes</code> <code>bool</code> <code>True</code> Deduplicate nodes table <code>deduplicate_edges</code> <code>bool</code> <code>True</code> Deduplicate edges table <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <code>show_progress</code> <code>bool</code> <code>True</code> Show progress indicators"},{"location":"graph-operations/reference/configuration/#validation-rules_4","title":"Validation Rules","text":"<ul> <li>Database Exists: The database file must exist</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_5","title":"Example","text":"<pre><code>from koza.model.graph_operations import DeduplicateConfig\n\nconfig = DeduplicateConfig(\n    database_path=Path(\"merged.duckdb\"),\n    deduplicate_nodes=True,\n    deduplicate_edges=True\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#deduplicateresult","title":"DeduplicateResult","text":"<p>Result of the deduplicate operation.</p>"},{"location":"graph-operations/reference/configuration/#fields_10","title":"Fields","text":"Field Type Default Description <code>success</code> <code>bool</code> required Whether the operation succeeded <code>duplicate_nodes_found</code> <code>int</code> <code>0</code> Number of duplicate nodes found <code>duplicate_nodes_removed</code> <code>int</code> <code>0</code> Rows removed from nodes table <code>duplicate_edges_found</code> <code>int</code> <code>0</code> Number of duplicate edges found <code>duplicate_edges_removed</code> <code>int</code> <code>0</code> Rows removed from edges table <code>final_stats</code> <code>DatabaseStats \\| None</code> <code>None</code> Final database statistics <code>total_time_seconds</code> <code>float</code> required Total operation time <code>summary</code> <code>OperationSummary</code> required Summary for CLI output <code>errors</code> <code>list[str]</code> <code>[]</code> Error messages <code>warnings</code> <code>list[str]</code> <code>[]</code> Warning messages"},{"location":"graph-operations/reference/configuration/#pruneconfig","title":"PruneConfig","text":"<p>Configuration for the prune operation, which handles dangling edges and singleton nodes.</p>"},{"location":"graph-operations/reference/configuration/#fields_11","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the DuckDB database <code>keep_singletons</code> <code>bool</code> <code>True</code> Preserve isolated nodes <code>remove_singletons</code> <code>bool</code> <code>False</code> Move singletons to separate table <code>min_component_size</code> <code>int \\| None</code> <code>None</code> Minimum connected component size <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <code>show_progress</code> <code>bool</code> <code>True</code> Show progress indicators <code>output_format</code> <code>KGXFormat \\| None</code> <code>None</code> Format for any exported files"},{"location":"graph-operations/reference/configuration/#validation-rules_5","title":"Validation Rules","text":"<ul> <li>Database Exists: The database file must exist</li> <li>Singleton Options: Cannot set both <code>keep_singletons</code> and <code>remove_singletons</code> to <code>True</code></li> </ul>"},{"location":"graph-operations/reference/configuration/#example_6","title":"Example","text":"<pre><code>from koza.model.graph_operations import PruneConfig\n\nconfig = PruneConfig(\n    database_path=Path(\"merged.duckdb\"),\n    keep_singletons=False,\n    remove_singletons=True\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#pruneresult","title":"PruneResult","text":"<p>Result of the prune operation.</p>"},{"location":"graph-operations/reference/configuration/#fields_12","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the database <code>dangling_edges_moved</code> <code>int</code> required Number of dangling edges moved <code>singleton_nodes_moved</code> <code>int</code> required Number of singleton nodes moved <code>singleton_nodes_kept</code> <code>int</code> required Number of singleton nodes kept <code>final_stats</code> <code>DatabaseStats</code> required Final database statistics <code>dangling_edges_by_source</code> <code>dict[str, int]</code> <code>{}</code> Dangling edges grouped by source <code>missing_nodes_by_source</code> <code>dict[str, int]</code> <code>{}</code> Missing nodes grouped by source <code>total_time_seconds</code> <code>float</code> required Total operation time <code>success</code> <code>bool</code> required Whether the operation succeeded <code>errors</code> <code>list[str]</code> <code>[]</code> Error messages"},{"location":"graph-operations/reference/configuration/#appendconfig","title":"AppendConfig","text":"<p>Configuration for the append operation, which adds new files to an existing database.</p>"},{"location":"graph-operations/reference/configuration/#fields_13","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the existing DuckDB database <code>node_files</code> <code>list[FileSpec]</code> <code>[]</code> Node files to append <code>edge_files</code> <code>list[FileSpec]</code> <code>[]</code> Edge files to append <code>deduplicate</code> <code>bool</code> <code>False</code> Run deduplication after append <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <code>show_progress</code> <code>bool</code> <code>True</code> Show progress indicators <code>schema_reporting</code> <code>bool</code> <code>False</code> Enable schema analysis reporting"},{"location":"graph-operations/reference/configuration/#validation-rules_6","title":"Validation Rules","text":"<ul> <li>Database Exists: The database file must exist</li> <li>Files Required: At least one node or edge file must be provided</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_7","title":"Example","text":"<pre><code>from koza.model.graph_operations import AppendConfig, FileSpec\n\nconfig = AppendConfig(\n    database_path=Path(\"merged.duckdb\"),\n    node_files=[\n        FileSpec(path=Path(\"data/new_nodes.tsv\")),\n    ],\n    edge_files=[\n        FileSpec(path=Path(\"data/new_edges.tsv\")),\n    ],\n    deduplicate=True\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#appendresult","title":"AppendResult","text":"<p>Result of the append operation.</p>"},{"location":"graph-operations/reference/configuration/#fields_14","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the database <code>files_loaded</code> <code>list[FileLoadResult]</code> required Details for each loaded file <code>records_added</code> <code>int</code> required Number of records added <code>new_columns_added</code> <code>int</code> required Number of new columns added <code>schema_changes</code> <code>list[str]</code> <code>[]</code> Description of schema changes <code>final_stats</code> <code>DatabaseStats</code> required Final database statistics <code>schema_report</code> <code>dict[str, Any] \\| None</code> <code>None</code> Schema analysis report <code>duplicates_handled</code> <code>int</code> <code>0</code> Number of duplicates handled <code>total_time_seconds</code> <code>float</code> required Total operation time"},{"location":"graph-operations/reference/configuration/#report-configurations","title":"Report Configurations","text":""},{"location":"graph-operations/reference/configuration/#qcreportconfig","title":"QCReportConfig","text":"<p>Configuration for QC (Quality Control) report generation.</p>"},{"location":"graph-operations/reference/configuration/#fields_15","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the DuckDB database <code>output_file</code> <code>Path \\| None</code> <code>None</code> Path for output YAML file <code>group_by</code> <code>str</code> <code>\"provided_by\"</code> Column to group statistics by <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output"},{"location":"graph-operations/reference/configuration/#example_8","title":"Example","text":"<pre><code>from koza.model.graph_operations import QCReportConfig\n\nconfig = QCReportConfig(\n    database_path=Path(\"merged.duckdb\"),\n    output_file=Path(\"qc_report.yaml\"),\n    group_by=\"provided_by\"\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#graphstatsconfig","title":"GraphStatsConfig","text":"<p>Configuration for graph statistics report generation.</p>"},{"location":"graph-operations/reference/configuration/#fields_16","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the DuckDB database <code>output_file</code> <code>Path \\| None</code> <code>None</code> Path for output YAML file <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output"},{"location":"graph-operations/reference/configuration/#example_9","title":"Example","text":"<pre><code>from koza.model.graph_operations import GraphStatsConfig\n\nconfig = GraphStatsConfig(\n    database_path=Path(\"merged.duckdb\"),\n    output_file=Path(\"graph_stats.yaml\")\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#schemareportconfig","title":"SchemaReportConfig","text":"<p>Configuration for schema analysis report generation.</p>"},{"location":"graph-operations/reference/configuration/#fields_17","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path</code> required Path to the DuckDB database <code>output_file</code> <code>Path \\| None</code> <code>None</code> Path for output YAML file <code>include_biolink_compliance</code> <code>bool</code> <code>True</code> Include Biolink model compliance analysis <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output"},{"location":"graph-operations/reference/configuration/#example_10","title":"Example","text":"<pre><code>from koza.model.graph_operations import SchemaReportConfig\n\nconfig = SchemaReportConfig(\n    database_path=Path(\"merged.duckdb\"),\n    output_file=Path(\"schema_report.yaml\"),\n    include_biolink_compliance=True\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#nodereportconfig","title":"NodeReportConfig","text":"<p>Configuration for tabular node report generation.</p>"},{"location":"graph-operations/reference/configuration/#fields_18","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path \\| None</code> <code>None</code> Path to the DuckDB database <code>node_file</code> <code>FileSpec \\| None</code> <code>None</code> Node file to load (alternative to database) <code>output_file</code> <code>Path \\| None</code> <code>None</code> Path for output file <code>output_format</code> <code>TabularReportFormat</code> <code>TSV</code> Output format <code>categorical_columns</code> <code>list[str]</code> <code>[\"namespace\", \"category\", \"in_taxon\", \"provided_by\"]</code> Columns to group by <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output"},{"location":"graph-operations/reference/configuration/#validation-rules_7","title":"Validation Rules","text":"<ul> <li>Input Required: Either <code>database_path</code> or <code>node_file</code> must be provided</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_11","title":"Example","text":"<pre><code>from koza.model.graph_operations import NodeReportConfig\n\nconfig = NodeReportConfig(\n    database_path=Path(\"merged.duckdb\"),\n    output_file=Path(\"node_report.tsv\"),\n    categorical_columns=[\"category\", \"provided_by\"]\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#edgereportconfig","title":"EdgeReportConfig","text":"<p>Configuration for tabular edge report generation.</p>"},{"location":"graph-operations/reference/configuration/#fields_19","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path \\| None</code> <code>None</code> Path to the DuckDB database <code>node_file</code> <code>FileSpec \\| None</code> <code>None</code> Node file to load (for category enrichment) <code>edge_file</code> <code>FileSpec \\| None</code> <code>None</code> Edge file to load (alternative to database) <code>output_file</code> <code>Path \\| None</code> <code>None</code> Path for output file <code>output_format</code> <code>TabularReportFormat</code> <code>TSV</code> Output format <code>categorical_columns</code> <code>list[str]</code> see below Columns to group by <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output <p>Default categorical columns:</p> <ul> <li><code>subject_category</code></li> <li><code>subject_namespace</code></li> <li><code>predicate</code></li> <li><code>object_category</code></li> <li><code>object_namespace</code></li> <li><code>primary_knowledge_source</code></li> <li><code>aggregator_knowledge_source</code></li> <li><code>knowledge_level</code></li> <li><code>agent_type</code></li> <li><code>provided_by</code></li> </ul>"},{"location":"graph-operations/reference/configuration/#validation-rules_8","title":"Validation Rules","text":"<ul> <li>Input Required: Either <code>database_path</code> or <code>edge_file</code> must be provided</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_12","title":"Example","text":"<pre><code>from koza.model.graph_operations import EdgeReportConfig\n\nconfig = EdgeReportConfig(\n    database_path=Path(\"merged.duckdb\"),\n    output_file=Path(\"edge_report.tsv\"),\n    categorical_columns=[\"predicate\", \"subject_category\", \"object_category\"]\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#nodeexamplesconfig","title":"NodeExamplesConfig","text":"<p>Configuration for node examples generation.</p>"},{"location":"graph-operations/reference/configuration/#fields_20","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path \\| None</code> <code>None</code> Path to the DuckDB database <code>node_file</code> <code>FileSpec \\| None</code> <code>None</code> Node file to load (alternative to database) <code>output_file</code> <code>Path \\| None</code> <code>None</code> Path for output file <code>output_format</code> <code>TabularReportFormat</code> <code>TSV</code> Output format <code>sample_size</code> <code>int</code> <code>5</code> Number of examples per type <code>type_column</code> <code>str</code> <code>\"category\"</code> Column defining the type for grouping <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output"},{"location":"graph-operations/reference/configuration/#validation-rules_9","title":"Validation Rules","text":"<ul> <li>Input Required: Either <code>database_path</code> or <code>node_file</code> must be provided</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_13","title":"Example","text":"<pre><code>from koza.model.graph_operations import NodeExamplesConfig\n\nconfig = NodeExamplesConfig(\n    database_path=Path(\"merged.duckdb\"),\n    output_file=Path(\"node_examples.tsv\"),\n    sample_size=10,\n    type_column=\"category\"\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#edgeexamplesconfig","title":"EdgeExamplesConfig","text":"<p>Configuration for edge examples generation.</p>"},{"location":"graph-operations/reference/configuration/#fields_21","title":"Fields","text":"Field Type Default Description <code>database_path</code> <code>Path \\| None</code> <code>None</code> Path to the DuckDB database <code>node_file</code> <code>FileSpec \\| None</code> <code>None</code> Node file to load (for category enrichment) <code>edge_file</code> <code>FileSpec \\| None</code> <code>None</code> Edge file to load (alternative to database) <code>output_file</code> <code>Path \\| None</code> <code>None</code> Path for output file <code>output_format</code> <code>TabularReportFormat</code> <code>TSV</code> Output format <code>sample_size</code> <code>int</code> <code>5</code> Number of examples per type <code>type_columns</code> <code>list[str]</code> <code>[\"subject_category\", \"predicate\", \"object_category\"]</code> Columns defining the type for grouping <code>quiet</code> <code>bool</code> <code>False</code> Suppress progress output"},{"location":"graph-operations/reference/configuration/#validation-rules_10","title":"Validation Rules","text":"<ul> <li>Input Required: Either <code>database_path</code> or <code>edge_file</code> must be provided</li> </ul>"},{"location":"graph-operations/reference/configuration/#example_14","title":"Example","text":"<pre><code>from koza.model.graph_operations import EdgeExamplesConfig\n\nconfig = EdgeExamplesConfig(\n    database_path=Path(\"merged.duckdb\"),\n    output_file=Path(\"edge_examples.tsv\"),\n    sample_size=10,\n    type_columns=[\"predicate\", \"primary_knowledge_source\"]\n)\n</code></pre>"},{"location":"graph-operations/reference/configuration/#supporting-models","title":"Supporting Models","text":""},{"location":"graph-operations/reference/configuration/#databasestats","title":"DatabaseStats","text":"<p>Database statistics model used in operation results.</p> Field Type Default Description <code>nodes</code> <code>int</code> <code>0</code> Total node count <code>edges</code> <code>int</code> <code>0</code> Total edge count <code>dangling_edges</code> <code>int</code> <code>0</code> Edges with missing subject/object nodes <code>duplicate_nodes</code> <code>int</code> <code>0</code> Duplicate node count <code>singleton_nodes</code> <code>int</code> <code>0</code> Nodes with no edges <code>database_size_mb</code> <code>float \\| None</code> <code>None</code> Database size in megabytes"},{"location":"graph-operations/reference/configuration/#fileloadresult","title":"FileLoadResult","text":"<p>Result of loading a single file.</p> Field Type Default Description <code>file_spec</code> <code>FileSpec</code> required The file specification <code>records_loaded</code> <code>int</code> required Number of records loaded <code>detected_format</code> <code>KGXFormat</code> required Detected file format <code>load_time_seconds</code> <code>float</code> required Time to load the file <code>errors</code> <code>list[str]</code> <code>[]</code> Any errors during loading <code>temp_table_name</code> <code>str \\| None</code> <code>None</code> Temp table name for schema analysis"},{"location":"graph-operations/reference/configuration/#operationsummary","title":"OperationSummary","text":"<p>Summary statistics for CLI output.</p> Field Type Default Description <code>operation</code> <code>str</code> required Operation name <code>success</code> <code>bool</code> required Whether operation succeeded <code>message</code> <code>str</code> required Summary message <code>stats</code> <code>DatabaseStats \\| None</code> <code>None</code> Database statistics <code>files_processed</code> <code>int</code> <code>0</code> Number of files processed <code>total_time_seconds</code> <code>float</code> <code>0.0</code> Total operation time <code>warnings</code> <code>list[str]</code> <code>[]</code> Warning messages <code>errors</code> <code>list[str]</code> <code>[]</code> Error messages"},{"location":"graph-operations/tutorials/","title":"Tutorials","text":"<p>Step-by-step lessons covering graph operations workflows. These tutorials guide you through complete processes from start to finish.</p>"},{"location":"graph-operations/tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before starting these tutorials, ensure you have:</p> <ul> <li>Koza installed (<code>uvx koza</code>, <code>uv add koza</code>, <code>poetry add koza</code>, or <code>pip install koza</code>)</li> <li>Basic familiarity with the command line</li> <li>Sample KGX files (provided in tutorials or from your own data)</li> </ul>"},{"location":"graph-operations/tutorials/#available-tutorials","title":"Available Tutorials","text":""},{"location":"graph-operations/tutorials/#build-your-first-graph","title":"Build Your First Graph","text":"<p>Level: Beginner | Time: 15 minutes</p> <p>Create a knowledge graph from KGX files. This tutorial covers:</p> <ul> <li>Joining node and edge files into a database</li> <li>Exploring your graph with SQL queries</li> <li>Generating statistics and reports</li> <li>Exporting to different formats</li> </ul>"},{"location":"graph-operations/tutorials/#complete-merge-workflow","title":"Complete Merge Workflow","text":"<p>Level: Intermediate | Time: 30 minutes</p> <p>Combine data from multiple sources using the graph merge pipeline. This tutorial covers:</p> <ul> <li>Preparing input files and SSSOM mappings</li> <li>Running the merge pipeline with customization</li> <li>Understanding pipeline output and diagnostics</li> <li>Handling common issues and edge cases</li> </ul>"},{"location":"graph-operations/tutorials/#next-steps","title":"Next Steps","text":"<p>After completing the tutorials, explore:</p> <ul> <li>How-to Guides for specific task recipes</li> <li>Reference for detailed command documentation</li> <li>Explanation for deeper understanding</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/","title":"Build Your First Graph","text":"<p>This tutorial covers creating a knowledge graph from scratch, exploring it with SQL queries, and exporting it to files. You will learn the core workflow used in graph processing pipelines.</p> <p>Note: If running from a source checkout, use <code>uv run koza</code> instead of <code>koza</code>. If installed via pip, use <code>koza</code> directly.</p>"},{"location":"graph-operations/tutorials/first-graph/#overview","title":"Overview","text":"<ul> <li>Create sample KGX node and edge files</li> <li>Join files into a DuckDB database</li> <li>Explore your graph with SQL queries</li> <li>Generate statistics and reports</li> <li>Export to different formats</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Koza installed:<ul> <li>uv: <code>uvx koza</code> (run directly), <code>uv add koza</code>, or <code>uv pip install koza</code></li> <li>poetry: <code>poetry add koza</code></li> <li>pip: <code>pip install koza</code></li> </ul> </li> <li>DuckDB CLI (optional but recommended): Install from duckdb.org or <code>pip install duckdb</code></li> <li>Basic command line familiarity</li> </ul> <p>Verify your installation:</p> <pre><code>koza --version\n</code></pre> <p>You should see the Koza version number printed.</p>"},{"location":"graph-operations/tutorials/first-graph/#sample-data","title":"Sample Data","text":"<p>We will create a small knowledge graph about genes and diseases. The graph will have:</p> <ul> <li>5 nodes: 3 genes and 2 diseases</li> <li>5 edges: Gene-disease associations</li> </ul> <p>This is a tiny example, but the same commands work identically on graphs with millions of nodes and edges.</p>"},{"location":"graph-operations/tutorials/first-graph/#understanding-kgx-format","title":"Understanding KGX Format","text":"<p>KGX (Knowledge Graph Exchange) is a standard format for biomedical knowledge graphs. It uses:</p> <ul> <li>Nodes file: Contains entities (genes, diseases, phenotypes, etc.)</li> <li>Edges file: Contains relationships between entities</li> </ul> <p>Both are tab-separated files with specific columns. The minimum required columns are:</p> <ul> <li>Nodes: <code>id</code>, <code>category</code>, <code>name</code></li> <li>Edges: <code>id</code>, <code>subject</code>, <code>predicate</code>, <code>object</code></li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#step-1-create-sample-files","title":"Step 1: Create Sample Files","text":"<p>Let us create our sample data files. You can copy-paste these commands into your terminal, or create the files manually in a text editor.</p>"},{"location":"graph-operations/tutorials/first-graph/#create-a-working-directory","title":"Create a working directory","text":"<pre><code>mkdir -p kgx-tutorial\ncd kgx-tutorial\n</code></pre>"},{"location":"graph-operations/tutorials/first-graph/#create-the-nodes-file","title":"Create the nodes file","text":"<p>Create a file named <code>sample_nodes.tsv</code> with the following content:</p> <pre><code>cat &gt; sample_nodes.tsv &lt;&lt; 'EOF'\nid  category    name    description provided_by\nHGNC:1100   biolink:Gene    BRCA1   BRCA1 DNA repair associated infores:hgnc\nHGNC:1101   biolink:Gene    BRCA2   BRCA2 DNA repair associated infores:hgnc\nHGNC:7881   biolink:Gene    NOTCH1  notch receptor 1    infores:hgnc\nMONDO:0007254   biolink:Disease breast cancer   A malignant neoplasm of the breast  infores:mondo\nMONDO:0005070   biolink:Disease leukemia    Cancer of blood-forming tissues infores:mondo\nEOF\n</code></pre>"},{"location":"graph-operations/tutorials/first-graph/#create-the-edges-file","title":"Create the edges file","text":"<p>Create a file named <code>sample_edges.tsv</code> with the following content:</p> <pre><code>cat &gt; sample_edges.tsv &lt;&lt; 'EOF'\nid  subject predicate   object  primary_knowledge_source    provided_by\nuuid:1  HGNC:1100   biolink:gene_associated_with_condition  MONDO:0007254   infores:clinvar infores:clinvar\nuuid:2  HGNC:1101   biolink:gene_associated_with_condition  MONDO:0007254   infores:clinvar infores:clinvar\nuuid:3  HGNC:7881   biolink:gene_associated_with_condition  MONDO:0005070   infores:clinvar infores:clinvar\nuuid:4  HGNC:1100   biolink:interacts_with  HGNC:1101   infores:string  infores:string\nuuid:5  HGNC:1100   biolink:interacts_with  HGNC:7881   infores:string  infores:string\nEOF\n</code></pre>"},{"location":"graph-operations/tutorials/first-graph/#verify-the-files","title":"Verify the files","text":"<p>Check that your files look correct:</p> <pre><code>head sample_nodes.tsv\nhead sample_edges.tsv\n</code></pre> <p>You should see the header row followed by data rows for each file.</p>"},{"location":"graph-operations/tutorials/first-graph/#step-2-join-into-a-database","title":"Step 2: Join Into a Database","text":"<p>Now we will combine these files into a DuckDB database. DuckDB is an embedded analytical database that supports SQL queries on your graph data.</p>"},{"location":"graph-operations/tutorials/first-graph/#run-the-join-command","title":"Run the join command","text":"<pre><code>koza join \\\n  --nodes sample_nodes.tsv \\\n  --edges sample_edges.tsv \\\n  --output my_graph.duckdb\n</code></pre> <p>You should see output similar to:</p> <pre><code>Join operation completed successfully!\n</code></pre> <p>The command creates <code>my_graph.duckdb</code> containing two tables:</p> <ul> <li><code>nodes</code> - All your node records</li> <li><code>edges</code> - All your edge records</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#what-just-happened","title":"What just happened?","text":"<p>The <code>koza join</code> command:</p> <ol> <li>Read both input files</li> <li>Detected the TSV format automatically</li> <li>Inferred column types from the data</li> <li>Created a DuckDB database with optimized storage</li> <li>Loaded all records into <code>nodes</code> and <code>edges</code> tables</li> </ol> <p>This same process works with:</p> <ul> <li>Mixed file formats (TSV, JSONL, Parquet)</li> <li>Multiple input files per table</li> <li>Compressed files (.gz, .bz2)</li> <li>Files with different column schemas (missing columns are filled with NULL)</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#step-3-explore-with-sql","title":"Step 3: Explore with SQL","text":"<p>DuckDB allows you to query your graph using standard SQL. Let us explore our data.</p>"},{"location":"graph-operations/tutorials/first-graph/#count-nodes-and-edges","title":"Count nodes and edges","text":"<pre><code>duckdb my_graph.duckdb \"SELECT COUNT(*) AS node_count FROM nodes\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 node_count \u2502\n\u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <pre><code>duckdb my_graph.duckdb \"SELECT COUNT(*) AS edge_count FROM edges\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_count \u2502\n\u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"graph-operations/tutorials/first-graph/#view-the-database-schema","title":"View the database schema","text":"<p>See what columns are available:</p> <pre><code>duckdb my_graph.duckdb \"DESCRIBE nodes\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502  null   \u2502   key   \u2502 default \u2502  extra  \u2502\n\u2502   varchar   \u2502   varchar   \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id          \u2502 VARCHAR     \u2502 YES     \u2502         \u2502         \u2502         \u2502\n\u2502 category    \u2502 VARCHAR     \u2502 YES     \u2502         \u2502         \u2502         \u2502\n\u2502 name        \u2502 VARCHAR     \u2502 YES     \u2502         \u2502         \u2502         \u2502\n\u2502 description \u2502 VARCHAR     \u2502 YES     \u2502         \u2502         \u2502         \u2502\n\u2502 provided_by \u2502 VARCHAR     \u2502 YES     \u2502         \u2502         \u2502         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"graph-operations/tutorials/first-graph/#list-all-categories","title":"List all categories","text":"<p>See what types of entities are in your graph:</p> <pre><code>duckdb my_graph.duckdb \"SELECT DISTINCT category FROM nodes\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    category     \u2502\n\u2502     varchar     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 biolink:Gene    \u2502\n\u2502 biolink:Disease \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"graph-operations/tutorials/first-graph/#count-nodes-by-category","title":"Count nodes by category","text":"<pre><code>duckdb my_graph.duckdb \"SELECT category, COUNT(*) AS count FROM nodes GROUP BY category\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    category     \u2502 count \u2502\n\u2502     varchar     \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 biolink:Gene    \u2502     3 \u2502\n\u2502 biolink:Disease \u2502     2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"graph-operations/tutorials/first-graph/#find-specific-nodes","title":"Find specific nodes","text":"<p>Search for nodes by name:</p> <pre><code>duckdb my_graph.duckdb \"SELECT id, name FROM nodes WHERE name LIKE '%BRCA%'\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    id     \u2502 name  \u2502\n\u2502  varchar  \u2502 varchar\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 HGNC:1100 \u2502 BRCA1 \u2502\n\u2502 HGNC:1101 \u2502 BRCA2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"graph-operations/tutorials/first-graph/#explore-edge-relationships","title":"Explore edge relationships","text":"<p>List all predicate types:</p> <pre><code>duckdb my_graph.duckdb \"SELECT predicate, COUNT(*) AS count FROM edges GROUP BY predicate\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                predicate                \u2502 count \u2502\n\u2502                 varchar                 \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 biolink:gene_associated_with_condition  \u2502     3 \u2502\n\u2502 biolink:interacts_with                  \u2502     2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"graph-operations/tutorials/first-graph/#find-edges-for-a-specific-node","title":"Find edges for a specific node","text":"<p>What diseases are associated with BRCA1?</p> <pre><code>duckdb my_graph.duckdb \"\nSELECT e.predicate, n.name AS disease_name\nFROM edges e\nJOIN nodes n ON e.object = n.id\nWHERE e.subject = 'HGNC:1100'\n  AND e.predicate = 'biolink:gene_associated_with_condition'\n\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               predicate                \u2502 disease_name  \u2502\n\u2502                varchar                 \u2502    varchar    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 biolink:gene_associated_with_condition \u2502 breast cancer \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"graph-operations/tutorials/first-graph/#step-4-generate-statistics","title":"Step 4: Generate Statistics","text":"<p>Koza provides built-in commands for generating reports about your graph. These are especially useful for quality control when working with larger datasets.</p>"},{"location":"graph-operations/tutorials/first-graph/#generate-graph-statistics","title":"Generate graph statistics","text":"<pre><code>koza report graph-stats --database my_graph.duckdb --output graph_stats.yaml\n</code></pre> <p>This creates a YAML file with comprehensive statistics:</p> <pre><code>cat graph_stats.yaml\n</code></pre> <p>The report includes:</p> <ul> <li>Total node and edge counts</li> <li>Counts by category and predicate</li> <li>Namespace distributions</li> <li>Data source breakdowns</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#generate-a-qc-report","title":"Generate a QC report","text":"<p>The QC (Quality Control) report provides more detailed analysis:</p> <pre><code>koza report qc --database my_graph.duckdb --output qc_report.yaml\n</code></pre> <p>This report helps identify potential data quality issues like:</p> <ul> <li>Missing required fields</li> <li>Orphan nodes (nodes not connected to any edges)</li> <li>Invalid identifiers</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#view-report-summary","title":"View report summary","text":"<p>You can also run reports without saving to a file to see output in the terminal:</p> <pre><code>koza report graph-stats --database my_graph.duckdb\n</code></pre>"},{"location":"graph-operations/tutorials/first-graph/#step-5-export-to-files","title":"Step 5: Export to Files","text":"<p>After working with your graph in the database, you may want to export it back to files. The <code>split</code> command can export your entire graph or create subsets based on field values.</p>"},{"location":"graph-operations/tutorials/first-graph/#export-all-nodes-and-edges","title":"Export all nodes and edges","text":"<p>First, let us export the complete graph. The simplest approach is to split on a field where all records have the same value, or to use a field that groups naturally.</p> <p>Export nodes by category:</p> <pre><code>koza split sample_nodes.tsv category --output-dir ./export\n</code></pre> <p>Output filenames are generated as <code>{input}_{field_value}_{type}.tsv</code>. Since our input is <code>sample_nodes.tsv</code> and we're splitting by <code>category</code>, this produces:</p> <pre><code>./export/\n  sample_biolink_Gene_nodes.tsv      # nodes where category = biolink:Gene\n  sample_biolink_Disease_nodes.tsv   # nodes where category = biolink:Disease\n</code></pre> <p>(The <code>:</code> in <code>biolink:Gene</code> becomes <code>_</code> in the filename.)</p> <p>Note: When data has been loaded through <code>koza transform</code>, the <code>provided_by</code> field is typically overwritten with the ingest name. If you want to split by data source after transformation, use a different field or ensure your data preserves the original source information in another column.</p>"},{"location":"graph-operations/tutorials/first-graph/#convert-to-different-formats","title":"Convert to different formats","text":"<p>You can convert between formats during export. Convert to Parquet (a columnar format ideal for analytics):</p> <pre><code>koza split sample_nodes.tsv category \\\n  --output-dir ./parquet_export \\\n  --format parquet\n</code></pre> <p>This produces files like <code>sample_biolink_Gene_nodes.parquet</code>.</p> <p>Or convert to JSONL (JSON Lines, useful for streaming):</p> <pre><code>koza split sample_edges.tsv predicate \\\n  --output-dir ./jsonl_export \\\n  --format jsonl\n</code></pre> <p>This produces files like <code>sample_biolink_related_to_edges.jsonl</code> (one per predicate value).</p>"},{"location":"graph-operations/tutorials/first-graph/#check-exported-files","title":"Check exported files","text":"<p>Verify the exports look correct:</p> <pre><code># Check Parquet files\nls ./parquet_export/\n\n# Read a Parquet file with DuckDB\nduckdb -c \"SELECT * FROM read_parquet('./parquet_export/sample_Gene_nodes.parquet')\"\n\n# Check JSONL files\nls ./jsonl_export/\nhead ./jsonl_export/*.jsonl\n</code></pre>"},{"location":"graph-operations/tutorials/first-graph/#summary","title":"Summary","text":"<p>This tutorial covered the core graph operations workflow. Here is what you accomplished:</p> <ol> <li> <p>Created KGX files - You made sample node and edge files in the standard KGX TSV format</p> </li> <li> <p>Joined files into a database - The <code>koza join</code> command combined your files into a DuckDB database</p> </li> <li> <p>Explored with SQL - You queried your graph to:</p> </li> <li>Count nodes and edges</li> <li>List categories and predicates</li> <li>Find specific entities</li> <li> <p>Traverse relationships</p> </li> <li> <p>Generated reports - You used <code>koza report</code> to create statistics and quality control reports</p> </li> <li> <p>Exported data - You used <code>koza split</code> to export subsets in different formats (TSV, Parquet, JSONL)</p> </li> </ol>"},{"location":"graph-operations/tutorials/first-graph/#key-commands-summary","title":"Key Commands Summary","text":"Command Purpose <code>koza join</code> Combine KGX files into a DuckDB database <code>koza report graph-stats</code> Generate graph statistics <code>koza report qc</code> Generate quality control report <code>koza split</code> Export/split graph by field values"},{"location":"graph-operations/tutorials/first-graph/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, explore more advanced capabilities:</p>"},{"location":"graph-operations/tutorials/first-graph/#continue-learning","title":"Continue Learning","text":"<ul> <li>Complete Merge Workflow - Learn to combine data from multiple sources, normalize identifiers with SSSOM mappings, and clean your graph in one pipeline</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#how-to-guides","title":"How-to Guides","text":"<ul> <li>Join Files - Advanced joining with glob patterns, mixed formats, and schema reporting</li> <li>Split Graphs - More splitting options including prefix removal and multivalued fields</li> <li>Generate Reports - All available report types and customization options</li> <li>Normalize IDs - Use SSSOM mappings to harmonize identifiers</li> <li>Clean Graphs - Remove duplicates and dangling edges</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#reference","title":"Reference","text":"<ul> <li>CLI Reference - Complete documentation for all commands and options</li> </ul>"},{"location":"graph-operations/tutorials/first-graph/#cleanup","title":"Cleanup","text":"<p>When you are done experimenting, you can remove the tutorial files:</p> <pre><code>cd ..\nrm -rf kgx-tutorial\n</code></pre> <p>Or keep them around to continue exploring.</p> <p>These patterns scale to handle graphs with millions of nodes and edges.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/","title":"Complete Merge Workflow","text":"<p>This tutorial covers the full graph merge pipeline. It demonstrates combining data from multiple sources with different identifier schemes.</p> <p>Note: If running from a source checkout, use <code>uv run koza</code> instead of <code>koza</code>. If installed via pip, use <code>koza</code> directly.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#overview","title":"Overview","text":"<ul> <li>Prepare input files from multiple sources</li> <li>Create SSSOM mapping files for identifier harmonization</li> <li>Run the complete merge pipeline</li> <li>Customize pipeline steps for your use case</li> <li>Troubleshoot common issues</li> </ul>"},{"location":"graph-operations/tutorials/merge-pipeline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed the \"Build Your First Graph\" tutorial</li> <li>Understanding of KGX format (nodes and edges as TSV/JSONL/Parquet)</li> <li>Basic familiarity with identifier namespaces (e.g., HGNC, OMIM, MONDO)</li> </ul>"},{"location":"graph-operations/tutorials/merge-pipeline/#scenario","title":"Scenario","text":"<p>You are building a knowledge graph that integrates data from three sources:</p> <ol> <li>HGNC genes: Gene nodes with HGNC identifiers</li> <li>OMIM diseases: Disease nodes with OMIM identifiers</li> <li>Gene-disease associations: Edges connecting genes to diseases, but using MONDO identifiers for diseases</li> </ol> <p>The challenge: Your edges reference MONDO disease IDs, but your disease nodes use OMIM IDs. Without normalization, these edges would be \"dangling\" (referencing nodes that do not exist in your graph).</p> <p>The solution: Use SSSOM mappings to normalize OMIM identifiers to MONDO, allowing edges to connect properly to your disease nodes.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#sample-data","title":"Sample Data","text":"<p>Create a working directory and add these sample files.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#source-1-gene-nodes-genes_nodestsv","title":"Source 1: Gene Nodes (genes_nodes.tsv)","text":"<pre><code>id  name    category    provided_by\nHGNC:1100   BRCA1   biolink:Gene    hgnc\nHGNC:1101   BRCA2   biolink:Gene    hgnc\nHGNC:7989   TP53    biolink:Gene    hgnc\nHGNC:3689   CFTR    biolink:Gene    hgnc\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#source-2-disease-nodes-diseases_nodestsv","title":"Source 2: Disease Nodes (diseases_nodes.tsv)","text":"<pre><code>id  name    category    provided_by\nOMIM:114480 Breast Cancer   biolink:Disease omim\nOMIM:219700 Cystic Fibrosis biolink:Disease omim\nOMIM:151623 Li-Fraumeni Syndrome    biolink:Disease omim\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#source-3-gene-disease-edges-associations_edgestsv","title":"Source 3: Gene-Disease Edges (associations_edges.tsv)","text":"<p>Note: These edges use MONDO identifiers for diseases (not OMIM).</p> <pre><code>id  subject predicate   object  primary_knowledge_source    provided_by\nuuid:1  HGNC:1100   biolink:gene_associated_with_condition  MONDO:0007254   infores:monarch associations\nuuid:2  HGNC:1101   biolink:gene_associated_with_condition  MONDO:0007254   infores:monarch associations\nuuid:3  HGNC:7989   biolink:gene_associated_with_condition  MONDO:0007903   infores:monarch associations\nuuid:4  HGNC:3689   biolink:gene_associated_with_condition  MONDO:0009061   infores:monarch associations\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#mapping-file-mondo_omimsssomtsv","title":"Mapping File (mondo_omim.sssom.tsv)","text":"<p>This SSSOM file maps OMIM identifiers (object_id) to their equivalent MONDO identifiers (subject_id).</p> <pre><code>#curie_map:\n#  MONDO: http://purl.obolibrary.org/obo/MONDO_\n#  OMIM: https://omim.org/entry/\n#mapping_set_id: https://example.org/mappings/mondo-omim\nsubject_id  predicate_id    object_id   mapping_justification\nMONDO:0007254   skos:exactMatch OMIM:114480 semapv:ManualMappingCuration\nMONDO:0009061   skos:exactMatch OMIM:219700 semapv:ManualMappingCuration\nMONDO:0007903   skos:exactMatch OMIM:151623 semapv:ManualMappingCuration\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#step-1-prepare-input-files","title":"Step 1: Prepare Input Files","text":"<p>First, let's understand what each file contributes and verify they exist.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#examine-the-input-files","title":"Examine the Input Files","text":"<pre><code># Check file contents\nhead -5 genes_nodes.tsv\nhead -5 diseases_nodes.tsv\nhead -5 associations_edges.tsv\nhead -10 mondo_omim.sssom.tsv\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#understanding-the-id-mismatch-problem","title":"Understanding the ID Mismatch Problem","text":"<p>Look at the edge file - it references MONDO identifiers:</p> <pre><code># See what disease IDs the edges reference\ncut -f4 associations_edges.tsv | tail -n +2 | sort -u\n</code></pre> <p>Output: <pre><code>MONDO:0007254\nMONDO:0007903\nMONDO:0009061\n</code></pre></p> <p>But the disease nodes use OMIM identifiers:</p> <pre><code># See what IDs the disease nodes have\ncut -f1 diseases_nodes.tsv | tail -n +2\n</code></pre> <p>Output: <pre><code>OMIM:114480\nOMIM:219700\nOMIM:151623\n</code></pre></p> <p>Without normalization, all four edges would be dangling because <code>MONDO:0007254</code> does not match <code>OMIM:114480</code>, even though they represent the same disease.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#step-2-create-sssom-mappings","title":"Step 2: Create SSSOM Mappings","text":"<p>The SSSOM (Simple Standard for Sharing Ontological Mappings) file defines how identifiers map to each other.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#sssom-file-structure","title":"SSSOM File Structure","text":"<p>An SSSOM file has two parts:</p> <ol> <li>Header (optional): YAML metadata starting with <code>#</code></li> <li>Data: Tab-separated mappings</li> </ol> <pre><code>#curie_map:\n#  MONDO: http://purl.obolibrary.org/obo/MONDO_\n#  OMIM: https://omim.org/entry/\n#mapping_set_id: https://example.org/mappings/mondo-omim\nsubject_id  predicate_id    object_id   mapping_justification\nMONDO:0007254   skos:exactMatch OMIM:114480 semapv:ManualMappingCuration\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#key-columns-for-normalization","title":"Key Columns for Normalization","text":"Column Purpose Direction <code>subject_id</code> Target identifier (normalize TO this) Output <code>object_id</code> Source identifier (normalize FROM this) Input <code>predicate_id</code> Relationship type Metadata <code>mapping_justification</code> How mapping was determined Metadata <p>During normalization, Koza replaces <code>object_id</code> values in your edges with the corresponding <code>subject_id</code> values.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#finding-sssom-mappings","title":"Finding SSSOM Mappings","text":"<p>For real projects, you can obtain SSSOM mappings from:</p> <ul> <li>Mondo disease ontology mappings</li> <li>OBO Foundry ontologies</li> <li>SSSOM-Py tools for creating your own</li> <li>Biomedical identifier registries</li> </ul>"},{"location":"graph-operations/tutorials/merge-pipeline/#step-3-run-the-merge-pipeline","title":"Step 3: Run the Merge Pipeline","text":"<p>Now run the complete merge pipeline with a single command.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#basic-merge-command","title":"Basic Merge Command","text":"<pre><code>koza merge \\\n  -n genes_nodes.tsv \\\n  -n diseases_nodes.tsv \\\n  -e associations_edges.tsv \\\n  -m mondo_omim.sssom.tsv \\\n  -o merged_graph.duckdb\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#understanding-the-command-options","title":"Understanding the Command Options","text":"Option Purpose <code>-n</code>, <code>--nodes</code> Node files to load (use <code>-n</code> multiple times for multiple files) <code>-e</code>, <code>--edges</code> Edge files to load (use <code>-e</code> multiple times for multiple files) <code>-m</code>, <code>--mappings</code> SSSOM mapping files for normalization <code>-o</code>, <code>--output</code> Output DuckDB database file"},{"location":"graph-operations/tutorials/merge-pipeline/#expected-output","title":"Expected Output","text":"<pre><code>Starting merge pipeline...\nPipeline: join -&gt; normalize -&gt; prune\nOutput database: merged_graph.duckdb\nStep 1: Join - Loading input files...\nJoin completed: 3 files | 7 nodes | 4 edges\nStep 2: Normalize - Applying SSSOM mappings...\nNormalize completed: 1 mapping files | 3 edge references normalized\nStep 3: Prune - Cleaning graph structure...\nPrune completed: 0 dangling edges moved | 0 singleton nodes handled\nMerge pipeline completed successfully!\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#step-4-understand-the-output","title":"Step 4: Understand the Output","text":"<p>Let's examine what happened at each pipeline step.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#pipeline-steps-explained","title":"Pipeline Steps Explained","text":"<ol> <li>Join: Loaded all node and edge files into a unified database</li> <li>Normalize: Applied SSSOM mappings to convert OMIM IDs to MONDO IDs in edge references</li> <li>Prune: Removed dangling edges and handled singleton nodes</li> </ol>"},{"location":"graph-operations/tutorials/merge-pipeline/#verify-normalization-worked","title":"Verify Normalization Worked","text":"<p>Check that edge references were normalized. Normalization updates the <code>subject</code> and <code>object</code> columns in the edges table, preserving the original values in <code>original_subject</code> and <code>original_object</code>:</p> <pre><code>duckdb merged_graph.duckdb -c \"\n  SELECT subject, object, original_subject, original_object\n  FROM edges\n  WHERE original_object IS NOT NULL\n\"\n</code></pre> <p>Expected output: <pre><code>subject   | object        | original_subject | original_object\n----------+---------------+------------------+----------------\nHGNC:1100 | MONDO:0007254 | NULL             | OMIM:114480\nHGNC:1101 | MONDO:0007254 | NULL             | OMIM:114480\nHGNC:7989 | MONDO:0007903 | NULL             | OMIM:151623\nHGNC:3689 | MONDO:0009061 | NULL             | OMIM:219700\n</code></pre></p> <p>The <code>original_subject</code> and <code>original_object</code> columns preserve the original identifiers for provenance. Note that in this example, the edge objects were normalized from OMIM to MONDO IDs, so <code>original_object</code> contains the original OMIM ID while <code>object</code> now contains the MONDO equivalent.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#verify-edges-connect-properly","title":"Verify Edges Connect Properly","text":"<pre><code>duckdb merged_graph.duckdb -c \"\n  SELECT\n    e.subject,\n    n1.name as subject_name,\n    e.object,\n    n2.name as object_name\n  FROM edges e\n  JOIN nodes n1 ON e.subject = n1.id\n  JOIN nodes n2 ON e.object = n2.id\n\"\n</code></pre> <p>All edges should now join successfully to nodes.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#check-for-dangling-edges","title":"Check for Dangling Edges","text":"<pre><code>duckdb merged_graph.duckdb -c \"\n  SELECT COUNT(*) as dangling_count\n  FROM edges e\n  WHERE NOT EXISTS (SELECT 1 FROM nodes n WHERE n.id = e.subject)\n     OR NOT EXISTS (SELECT 1 FROM nodes n WHERE n.id = e.object)\n\"\n</code></pre> <p>Expected output: <code>0</code> (no dangling edges)</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#step-5-customize-the-pipeline","title":"Step 5: Customize the Pipeline","text":"<p>The merge pipeline is flexible and can be customized for different needs.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#skip-normalization","title":"Skip Normalization","text":"<p>If you do not need identifier normalization (e.g., IDs already match):</p> <pre><code>koza merge \\\n  -n \"*.nodes.tsv\" \\\n  -e \"*.edges.tsv\" \\\n  -o graph.duckdb \\\n  --skip-normalize\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#skip-pruning","title":"Skip Pruning","text":"<p>Keep all edges and nodes, even if some are dangling or disconnected:</p> <pre><code>koza merge \\\n  -n \"*.nodes.tsv\" \\\n  -e \"*.edges.tsv\" \\\n  -m \"*.sssom.tsv\" \\\n  -o graph.duckdb \\\n  --skip-prune\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#handle-singleton-nodes","title":"Handle Singleton Nodes","text":"<p>By default, singleton nodes (nodes with no edges) are kept. To move them to a separate table:</p> <pre><code>koza merge \\\n  -n \"*.nodes.tsv\" \\\n  -e \"*.edges.tsv\" \\\n  -m \"*.sssom.tsv\" \\\n  -o graph.duckdb \\\n  --remove-singletons\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#export-final-data","title":"Export Final Data","text":"<p>Export the merged graph to files for use in other tools:</p> <pre><code>koza merge \\\n  -n \"*.nodes.tsv\" \\\n  -e \"*.edges.tsv\" \\\n  -m \"*.sssom.tsv\" \\\n  -o graph.duckdb \\\n  --export \\\n  --export-dir ./output/ \\\n  --graph-name my_knowledge_graph\n</code></pre> <p>This creates <code>my_knowledge_graph_nodes.tsv</code> and <code>my_knowledge_graph_edges.tsv</code>.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#create-a-compressed-archive","title":"Create a Compressed Archive","text":"<p>For distribution, create a compressed tar archive:</p> <pre><code>koza merge \\\n  -n \"*.nodes.tsv\" \\\n  -e \"*.edges.tsv\" \\\n  -m \"*.sssom.tsv\" \\\n  -o graph.duckdb \\\n  --export \\\n  --export-dir ./output/ \\\n  --archive \\\n  --compress \\\n  --graph-name monarch_kg\n</code></pre> <p>This creates <code>monarch_kg.tar.gz</code> containing both node and edge files.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#auto-discovery-mode","title":"Auto-Discovery Mode","text":"<p>For directories with standard naming conventions, use auto-discovery:</p> <pre><code>koza merge \\\n  --input-dir ./data/ \\\n  --mappings-dir ./sssom/ \\\n  -o merged.duckdb\n</code></pre> <p>This automatically finds files matching patterns like <code>*_nodes.tsv</code> and <code>*.sssom.tsv</code>.</p>"},{"location":"graph-operations/tutorials/merge-pipeline/#troubleshooting","title":"Troubleshooting","text":""},{"location":"graph-operations/tutorials/merge-pipeline/#error-no-mapping-files-found","title":"Error: No Mapping Files Found","text":"<pre><code>Error: Must specify --mappings-dir, --mappings, or --skip-normalize\n</code></pre> <p>Solution: Either provide SSSOM mappings or skip normalization:</p> <pre><code># Option 1: Provide mappings\nkoza merge -n \"*.nodes.tsv\" -e \"*.edges.tsv\" \\\n  -m mondo.sssom.tsv -o graph.duckdb\n\n# Option 2: Skip normalization\nkoza merge -n \"*.nodes.tsv\" -e \"*.edges.tsv\" \\\n  --skip-normalize -o graph.duckdb\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#many-dangling-edges-after-merge","title":"Many Dangling Edges After Merge","text":"<p>If you see many dangling edges in the output, your mappings may be incomplete.</p> <p>Diagnose:</p> <pre><code># Check which edge objects are not in nodes\nduckdb merged_graph.duckdb -c \"\n  SELECT DISTINCT e.object\n  FROM edges e\n  WHERE NOT EXISTS (SELECT 1 FROM nodes n WHERE n.id = e.object)\n  LIMIT 20\n\"\n</code></pre> <p>Solutions:</p> <ol> <li>Add more mappings to cover missing identifiers</li> <li>Add the missing nodes from another source</li> <li>Use <code>--skip-prune</code> to keep dangling edges for later resolution</li> </ol>"},{"location":"graph-operations/tutorials/merge-pipeline/#duplicate-mappings-warning","title":"Duplicate Mappings Warning","text":"<pre><code>Found 234 duplicate mappings (one object_id mapped to multiple subject_ids).\nKeeping only one mapping per object_id.\n</code></pre> <p>This occurs when your SSSOM file contains one-to-many mappings (one source ID maps to multiple targets).</p> <p>Solution: Order your mapping files with preferred mappings first, or curate your SSSOM files to have deterministic mappings:</p> <pre><code># Preferred mappings file first\nkoza merge \\\n  -n \"*.nodes.tsv\" \\\n  -e \"*.edges.tsv\" \\\n  -m preferred_mappings.sssom.tsv \\\n  -m secondary_mappings.sssom.tsv \\\n  -o graph.duckdb\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#join-failed-no-files-loaded","title":"Join Failed - No Files Loaded","text":"<pre><code>Error: Join operation failed - no files were loaded\n</code></pre> <p>Causes:</p> <ul> <li>File paths are incorrect</li> <li>Glob patterns do not match any files</li> <li>Files are in an unsupported format</li> </ul> <p>Solution: Verify files exist and use correct patterns:</p> <pre><code># Check files exist\nls -la *.nodes.tsv *.edges.tsv\n\n# Use explicit paths instead of patterns\nkoza merge \\\n  -n genes_nodes.tsv \\\n  -n diseases_nodes.tsv \\\n  -e associations_edges.tsv \\\n  -m mondo_omim.sssom.tsv \\\n  -o graph.duckdb\n</code></pre>"},{"location":"graph-operations/tutorials/merge-pipeline/#out-of-memory-for-large-graphs","title":"Out of Memory for Large Graphs","text":"<p>For very large graphs, DuckDB handles most memory management automatically. If you encounter issues:</p> <ol> <li>Use a persistent database (always use <code>--output</code>)</li> <li>Process files in batches using <code>koza append</code></li> <li>Ensure sufficient disk space for temporary files</li> </ol>"},{"location":"graph-operations/tutorials/merge-pipeline/#summary","title":"Summary","text":"<p>This tutorial covered:</p> <ul> <li>Preparing KGX files from multiple sources with different ID schemes</li> <li>Creating SSSOM mapping files to harmonize identifiers</li> <li>Running the complete merge pipeline (join, deduplicate, normalize, prune)</li> <li>Customizing the pipeline by skipping steps or changing options</li> <li>Verifying merge results using DuckDB queries</li> <li>Troubleshooting common issues with mappings and dangling edges</li> </ul>"},{"location":"graph-operations/tutorials/merge-pipeline/#next-steps","title":"Next Steps","text":"<p>Related documentation:</p> <ul> <li>How to Normalize IDs - Deep dive into SSSOM mappings</li> <li>How to Join Files - Advanced file joining options</li> <li>How to Clean Graphs - Graph pruning strategies</li> <li>How to Generate Reports - QC and statistics</li> <li>CLI Reference - Complete command documentation</li> </ul>"}]}